{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a3a3c6-c029-486d-9a9b-d3bc1cf7c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed(666)\n",
    "np.random.seed(666)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import logsumexp\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy import optimize\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "torch.manual_seed(666)\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "from clusterless import preprocess\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779810ad-6236-439a-9c42-920935cc363c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cd1c562-85c3-4d92-98da-b1ae930d9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = 'dab512bd-a02d-4c1f-8dbc-9155a163efc0'\n",
    "rootpath = '/mnt/3TB/yizi/Downloads/ONE/openalyx.internationalbrainlab.org'\n",
    "trial_data_path = rootpath + '/danlab/Subjects/DY_016/2020-09-12/001/alf'\n",
    "neural_data_path = '/mnt/3TB/yizi/danlab/Subjects/DY_016'\n",
    "behavior_data_path = rootpath + '/paper_repro_ephys_data/figure9_10/original_data'\n",
    "save_path = '../saved_results/danlab/Subjects/DY_016/cavi_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc296e89-bada-4170-b524-5759675110d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "behave_dict = preprocess.load_behaviors_data(behavior_data_path, pid)\n",
    "motion_energy, wheel_velocity, wheel_speed, paw_speed, nose_speed, pupil_diameter = preprocess.preprocess_dynamic_behaviors(behave_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2d94ef8-0d09-4bb5-9342-3179e7c0ba04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pid: dab512bd-a02d-4c1f-8dbc-9155a163efc0\n",
      "eid: d23a44ef-1402-4ed7-97f5-47e9a7a504d9\n",
      "1st trial stim on time: 17.56, last trial stim on time 2310.24\n"
     ]
    }
   ],
   "source": [
    "unsorted_trials, stim_on_times, np1_channel_map = preprocess.load_neural_data(\n",
    "    pid=pid, \n",
    "    trial_data_path=trial_data_path,\n",
    "    neural_data_path=neural_data_path,\n",
    "    behavior_data_path=behavior_data_path,\n",
    "    keep_active_trials=True, \n",
    "    roi='all',\n",
    "    kilosort=False,\n",
    "    triage=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86822a8b-3a9e-48b4-b43f-8368c0a3bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = stim_on_times.shape[0]\n",
    "n_channels = np1_channel_map.shape[0]\n",
    "\n",
    "rand_idx = np.arange(n_trials)\n",
    "train_idx = rand_idx[:int(.8*n_trials)]\n",
    "test_idx = rand_idx[int(.8*n_trials):]\n",
    "\n",
    "train_y = wheel_velocity[train_idx]\n",
    "test_y = wheel_velocity[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d9b3225-ea05-463b-8058-db140f01f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_time_bins = 30\n",
    "binning = np.arange(0, 1.5, step=(1.5 - 0)/n_time_bins)\n",
    "n_trials = stim_on_times.shape[0]\n",
    "spike_train = np.concatenate(unsorted_trials)[:,[0,2,3,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "810fa125-e620-4724-95e2-a73cc08f75c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_idx = []\n",
    "time_idx = []\n",
    "all_trials = []\n",
    "for k in range(n_trials):\n",
    "    mask = np.logical_and(spike_train[:,0] >= stim_on_times[k]-0.5,\n",
    "                          spike_train[:,0] <= stim_on_times[k]+1\n",
    "                         )\n",
    "    trial = spike_train[mask,:]\n",
    "    trial[:,0] = trial[:,0] - trial[:,0].min()\n",
    "    time_bins = np.digitize(trial[:,0], binning, right=False)-1\n",
    "    for t in range(n_time_bins):\n",
    "        time_bin = trial[time_bins == t, 1:]\n",
    "        time_idx.append(np.ones_like(time_bin[:,0]) * t)\n",
    "        trial_idx.append(np.ones_like(time_bin[:,0]) * k)\n",
    "        all_trials.append(time_bin)\n",
    "time_idx = np.concatenate(time_idx)\n",
    "trial_idx = np.concatenate(trial_idx)\n",
    "all_trials = np.concatenate(all_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "622d9850-9845-44d9-9f99-d653173ac5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = np.sum([trial_idx == idx for idx in train_idx], axis=0).astype(bool)\n",
    "train_trials = all_trials[train_mask]\n",
    "train_time_idx = time_idx[train_mask]\n",
    "train_trial_idx = trial_idx[train_mask]\n",
    "\n",
    "test_mask = np.sum([trial_idx == idx for idx in test_idx], axis=0).astype(bool)\n",
    "test_trials = all_trials[test_mask]\n",
    "test_time_idx = time_idx[test_mask]\n",
    "test_trial_idx = trial_idx[test_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bff290-b554-4695-8f83-25b3d3d51bff",
   "metadata": {},
   "source": [
    "pre-compute mu and cov from training trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7484aa0b-7a3d-482a-ae88-0ba05c2ca98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization 0\n",
      "  Iteration 10\n",
      "  Iteration 20\n",
      "Initialization converged: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianMixture(init_params=&#x27;k-means++&#x27;, n_components=5, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianMixture</label><div class=\"sk-toggleable__content\"><pre>GaussianMixture(init_params=&#x27;k-means++&#x27;, n_components=5, verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianMixture(init_params='k-means++', n_components=5, verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = 5\n",
    "gmm = GaussianMixture(n_components=C, \n",
    "                      covariance_type='full', \n",
    "                      init_params='k-means++',\n",
    "                      verbose=1)\n",
    "gmm.fit(train_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efb40aa6-4b2e-47b6-b77e-313c5705da00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_gmm = {\n",
    "#     'weights': gmm.weights_,\n",
    "#     'mu': gmm.means_,\n",
    "#     'cov': gmm.covariances_,\n",
    "# }\n",
    "# np.save(save_path + f'init_gmm_c{C}_t{n_time_bins}.npy', init_gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88f653a-f4e2-4778-920d-404131e7f08c",
   "metadata": {},
   "source": [
    "how to initialize lambda?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2740077e-fac5-4beb-b97c-22be863121a8",
   "metadata": {},
   "source": [
    "### ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84e309c8-3c5e-4982-9fc7-366cd0cdedfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_log(x, minval=1e-10):\n",
    "    return torch.log(x + minval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9420fec-54d7-40e3-a9db-a83cdc1ab12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_elbo(s, r, mu, cov, lam, k_idxs, t_idxs, K, T, C):\n",
    "\n",
    "    # expected log-likelihood ( E_q(z)[log p(s,z|y)] )\n",
    "    # sum_{ktij} r_{ijt}^k * log(dN(s_i^kt; mu_j, cov_j))\n",
    "    \n",
    "    # sum_{ktij} r_{ijt}^k * ( log(lambda_jt^(k)) - log(sum_j' lambda_j't^(k)) ) \n",
    "    elbo_1 = 0; log_dens = []\n",
    "    for j in range(C):\n",
    "        log_dens.append(\n",
    "            torch.tensor(\n",
    "                multivariate_normal.logpdf(s, mu[j], cov[j])\n",
    "            )\n",
    "        )\n",
    "        elbo_1 += torch.einsum('i,i->', r[:,j], log_dens[-1])\n",
    "      \n",
    "    # einsum is slower\n",
    "    elbo_2 = torch.tensor([ \n",
    "                torch.sum( r[np.intersect1d(k_idxs[k], t_idxs[t])] * norm_lam[k,:,t] ) \\\n",
    "                for t in range(T) for k in range(K)\n",
    "             ]).sum()\n",
    "        \n",
    "    # entropy of q(z) ( E_q(z)[log q(z)] )\n",
    "    # sum_{ktij} log(r_{ijt}^k) * r_{ijt}^k\n",
    "    elbo_3 = - torch.einsum('ij,ij->', safe_log(r), r)\n",
    "    \n",
    "    elbo = elbo_1 + elbo_2 + elbo_3 \n",
    "    \n",
    "    print(f'elbo 1: {elbo_1}')\n",
    "    print(f'elbo 2: {elbo_2}')\n",
    "    print(f'elbo 3: {elbo_3}')\n",
    "    \n",
    "    return elbo, torch.vstack(log_dens).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ea3b25d-4b85-4a19-850c-e2f17ac7b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = len(train_idx)\n",
    "C = len(gmm.means_)\n",
    "T = n_time_bins\n",
    "\n",
    "s = torch.tensor(train_trials)\n",
    "r = torch.ones((train_trials.shape[0], C)) / C\n",
    "k_idxs = [torch.argwhere(torch.tensor(train_trial_idx) == k).reshape(-1) for k in range(K)]\n",
    "t_idxs = [torch.argwhere(torch.tensor(train_time_idx) == t).reshape(-1) for t in range(T)]\n",
    "\n",
    "y_k = torch.tensor(train_y)\n",
    "y = torch.zeros((train_trials.shape[0], T))\n",
    "for k in range(K):\n",
    "    y[torch.tensor(train_trial_idx) == k,:] = y_k[k]\n",
    "\n",
    "b = torch.normal(0, 1, size=(C,))\n",
    "beta = torch.normal(0, 1, size=(C,T,T))\n",
    "\n",
    "init_lam = torch.zeros((K,C,T))\n",
    "for k in range(K):\n",
    "    for t in range(T):\n",
    "        init_lam[k,:,t] = torch.exp(b + beta[:,t,:] @ y_k[k])\n",
    "norm_lam = safe_log(init_lam) - safe_log(init_lam.sum(1)[:,None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "33541c69-0ab1-4a60-b988-e350d84cdd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elbo 1: -115223206.2406357\n",
      "elbo 2: -7736824.149074271\n",
      "elbo 3: 1368129.286682262\n",
      "tensor(-1.2159e+08)\n",
      "CPU times: user 5.73 s, sys: 196 ms, total: 5.93 s\n",
      "Wall time: 2.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "elbo, log_dens = compute_elbo(s=s, \n",
    "                    r=r, \n",
    "                    mu=gmm.means_, \n",
    "                    cov=gmm.covariances_, \n",
    "                    lam=init_lam, \n",
    "                    k_idxs=k_idxs,\n",
    "                    t_idxs=t_idxs,\n",
    "                    K=len(train_idx),\n",
    "                    T=n_time_bins,\n",
    "                    C=len(gmm.means_))\n",
    "print(elbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbbc3cb-e6ad-4df8-b266-98f78ecb48d9",
   "metadata": {},
   "source": [
    "### Update $b_j$ and $\\beta_{jt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35239d03-ee3a-4c68-bc4f-09a6dddedd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.normal(0, 1, size=(C,)).requires_grad_(True)\n",
    "beta = torch.normal(0, 1, size=(C,T,T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc9e541-32de-4d44-b62e-8411198366f9",
   "metadata": {},
   "source": [
    "#### 1) find $b_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "310432bc-1de1-4322-a3d8-8daba74a7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_grad_bj(b_j, j, b_no_j, beta, y_k, r, k_idxs, t_idxs):\n",
    "    val = 0\n",
    "    for k in range(len(k_idxs)):\n",
    "        for t in range(len(t_idxs)):\n",
    "            k_t_idx = np.intersect1d(k_idxs[k], t_idxs[t])\n",
    "            num = b_j + beta[j,t,:] @ y_k[k]\n",
    "            denom = torch.logsumexp(torch.hstack([b_j, b_no_j]) + beta[:,t,:] @ y_k[k], 0)\n",
    "            val += torch.sum( r[k_t_idx, j] * ( 1 - torch.exp(num - denom) ) )\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10d0ce9d-dd13-47c2-81d8-de4c9f4c4047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(101833.2361, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = 3\n",
    "no_j = torch.cat([torch.arange(j), torch.arange(j+1,C)])\n",
    "b_j = b[j]\n",
    "b_no_j = b[no_j]\n",
    "safe_grad_bj(b_j, 3, b_no_j, beta, y_k, r, k_idxs, t_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e9a8bce-9711-4e01-9dba-a23b7728c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_bj(b_j, j, b_no_j, beta, y_k, r, k_idxs, t_idxs):\n",
    "    val = 0\n",
    "    for k in range(len(k_idxs)):\n",
    "        for t in range(len(t_idxs)):\n",
    "            k_t_idx = np.intersect1d(k_idxs[k], t_idxs[t])\n",
    "            val += torch.sum( r[k_t_idx, j] * \\\n",
    "                (1 - ( torch.exp(b_j + beta[j,t,:] @ y_k[k]) / \\\n",
    "                      (torch.exp(torch.hstack([b_j, b_no_j]) + beta[:,t,:] @ y_k[k])).sum() ) )\n",
    "                )\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7eecbc7f-6b23-4137-808c-2bca36ab62a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(101833.2361, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = 3\n",
    "no_j = torch.cat([torch.arange(j), torch.arange(j+1,C)])\n",
    "b_j = b[j]\n",
    "b_no_j = b[no_j]\n",
    "grad_bj(b_j, 3, b_no_j, beta, y_k, r, k_idxs, t_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f0e847b9-c46e-4a76-912b-f3903fcac6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_bj(j, b, beta, y_k, r, k_idxs, t_idxs, eps=1e-6, max_iter=1000):\n",
    "    b_j_new = b[j]\n",
    "    no_j = torch.cat([torch.arange(j), torch.arange(j+1, len(b))])\n",
    "    b_no_j = b[no_j]\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        f = safe_grad_bj(b_j_new, j, b_no_j, beta, y_k, r, k_idxs, t_idxs)\n",
    "        if abs(f) < eps:\n",
    "            print(f'grad at b_{j} = {b_j_new:.2f} is {f.item():.2f}')\n",
    "            return b_j_new.item()\n",
    "        \n",
    "        Df = grad(outputs=f, inputs=b_j_new)[0]\n",
    "        if Df == 0:\n",
    "            print('no solution found (0 derivative).')\n",
    "            return b[j].item()\n",
    "        b_j_new = b_j_new - f / Df\n",
    "        \n",
    "        if (i % 10) == 0:\n",
    "            print(f'grad at b_{j} = {b_j_new:.2f} is {f.item():.2f}')\n",
    "    print('exceeded max iter. no solution found.')\n",
    "    \n",
    "    return b[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b631f91e-ef4c-46bc-9ceb-cf55619d1aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad at b_1 = 4.02 is 92887.55\n",
      "grad at b_1 = 1.86 is 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.8628824473149748"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newton_bj(1, b, beta, y_k, r, k_idxs, t_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba024d2-963a-45fb-831e-66ea2545ba66",
   "metadata": {},
   "source": [
    "#### 2) find $\\beta_{jtl}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "927faf00-5d9a-4065-b5bc-eb21ade3666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_grad_beta_jtl(beta_jtl, j, t, l, beta_jt_no_l, beta_no_jt, b, y_k, r, k_idxs, t_idxs):\n",
    "    val = 0\n",
    "    no_j = np.concatenate([np.arange(j), np.arange(j+1,len(b))])\n",
    "    no_l = np.concatenate([np.arange(l), np.arange(l+1,len(t_idxs))])\n",
    "    for k in range(len(k_idxs)):\n",
    "        k_t_idx = np.intersect1d(k_idxs[k], t_idxs[t])\n",
    "        num = b[j] + beta_jtl * y_k[k][l] + beta_jt_no_l @ y_k[k][no_l]\n",
    "        x = b[no_j] + beta_no_jt @ y_k[k]\n",
    "        y = (b[j] + np.hstack([beta_jtl, beta_jt_no_l]) @ \\\n",
    "             np.hstack([y_k[k][l], y_k[k][no_l]])).reshape(-1)\n",
    "        denom = logsumexp(np.hstack([x, y]), 0)\n",
    "        val += ( r[k_t_idx, j, None] * y_k[k][l] * ( 1 - np.exp(num - denom) ) ).sum()\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8e1aae48-4cd1-460f-9253-439d68205659",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 3; t = 5; l = 1\n",
    "no_j = torch.cat([torch.arange(j), torch.arange(j+1, len(b))])\n",
    "no_t = torch.cat([torch.arange(t), torch.arange(t+1, len(t_idxs))])\n",
    "no_l = torch.cat([torch.arange(l), torch.arange(l+1,len(t_idxs))])\n",
    "beta_jtl = beta[j,t,l]\n",
    "beta_jt_no_l = beta[j,t,no_l]\n",
    "beta_no_j_t = beta[no_j][:,t]\n",
    "\n",
    "res = [safe_grad_beta_jtl(beta_jtl.detach().numpy(), \n",
    "                     j, t, l, \n",
    "                     beta_jt_no_l.detach().numpy(), \n",
    "                     beta_no_j_t.detach().numpy(), \n",
    "                     b.detach().numpy(), \n",
    "                     y_k.detach().numpy(), \n",
    "                     r.detach().numpy(), \n",
    "                     k_idxs, t_idxs) for l in range(len(t_idxs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b07c73a4-b00b-4ea4-82ee-a7a2c28368c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(res);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9cda4aa2-81ae-4598-924b-ade1e9374e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_beta_jtl(beta_jtl, j, t, l, beta_jt_no_l, beta_no_jt, b, y_k, r, k_idxs, t_idxs):\n",
    "    val = 0\n",
    "    no_j = np.concatenate([np.arange(j), np.arange(j+1,len(b))])\n",
    "    no_l = np.concatenate([np.arange(l), np.arange(l+1,len(t_idxs))])\n",
    "    for k in range(len(k_idxs)):\n",
    "        k_t_idx = np.intersect1d(k_idxs[k], t_idxs[t])\n",
    "        val += ( r[k_t_idx, j, None] * y_k[k][l] * \\\n",
    "            (1 - ( np.exp(b[j] + beta_jtl * y_k[k][l] + beta_jt_no_l @ y_k[k][no_l]) / \\\n",
    "                  ((np.exp(b[no_j, None] + beta_no_jt @ y_k[k])).sum() + \\\n",
    "                    np.exp(b[j] + beta_jtl * y_k[k][l] + beta_jt_no_l @ y_k[k][no_l]) ) ) )\n",
    "            ).sum(0)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9e3bd160-4ac3-4cef-8bb2-3517d3276fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 3; t = 5; l = 1\n",
    "no_j = torch.cat([torch.arange(j), torch.arange(j+1, len(b))])\n",
    "no_t = torch.cat([torch.arange(t), torch.arange(t+1, len(t_idxs))])\n",
    "no_l = torch.cat([torch.arange(l), torch.arange(l+1,len(t_idxs))])\n",
    "beta_jtl = beta[j,t,l]\n",
    "beta_jt_no_l = beta[j,t,no_l]\n",
    "beta_no_j_t = beta[no_j][:,t]\n",
    "\n",
    "res = [grad_beta_jtl(beta_jtl.detach().numpy(), \n",
    "                     j, t, l, \n",
    "                     beta_jt_no_l.detach().numpy(), \n",
    "                     beta_no_j_t.detach().numpy(), \n",
    "                     b.detach().numpy(), \n",
    "                     y_k.detach().numpy(), \n",
    "                     r.detach().numpy(), \n",
    "                     k_idxs, t_idxs) for l in range(len(t_idxs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5faf11cf-4aca-4a5a-a25b-5d6db7c01026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(res);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "195e47f2-d310-48de-a3bd-15f99806aca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    fjac: array([[-1.]])\n",
       "     fun: array([-3.33066907e-16])\n",
       " message: 'The solution converged.'\n",
       "    nfev: 10\n",
       "     qtf: array([-1.4182322e-11])\n",
       "       r: array([0.00823858])\n",
       "  status: 1\n",
       " success: True\n",
       "       x: array([-215.5655483])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize.root(grad_beta_jtl, beta_jtl.detach().numpy(),\n",
    "       args=(j, t, l, beta_jt_no_l.detach().numpy(), \n",
    "             beta_no_jt.detach().numpy(),\n",
    "             b.detach().numpy(), y_k.detach().numpy(), \n",
    "             r.detach().numpy(), k_idxs, t_idxs), method='hybr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c94f33d-f50f-4bc6-a58a-1204d5cec667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad at beta (j = 2, t = 0, l = 9) : 145.29 is -0.00.\n"
     ]
    }
   ],
   "source": [
    "for j in [2]:\n",
    "    for t in [0]:\n",
    "        for l in [9]:\n",
    "            no_j = torch.cat([torch.arange(j), torch.arange(j+1, len(b))])\n",
    "            no_t = torch.cat([torch.arange(t), torch.arange(t+1, len(t_idxs))])\n",
    "            no_l = torch.cat([torch.arange(l), torch.arange(l+1,len(t_idxs))])\n",
    "\n",
    "            init_guess = np.linspace(-500, -100, 100)\n",
    "            inputs = (j, t, l, \n",
    "                       beta[j,t,no_l].detach().numpy(), \n",
    "                       beta[no_j][:,no_t].detach().numpy(),\n",
    "                       b.detach().numpy(), \n",
    "                       y_k.detach().numpy(), \n",
    "                       r.detach().numpy(), \n",
    "                       k_idxs, t_idxs\n",
    "                      )\n",
    "            sol = optimize.root(grad_beta_jtl, beta[j,t,l].detach().numpy(), \n",
    "                                            args = inputs, method='hybr')\n",
    "            beta[j,t,l] = sol.x[0]\n",
    "            res = sol.fun[0]\n",
    "            print(f'grad at beta (j = {j}, t = {t}, l = {l}) : {beta[j,t,l]:.2f} is {res:.2f}.')\n",
    "            counter = 0\n",
    "            while not np.allclose(res, 0): \n",
    "                sol = optimize.root(grad_beta_jtl, init_guess[counter], args = inputs, method='hybr')\n",
    "                beta[j,t,l] = sol.x[0]\n",
    "                res = sol.fun[0]\n",
    "                counter += 1\n",
    "                print(f'grad at beta (j = {j}, t = {t}, l = {l}) : {beta[j,t,l]:.2f} is {res:.2f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b22ad081-3b7a-48e3-bcd0-f9bf85c7f58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(beta_new[0,2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fad509f-9f9d-4001-a314-e45432fb0c70",
   "metadata": {},
   "source": [
    "#### CAVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5687868e-e024-4c4d-ac5e-24804bda583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cavi(s, y_k, means, covs, K, T, C, D, k_idxs, t_idxs, max_iter=20, eps=1e-10):\n",
    "    \n",
    "    # initialize\n",
    "    r = torch.ones((s.shape[0], C)) / C\n",
    "    mu, cov = torch.tensor(means), torch.tensor(covs)\n",
    "    b = torch.normal(0, 1, size=(C,)).requires_grad_(True)\n",
    "    beta = torch.normal(0, 1, size=(C,T,T))\n",
    "\n",
    "    lam = torch.zeros((K,C,T))\n",
    "    for k in range(K):\n",
    "        for t in range(T):\n",
    "            lam[k,:,t] = torch.exp(b + beta[:,t,:] @ y_k[k])\n",
    "    norm_lam = safe_log(lam) - safe_log(lam.sum(1)[:,None,:])\n",
    "    \n",
    "    # compute ELBO\n",
    "    elbo, log_dens = compute_elbo(s, r, mu, cov, lam, k_idxs, t_idxs, K, T, C)\n",
    "    convergence = 1.\n",
    "    elbos = [elbo]\n",
    "    print(f'initial elbo: {elbos[-1]:.2f}')\n",
    "    \n",
    "    it = 1\n",
    "    while convergence > eps or convergence < 0:  # while ELBO not converged   \n",
    "        # update q(z)\n",
    "        # r_{ijt}^k = rho_{ijt}^k / sum_j rho_{ijt}^k\n",
    "        # rho_{ijt}^k = exp( log(dN(s_i^kt; mu_j, cov_j)) + \n",
    "        #                      ( log(lambda_jt^(k)) - log(sum_j' lambda_j't^(k)) ) )    \n",
    "        for k in range(K):\n",
    "            for t in range(T):\n",
    "                k_t_idx = np.intersect1d(k_idxs[k], t_idxs[t])\n",
    "                r[k_t_idx] = torch.exp(log_dens[k_t_idx] + norm_lam[k,:,t])\n",
    "                r[k_t_idx] = torch.einsum('ij,i->ij', r[k_t_idx], 1/r[k_t_idx].sum(1))\n",
    "            \n",
    "        \n",
    "        # update beta\n",
    "        # beta_new = torch.zeros_like(beta)\n",
    "        for j in range(C):\n",
    "            no_j = torch.cat([torch.arange(j), torch.arange(j+1, len(b))])\n",
    "            for t in range(T):\n",
    "                no_t = torch.cat([torch.arange(t), torch.arange(t+1, len(t_idxs))])\n",
    "                for l in range(T):\n",
    "                    no_l = torch.cat([torch.arange(l), torch.arange(l+1,len(t_idxs))])\n",
    "                    \n",
    "                    inputs = (j, t, l, \n",
    "                               beta[j,t,no_l].detach().numpy(), \n",
    "                               beta[no_j][:,t].detach().numpy(),\n",
    "                               b.detach().numpy(), \n",
    "                               y_k.detach().numpy(), \n",
    "                               r.detach().numpy(), \n",
    "                               k_idxs, t_idxs\n",
    "                              )\n",
    "                    sol = optimize.root(safe_grad_beta_jtl, beta[j,t,l].detach().numpy(), \n",
    "                                                    args = inputs, method='hybr')\n",
    "                    beta[j,t,l] = sol.x[0]\n",
    "                    res = sol.fun\n",
    "                    counter = 0\n",
    "                    init_guess = sol.x[0]-250.\n",
    "                    while not np.allclose(res, 0): \n",
    "                        # if np.abs(res) < 50:\n",
    "                        #     init_guess += 1e-1\n",
    "                        # elif np.abs(res) < 1:\n",
    "                        #     init_guess += 1e-2\n",
    "                        # elif np.abs(res) < .2:\n",
    "                        #     # hard-coding\n",
    "                        #     res = 0\n",
    "                        # else:\n",
    "                        #     init_guess += 10.\n",
    "                        init_guess += 10.\n",
    "                        sol = optimize.root(safe_grad_beta_jtl, init_guess, args = inputs, method='hybr')\n",
    "                        beta[j,t,l] = sol.x[0]\n",
    "                        res = sol.fun[0]\n",
    "                        # beta[j,t,l] = init_guess\n",
    "                        # res = safe_grad_beta_jtl(init_guess, \n",
    "                        #                    j, t, l, \n",
    "                        #                    beta[j,t,no_l].detach().numpy(), \n",
    "                        #                    beta[no_j][:,t].detach().numpy(),\n",
    "                        #                    b.detach().numpy(), \n",
    "                        #                    y_k.detach().numpy(), \n",
    "                        #                    r.detach().numpy(), \n",
    "                        #                    k_idxs, t_idxs).item()\n",
    "                        counter += 1\n",
    "                        print(f'grad at beta (j = {j}, t = {t}, l = {l}) : {beta[j,t,l]:.2f} is {res:.2f}.')\n",
    "                    if np.logical_and((t+1) % 1 == 0, (l+1) % 1 == 0):\n",
    "                        print(f'grad at beta (j = {j}, t = {t}, l = {l}) : {beta[j,t,l]:.2f} is {res:.2f}.')\n",
    "        # beta = beta_new.clone()\n",
    "        \n",
    "        \n",
    "        # update b \n",
    "        # FIX: order of update\n",
    "        b_new = torch.zeros_like(b)\n",
    "        for j in range(C):\n",
    "            b_new[j] = newton_bj(j, b, beta, y_k, r, k_idxs, t_idxs)\n",
    "        b = b_new.clone().requires_grad_(True)\n",
    "            \n",
    "        \n",
    "        # compute lambda's\n",
    "        for k in range(K):\n",
    "            for t in range(T):\n",
    "                lam[k,:,t] = torch.exp(b + beta[:,t,:] @ y_k[k])\n",
    "        norm_lam = safe_log(lam) - safe_log(lam.sum(1)[:,None,:]) \n",
    "        \n",
    "        \n",
    "        # update means and covs\n",
    "        norm = r.sum(0)\n",
    "        mu = torch.einsum('j,ij,ip->jp', 1/norm, r, s)\n",
    "        cov = [torch.einsum(\n",
    "            ',i,ip,id->pd', 1/norm[j], r[:,j], s-mu[j], s-mu[j] \n",
    "            ) for j in range(C)]\n",
    "        \n",
    "        # compute ELBO\n",
    "        elbo, log_dens = compute_elbo(s, r, mu, cov, lam, k_idxs, t_idxs, K, T, C)\n",
    "        elbos.append(elbo)\n",
    "        convergence = elbos[-1] - elbos[-2]\n",
    "        \n",
    "        print(f'iter: {it} elbo: {elbos[-1]:.2f}.')\n",
    "        it +=1 \n",
    "        if it > max_iter: \n",
    "            print('reached max iter allowed.')\n",
    "            break\n",
    "            \n",
    "    if abs(convergence) <= eps:\n",
    "        print('converged.')\n",
    "    \n",
    "    return r, lam, b, beta, mu, cov, elbos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30205606-6402-4fec-915b-fcae3f47e96e",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641524f1-d532-4222-b802-4da5e6a74258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elbo 1: -120315906.67333016\n",
      "elbo 2: -5870987.152173714\n",
      "elbo 3: 956281.3335645112\n",
      "initial elbo: -125230612.49\n",
      "grad at beta (j = 0, t = 0, l = 0) : -9.29 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 1) : -25.68 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 2) : -23.43 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 3) : -17.72 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 4) : -41.30 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 5) : 13.28 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 6) : 29.74 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 7) : 5.13 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 8) : -6.36 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 9) : 16.43 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 10) : 0.69 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 11) : 0.42 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 12) : 0.65 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 13) : 0.50 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 14) : 0.96 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 15) : -1.25 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 16) : 1.14 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 17) : 0.54 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 18) : -0.72 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 19) : 0.26 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 20) : -0.72 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 21) : -0.74 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 22) : -1.63 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 23) : -5.90 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 24) : -3.96 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 25) : 1.53 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 26) : 0.48 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 27) : 0.68 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 28) : 1.14 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 29) : 1.12 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 0) : -18.00 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 1) : -1.07 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 2) : -39.68 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 3) : -22.86 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 4) : -25.01 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 5) : -18.82 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 6) : -10.55 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 7) : -30.52 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 8) : 50.89 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 9) : 22.99 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 10) : 0.94 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 11) : -1.01 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 12) : 2.82 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 13) : 2.20 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 14) : -0.10 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 15) : -0.16 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 16) : -0.51 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 17) : -0.56 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 18) : 0.70 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 19) : 0.05 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 20) : -0.23 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 21) : -2.05 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 22) : 0.17 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 23) : -0.20 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 24) : -1.41 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 25) : 2.28 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 26) : 3.72 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 27) : 2.25 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 28) : -0.08 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 29) : 2.88 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 0) : 16.00 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 1) : -93.96 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 2) : -28.13 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 3) : -57.91 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 4) : -34.21 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 5) : 45.39 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 6) : 34.44 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 7) : -23.96 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 8) : 15.18 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 9) : -5.31 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 10) : -1.42 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 11) : 0.30 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 12) : 1.21 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 13) : -0.41 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 14) : -0.30 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 15) : -0.19 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 16) : -1.14 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 17) : -3.32 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 18) : -4.89 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 19) : 1.10 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 20) : -0.50 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 21) : 0.44 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 22) : 1.17 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 23) : 0.40 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 24) : 0.81 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 25) : 0.92 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 26) : 1.01 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 27) : -0.41 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 28) : 0.94 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 29) : 0.83 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 0) : -44.49 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 1) : 14.40 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 2) : 22.00 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 3) : -2.89 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 4) : -11.89 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 5) : -9.35 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 6) : 14.14 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 7) : -64.50 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 8) : 11.44 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 9) : 3.45 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 10) : 1.26 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 11) : -0.21 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 12) : 3.11 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 13) : 1.24 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 14) : -0.45 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 15) : -1.32 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 16) : 0.13 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 17) : -1.24 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 18) : -0.20 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 19) : -0.13 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 20) : -0.33 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 21) : 0.74 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 22) : 0.45 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 23) : 1.18 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 24) : -0.32 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 25) : 2.41 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 26) : 3.90 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 27) : 7.94 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 28) : 13.03 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 29) : 4.52 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 0) : -47.78 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 1) : -18.65 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 2) : -44.54 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 3) : -37.32 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 4) : -12.26 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 5) : -4.84 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 6) : -84.70 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 7) : -122.95 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 8) : 8.42 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 9) : 47.27 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 10) : 2.61 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 11) : 1.94 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 12) : 0.14 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 13) : 1.36 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 14) : -1.94 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 15) : 1.94 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 16) : 0.31 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 17) : 0.79 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 18) : 0.17 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 19) : 2.43 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 20) : 5.45 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 21) : 11.31 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 22) : 10.50 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 23) : 1.77 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 24) : 4.54 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 25) : -0.51 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 26) : 2.27 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 27) : 8.99 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 28) : 10.42 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 29) : 12.44 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 0) : 3.67 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 1) : 0.53 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 2) : -19.44 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 3) : -10.80 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 4) : 0.88 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 5) : 21.87 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 6) : -46.26 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 7) : -99.77 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 8) : 4.04 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 9) : 28.35 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 10) : 2.48 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 11) : -1.74 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 12) : 2.84 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 13) : 1.05 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 14) : 1.64 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 15) : 0.06 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 16) : -0.76 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 17) : 2.23 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 18) : -1.70 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 19) : -0.72 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 20) : -1.20 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 21) : -2.18 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 22) : 0.25 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 23) : 0.09 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 24) : 1.36 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 25) : -0.21 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 26) : 0.80 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 27) : 0.11 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 28) : 1.50 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 29) : 1.50 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 0) : -17.66 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 1) : -9.15 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 2) : 12.26 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 3) : 2.48 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 4) : -1.45 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 5) : 41.70 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 6) : -3.42 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 7) : -16.36 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 8) : -1.22 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 9) : 5.64 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 10) : 3.47 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 11) : 0.15 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 12) : 2.73 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 13) : 2.67 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 14) : 0.42 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 15) : 0.59 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 16) : 0.34 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 17) : -0.87 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 18) : -0.02 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 19) : -0.20 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 20) : -0.61 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 21) : 0.24 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 22) : -0.95 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 23) : -1.04 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 24) : -1.33 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 25) : -1.13 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 26) : -0.84 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 27) : -0.25 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 28) : 1.70 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 29) : 0.12 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 0) : 24.22 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 1) : -63.48 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 2) : -45.43 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 3) : -53.78 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 4) : -9.52 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 5) : 19.15 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 6) : 40.85 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 7) : -10.29 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 8) : 34.82 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 9) : 1.25 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 10) : 0.70 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 11) : -0.11 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 12) : 0.32 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 13) : 0.01 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 14) : 1.02 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 15) : -1.28 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 16) : -1.02 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 17) : -0.53 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 18) : -2.08 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 19) : -0.64 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 20) : 0.82 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 21) : -2.11 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 22) : -0.38 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 23) : -0.45 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 24) : 0.95 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 25) : 1.04 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 26) : 1.26 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 27) : 2.28 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 28) : 6.80 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 29) : 5.34 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 0) : -18.57 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 1) : -80.16 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 2) : -42.00 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 3) : -41.64 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 4) : -27.88 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 5) : 49.39 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 6) : 2.43 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 7) : 14.88 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 8) : 90.56 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 9) : -0.32 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 10) : 0.27 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 11) : -0.18 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 12) : -0.10 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 13) : -1.18 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 14) : -2.77 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 15) : -0.27 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 16) : 0.15 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 17) : 1.51 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 18) : 0.30 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 19) : -1.34 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 20) : 0.30 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 21) : -0.74 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 22) : 1.96 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 23) : 0.34 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 24) : -1.84 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 25) : 0.14 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 26) : -0.75 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 27) : 0.23 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 28) : 1.04 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 29) : 1.51 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 0) : -1.42 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 1) : 4.95 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 2) : -2.56 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 3) : 0.93 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 4) : 1.13 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 5) : -3.81 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 6) : -34.53 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 7) : -74.84 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 8) : 19.29 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 9) : 30.86 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 10) : 2.73 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 11) : 1.68 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 12) : 0.82 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 13) : -0.09 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 14) : 0.97 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 15) : -1.36 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 16) : 0.16 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 17) : 2.29 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 18) : 1.28 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 19) : 6.61 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 20) : 1.41 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 21) : 0.20 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 22) : -1.92 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 23) : -0.61 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 24) : 1.44 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 25) : -0.11 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 26) : 1.28 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 27) : 1.83 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 28) : 3.13 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 29) : 7.33 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 0) : -10.40 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 1) : 2.83 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 2) : 0.91 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 3) : -57.08 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 4) : -14.51 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 5) : -26.51 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 6) : 16.58 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 7) : 12.20 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 8) : 48.86 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 9) : 14.58 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 10) : -0.44 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 11) : 0.95 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 12) : 1.62 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 13) : 0.78 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 14) : 0.92 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 15) : 0.77 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 16) : -1.02 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 17) : 0.22 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 18) : 0.52 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 19) : 0.50 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 20) : 2.28 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 21) : 3.66 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 22) : 0.07 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 23) : 1.32 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 24) : 0.25 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 25) : -0.29 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 26) : 1.80 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 27) : 0.88 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 28) : 0.72 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 29) : 1.32 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 0) : 11.99 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 1) : -8.11 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 2) : -200.04 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 3) : -74.39 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 4) : -14.91 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 5) : -11.28 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 6) : -42.43 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 7) : -148.36 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 8) : 26.87 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 9) : 19.71 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 10) : 1.24 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 11) : -1.19 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 12) : 3.54 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 13) : 2.75 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 14) : -1.18 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 15) : -0.45 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 16) : 0.48 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 17) : -0.29 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 18) : -2.33 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 19) : 0.35 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 20) : -0.04 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 21) : 1.47 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 22) : -1.58 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 23) : -0.08 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 24) : -1.09 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 25) : -4.55 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 26) : -2.84 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 27) : 1.84 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 28) : -0.15 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 29) : -0.50 is -0.00.\n",
      "grad at beta (j = 0, t = 12, l = 0) : 10.82 is -0.00.\n",
      "grad at beta (j = 0, t = 12, l = 1) : -45.82 is 0.00.\n",
      "grad at beta (j = 0, t = 12, l = 2) : -36.32 is 0.00.\n",
      "grad at beta (j = 0, t = 12, l = 3) : -71.82 is -0.00.\n",
      "grad at beta (j = 0, t = 12, l = 4) : -71.30 is -0.00.\n",
      "grad at beta (j = 0, t = 12, l = 5) : 13.24 is -0.00.\n",
      "grad at beta (j = 0, t = 12, l = 6) : 165.19 is -0.00.\n",
      "grad at beta (j = 0, t = 12, l = 7) : -16.60 is 0.00.\n",
      "grad at beta (j = 0, t = 12, l = 8) : 52.71 is 0.00.\n",
      "grad at beta (j = 0, t = 12, l = 9) : -21.33 is -0.00.\n",
      "grad at beta (j = 0, t = 12, l = 10) : 1.70 is 0.00.\n",
      "grad at beta (j = 0, t = 12, l = 11) : 0.54 is 0.00.\n",
      "grad at beta (j = 0, t = 12, l = 12) : -1.02 is 0.00.\n",
      "grad at beta (j = 0, t = 12, l = 13) : -1.92 is -0.00.\n",
      "grad at beta (j = 0, t = 12, l = 14) : -13.36 is 0.00.\n",
      "grad at beta (j = 0, t = 12, l = 15) : -2.34 is -0.00.\n",
      "grad at beta (j = 0, t = 12, l = 16) : -1.94 is -0.00.\n",
      "grad at beta (j = 0, t = 12, l = 17) : -6.19 is 0.00.\n",
      "grad at beta (j = 0, t = 12, l = 18) : -0.79 is -0.00.\n",
      "grad at beta (j = 0, t = 12, l = 19) : 0.19 is 0.00.\n",
      "grad at beta (j = 0, t = 12, l = 20) : -1.72 is -0.00.\n",
      "grad at beta (j = 0, t = 12, l = 21) : -1.06 is 0.00.\n",
      "grad at beta (j = 0, t = 12, l = 22) : 0.65 is -0.00.\n",
      "grad at beta (j = 0, t = 12, l = 23) : -0.49 is -0.00.\n",
      "grad at beta (j = 0, t = 12, l = 24) : 1.88 is -0.00.\n",
      "grad at beta (j = 0, t = 12, l = 25) : 1.06 is 0.00.\n",
      "grad at beta (j = 0, t = 12, l = 26) : -0.77 is -0.00.\n",
      "grad at beta (j = 0, t = 12, l = 27) : 1.46 is 0.00.\n",
      "grad at beta (j = 0, t = 12, l = 28) : 1.83 is 0.00.\n",
      "grad at beta (j = 0, t = 12, l = 29) : 1.08 is -0.00.\n",
      "grad at beta (j = 0, t = 13, l = 0) : 9.32 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 1) : -26.93 is -0.00.\n",
      "grad at beta (j = 0, t = 13, l = 2) : -54.01 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 3) : -19.07 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 4) : -58.04 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 5) : 14.88 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 6) : -1.24 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 7) : -59.60 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 8) : 8.79 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 9) : 18.15 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 10) : 2.11 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 11) : -0.35 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 12) : -0.51 is -0.00.\n",
      "grad at beta (j = 0, t = 13, l = 13) : 0.32 is -0.00.\n",
      "grad at beta (j = 0, t = 13, l = 14) : 0.64 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 15) : 1.45 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 16) : 1.12 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 17) : 2.93 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 18) : 2.06 is -0.00.\n",
      "grad at beta (j = 0, t = 13, l = 19) : 2.16 is -0.00.\n",
      "grad at beta (j = 0, t = 13, l = 20) : 2.56 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 21) : 1.29 is -0.00.\n",
      "grad at beta (j = 0, t = 13, l = 22) : -0.48 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 23) : -0.01 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 24) : 7.23 is -0.00.\n",
      "grad at beta (j = 0, t = 13, l = 25) : 0.55 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 26) : -0.36 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 27) : 0.48 is 0.00.\n",
      "grad at beta (j = 0, t = 13, l = 28) : 0.40 is -0.00.\n",
      "grad at beta (j = 0, t = 13, l = 29) : 1.18 is 0.00.\n",
      "grad at beta (j = 0, t = 14, l = 0) : -32.89 is 0.00.\n",
      "grad at beta (j = 0, t = 14, l = 1) : -8.20 is -0.00.\n",
      "grad at beta (j = 0, t = 14, l = 2) : -43.18 is 0.00.\n",
      "grad at beta (j = 0, t = 14, l = 3) : -40.34 is 0.00.\n",
      "grad at beta (j = 0, t = 14, l = 4) : -39.78 is -0.00.\n",
      "grad at beta (j = 0, t = 14, l = 5) : 26.56 is -0.00.\n",
      "grad at beta (j = 0, t = 14, l = 6) : -70.98 is 0.00.\n",
      "grad at beta (j = 0, t = 14, l = 7) : -29.29 is -0.00.\n",
      "grad at beta (j = 0, t = 14, l = 8) : 16.22 is -0.00.\n",
      "grad at beta (j = 0, t = 14, l = 9) : 32.42 is 0.00.\n",
      "grad at beta (j = 0, t = 14, l = 10) : 1.84 is 0.00.\n",
      "grad at beta (j = 0, t = 14, l = 11) : 0.90 is -0.00.\n",
      "grad at beta (j = 0, t = 14, l = 12) : 0.26 is -0.00.\n",
      "grad at beta (j = 0, t = 14, l = 13) : 1.38 is -0.00.\n",
      "grad at beta (j = 0, t = 14, l = 14) : 0.94 is 0.00.\n",
      "grad at beta (j = 0, t = 14, l = 15) : -2.37 is 0.00.\n",
      "grad at beta (j = 0, t = 14, l = 16) : 0.48 is -0.00.\n",
      "grad at beta (j = 0, t = 14, l = 17) : 0.08 is 0.00.\n",
      "grad at beta (j = 0, t = 14, l = 18) : 0.14 is -0.00.\n",
      "grad at beta (j = 0, t = 14, l = 19) : 0.73 is 0.00.\n",
      "grad at beta (j = 0, t = 14, l = 20) : 2.38 is 0.00.\n",
      "grad at beta (j = 0, t = 14, l = 21) : 0.65 is 0.00.\n",
      "grad at beta (j = 0, t = 14, l = 22) : 2.15 is -0.00.\n",
      "grad at beta (j = 0, t = 14, l = 23) : 1.81 is -0.00.\n",
      "grad at beta (j = 0, t = 14, l = 24) : -3.95 is -0.00.\n",
      "grad at beta (j = 0, t = 14, l = 25) : -1.71 is -0.00.\n",
      "grad at beta (j = 0, t = 14, l = 26) : -0.52 is 0.00.\n",
      "grad at beta (j = 0, t = 14, l = 27) : 1.25 is 0.00.\n",
      "grad at beta (j = 0, t = 14, l = 28) : 5.11 is -0.00.\n",
      "grad at beta (j = 0, t = 14, l = 29) : 3.78 is 0.00.\n",
      "grad at beta (j = 0, t = 15, l = 0) : -11.86 is -0.00.\n",
      "grad at beta (j = 0, t = 15, l = 1) : 10.36 is 0.00.\n",
      "grad at beta (j = 0, t = 15, l = 2) : -121.65 is -0.00.\n",
      "grad at beta (j = 0, t = 15, l = 3) : -64.21 is -0.00.\n",
      "grad at beta (j = 0, t = 15, l = 4) : 7.30 is -0.00.\n",
      "grad at beta (j = 0, t = 15, l = 5) : -78.78 is -0.00.\n",
      "grad at beta (j = 0, t = 15, l = 6) : 35.11 is -0.00.\n",
      "grad at beta (j = 0, t = 15, l = 7) : 14.74 is -0.00.\n",
      "grad at beta (j = 0, t = 15, l = 8) : 37.44 is 0.00.\n",
      "grad at beta (j = 0, t = 15, l = 9) : 21.87 is -0.00.\n",
      "grad at beta (j = 0, t = 15, l = 10) : 0.68 is 0.00.\n",
      "grad at beta (j = 0, t = 15, l = 11) : 1.67 is 0.00.\n",
      "grad at beta (j = 0, t = 15, l = 12) : -0.14 is 0.00.\n",
      "grad at beta (j = 0, t = 15, l = 13) : 1.93 is -0.00.\n",
      "grad at beta (j = 0, t = 15, l = 14) : -0.72 is 0.00.\n",
      "grad at beta (j = 0, t = 15, l = 15) : -0.03 is 0.00.\n",
      "grad at beta (j = 0, t = 15, l = 16) : 0.62 is -0.00.\n",
      "grad at beta (j = 0, t = 15, l = 17) : -1.40 is 0.00.\n",
      "grad at beta (j = 0, t = 15, l = 18) : -1.47 is -0.00.\n",
      "grad at beta (j = 0, t = 15, l = 19) : -1.32 is -0.00.\n",
      "grad at beta (j = 0, t = 15, l = 20) : -0.25 is 0.00.\n",
      "grad at beta (j = 0, t = 15, l = 21) : 0.02 is -0.00.\n",
      "grad at beta (j = 0, t = 15, l = 22) : -1.05 is 0.00.\n",
      "grad at beta (j = 0, t = 15, l = 23) : -0.44 is 0.00.\n",
      "grad at beta (j = 0, t = 15, l = 24) : -0.77 is 0.00.\n",
      "grad at beta (j = 0, t = 15, l = 25) : -0.02 is 0.00.\n",
      "grad at beta (j = 0, t = 15, l = 26) : -3.02 is 0.00.\n",
      "grad at beta (j = 0, t = 15, l = 27) : -5.02 is -0.00.\n",
      "grad at beta (j = 0, t = 15, l = 28) : -0.55 is 0.00.\n",
      "grad at beta (j = 0, t = 15, l = 29) : 0.18 is 0.00.\n",
      "grad at beta (j = 0, t = 16, l = 0) : 14.85 is -0.00.\n",
      "grad at beta (j = 0, t = 16, l = 1) : 14.68 is -0.00.\n",
      "grad at beta (j = 0, t = 16, l = 2) : -94.79 is 0.00.\n",
      "grad at beta (j = 0, t = 16, l = 3) : -151.14 is -0.00.\n",
      "grad at beta (j = 0, t = 16, l = 4) : -59.94 is 0.00.\n",
      "grad at beta (j = 0, t = 16, l = 5) : -82.92 is -0.00.\n",
      "grad at beta (j = 0, t = 16, l = 6) : -17.55 is 0.00.\n",
      "grad at beta (j = 0, t = 16, l = 7) : 12.36 is 0.00.\n",
      "grad at beta (j = 0, t = 16, l = 8) : 43.03 is -0.00.\n",
      "grad at beta (j = 0, t = 16, l = 9) : 1.58 is -0.00.\n",
      "grad at beta (j = 0, t = 16, l = 10) : 0.22 is -0.00.\n",
      "grad at beta (j = 0, t = 16, l = 11) : 0.67 is -0.00.\n",
      "grad at beta (j = 0, t = 16, l = 12) : -0.15 is -0.00.\n",
      "grad at beta (j = 0, t = 16, l = 13) : -2.11 is 0.00.\n",
      "grad at beta (j = 0, t = 16, l = 14) : -1.45 is -0.00.\n",
      "grad at beta (j = 0, t = 16, l = 15) : -0.21 is -0.00.\n",
      "grad at beta (j = 0, t = 16, l = 16) : 1.42 is 0.00.\n",
      "grad at beta (j = 0, t = 16, l = 17) : 0.33 is -0.00.\n",
      "grad at beta (j = 0, t = 16, l = 18) : 2.35 is 0.00.\n",
      "grad at beta (j = 0, t = 16, l = 19) : -1.05 is 0.00.\n",
      "grad at beta (j = 0, t = 16, l = 20) : 0.98 is -0.00.\n",
      "grad at beta (j = 0, t = 16, l = 21) : -0.53 is 0.00.\n",
      "grad at beta (j = 0, t = 16, l = 22) : -0.62 is -0.00.\n",
      "grad at beta (j = 0, t = 16, l = 23) : -0.52 is 0.00.\n",
      "grad at beta (j = 0, t = 16, l = 24) : -1.58 is -0.00.\n",
      "grad at beta (j = 0, t = 16, l = 25) : 0.41 is 0.00.\n",
      "grad at beta (j = 0, t = 16, l = 26) : -0.11 is -0.00.\n",
      "grad at beta (j = 0, t = 16, l = 27) : -0.02 is 0.00.\n",
      "grad at beta (j = 0, t = 16, l = 28) : 1.17 is 0.00.\n",
      "grad at beta (j = 0, t = 16, l = 29) : 1.41 is -0.00.\n",
      "grad at beta (j = 0, t = 17, l = 0) : -104.57 is -0.00.\n",
      "grad at beta (j = 0, t = 17, l = 1) : 37.04 is -0.00.\n",
      "grad at beta (j = 0, t = 17, l = 2) : -46.09 is 0.00.\n",
      "grad at beta (j = 0, t = 17, l = 3) : -131.81 is -0.00.\n",
      "grad at beta (j = 0, t = 17, l = 4) : -223.91 is 0.00.\n",
      "grad at beta (j = 0, t = 17, l = 5) : -82.55 is -0.00.\n",
      "grad at beta (j = 0, t = 17, l = 6) : -110.30 is 0.00.\n",
      "grad at beta (j = 0, t = 17, l = 7) : -336.50 is 0.00.\n",
      "grad at beta (j = 0, t = 17, l = 8) : 35.65 is 0.00.\n",
      "grad at beta (j = 0, t = 17, l = 9) : 161.90 is -0.00.\n",
      "grad at beta (j = 0, t = 17, l = 10) : 5.45 is 0.00.\n",
      "grad at beta (j = 0, t = 17, l = 11) : 5.27 is -0.00.\n",
      "grad at beta (j = 0, t = 17, l = 12) : 5.54 is -0.00.\n",
      "grad at beta (j = 0, t = 17, l = 13) : 3.89 is -0.00.\n",
      "grad at beta (j = 0, t = 17, l = 14) : -1.20 is 0.00.\n",
      "grad at beta (j = 0, t = 17, l = 15) : -1.26 is -0.00.\n",
      "grad at beta (j = 0, t = 17, l = 16) : -0.81 is 0.00.\n",
      "grad at beta (j = 0, t = 17, l = 17) : -1.49 is 0.00.\n",
      "grad at beta (j = 0, t = 17, l = 18) : -0.25 is -0.00.\n",
      "grad at beta (j = 0, t = 17, l = 19) : 0.59 is -0.00.\n",
      "grad at beta (j = 0, t = 17, l = 20) : 6.22 is -0.00.\n",
      "grad at beta (j = 0, t = 17, l = 21) : 10.90 is -0.00.\n",
      "grad at beta (j = 0, t = 17, l = 22) : 4.02 is 0.00.\n",
      "grad at beta (j = 0, t = 17, l = 23) : 9.78 is 0.00.\n",
      "grad at beta (j = 0, t = 17, l = 24) : 30.87 is 0.00.\n",
      "grad at beta (j = 0, t = 17, l = 25) : 16.59 is 0.00.\n",
      "grad at beta (j = 0, t = 17, l = 26) : 10.03 is 0.00.\n",
      "grad at beta (j = 0, t = 17, l = 27) : 0.01 is -0.00.\n",
      "grad at beta (j = 0, t = 17, l = 28) : -0.06 is 0.00.\n",
      "grad at beta (j = 0, t = 17, l = 29) : 5.26 is -0.00.\n",
      "grad at beta (j = 0, t = 18, l = 0) : -14.85 is 0.00.\n",
      "grad at beta (j = 0, t = 18, l = 1) : 19.91 is -0.00.\n",
      "grad at beta (j = 0, t = 18, l = 2) : -122.80 is -0.00.\n",
      "grad at beta (j = 0, t = 18, l = 3) : 0.23 is 0.00.\n",
      "grad at beta (j = 0, t = 18, l = 4) : -25.10 is 0.00.\n",
      "grad at beta (j = 0, t = 18, l = 5) : -133.80 is 0.00.\n",
      "grad at beta (j = 0, t = 18, l = 6) : 46.72 is 0.00.\n",
      "grad at beta (j = 0, t = 18, l = 7) : -12.82 is -0.00.\n",
      "grad at beta (j = 0, t = 18, l = 8) : 118.53 is -0.00.\n",
      "grad at beta (j = 0, t = 18, l = 9) : 16.14 is 0.00.\n",
      "grad at beta (j = 0, t = 18, l = 10) : -0.52 is 0.00.\n",
      "grad at beta (j = 0, t = 18, l = 11) : -0.77 is -0.00.\n",
      "grad at beta (j = 0, t = 18, l = 12) : 1.99 is -0.00.\n",
      "grad at beta (j = 0, t = 18, l = 13) : -0.16 is -0.00.\n",
      "grad at beta (j = 0, t = 18, l = 14) : 1.32 is -0.00.\n",
      "grad at beta (j = 0, t = 18, l = 15) : -1.37 is -0.00.\n",
      "grad at beta (j = 0, t = 18, l = 16) : -1.22 is -0.00.\n",
      "grad at beta (j = 0, t = 18, l = 17) : -0.90 is 0.00.\n",
      "grad at beta (j = 0, t = 18, l = 18) : -1.41 is 0.00.\n",
      "grad at beta (j = 0, t = 18, l = 19) : 0.02 is 0.00.\n",
      "grad at beta (j = 0, t = 18, l = 20) : -0.43 is 0.00.\n",
      "grad at beta (j = 0, t = 18, l = 21) : -0.27 is -0.00.\n",
      "grad at beta (j = 0, t = 18, l = 22) : -1.24 is 0.00.\n",
      "grad at beta (j = 0, t = 18, l = 23) : -2.38 is -0.00.\n",
      "grad at beta (j = 0, t = 18, l = 24) : -0.53 is 0.00.\n",
      "grad at beta (j = 0, t = 18, l = 25) : 0.80 is -0.00.\n",
      "grad at beta (j = 0, t = 18, l = 26) : -0.62 is 0.00.\n",
      "grad at beta (j = 0, t = 18, l = 27) : 1.54 is -0.00.\n",
      "grad at beta (j = 0, t = 18, l = 28) : 1.06 is 0.00.\n",
      "grad at beta (j = 0, t = 18, l = 29) : 1.40 is 0.00.\n",
      "grad at beta (j = 0, t = 19, l = 0) : -14.24 is 0.00.\n",
      "grad at beta (j = 0, t = 19, l = 1) : 40.00 is -0.00.\n",
      "grad at beta (j = 0, t = 19, l = 2) : -47.94 is 0.00.\n",
      "grad at beta (j = 0, t = 19, l = 3) : -108.23 is -0.00.\n",
      "grad at beta (j = 0, t = 19, l = 4) : -168.88 is -0.00.\n",
      "grad at beta (j = 0, t = 19, l = 5) : -73.30 is 0.00.\n",
      "grad at beta (j = 0, t = 19, l = 6) : 87.86 is 0.00.\n",
      "grad at beta (j = 0, t = 19, l = 7) : -31.88 is 0.00.\n",
      "grad at beta (j = 0, t = 19, l = 8) : 60.42 is -0.00.\n",
      "grad at beta (j = 0, t = 19, l = 9) : 3.34 is -0.00.\n",
      "grad at beta (j = 0, t = 19, l = 10) : -0.38 is 0.00.\n",
      "grad at beta (j = 0, t = 19, l = 11) : 0.08 is -0.00.\n",
      "grad at beta (j = 0, t = 19, l = 12) : -0.15 is -0.00.\n",
      "grad at beta (j = 0, t = 19, l = 13) : -1.35 is -0.00.\n",
      "grad at beta (j = 0, t = 19, l = 14) : -0.88 is -0.00.\n",
      "grad at beta (j = 0, t = 19, l = 15) : -2.59 is -0.00.\n",
      "grad at beta (j = 0, t = 19, l = 16) : -0.82 is 0.00.\n",
      "grad at beta (j = 0, t = 19, l = 17) : 0.08 is 0.00.\n",
      "grad at beta (j = 0, t = 19, l = 18) : 0.64 is 0.00.\n",
      "grad at beta (j = 0, t = 19, l = 19) : -2.89 is -0.00.\n",
      "grad at beta (j = 0, t = 19, l = 20) : -0.16 is -0.00.\n",
      "grad at beta (j = 0, t = 19, l = 21) : -1.26 is -0.00.\n",
      "grad at beta (j = 0, t = 19, l = 22) : -0.09 is -0.00.\n",
      "grad at beta (j = 0, t = 19, l = 23) : -0.19 is 0.00.\n",
      "grad at beta (j = 0, t = 19, l = 24) : 0.50 is -0.00.\n",
      "grad at beta (j = 0, t = 19, l = 25) : -1.12 is 0.00.\n",
      "grad at beta (j = 0, t = 19, l = 26) : 0.75 is -0.00.\n",
      "grad at beta (j = 0, t = 19, l = 27) : 0.50 is -0.00.\n",
      "grad at beta (j = 0, t = 19, l = 28) : 0.43 is 0.00.\n",
      "grad at beta (j = 0, t = 19, l = 29) : 2.97 is 0.00.\n",
      "grad at beta (j = 0, t = 20, l = 0) : -24.75 is 0.00.\n",
      "grad at beta (j = 0, t = 20, l = 1) : 3.28 is 0.00.\n",
      "grad at beta (j = 0, t = 20, l = 2) : -10.92 is 0.00.\n",
      "grad at beta (j = 0, t = 20, l = 3) : -20.82 is -0.00.\n",
      "grad at beta (j = 0, t = 20, l = 4) : -22.76 is -0.00.\n",
      "grad at beta (j = 0, t = 20, l = 5) : -12.69 is -0.00.\n",
      "grad at beta (j = 0, t = 20, l = 6) : -29.48 is -0.00.\n",
      "grad at beta (j = 0, t = 20, l = 7) : -70.15 is -0.00.\n",
      "grad at beta (j = 0, t = 20, l = 8) : 7.07 is -0.00.\n",
      "grad at beta (j = 0, t = 20, l = 9) : 48.03 is 0.00.\n",
      "grad at beta (j = 0, t = 20, l = 10) : 3.00 is -0.00.\n",
      "grad at beta (j = 0, t = 20, l = 11) : 1.48 is -0.00.\n",
      "grad at beta (j = 0, t = 20, l = 12) : 1.65 is 0.00.\n",
      "grad at beta (j = 0, t = 20, l = 13) : 0.64 is 0.00.\n",
      "grad at beta (j = 0, t = 20, l = 14) : -0.67 is 0.00.\n",
      "grad at beta (j = 0, t = 20, l = 15) : 0.94 is -0.00.\n",
      "grad at beta (j = 0, t = 20, l = 16) : 0.31 is 0.00.\n",
      "grad at beta (j = 0, t = 20, l = 17) : 1.23 is 0.00.\n",
      "grad at beta (j = 0, t = 20, l = 18) : 3.32 is 0.00.\n",
      "grad at beta (j = 0, t = 20, l = 19) : 9.28 is 0.00.\n",
      "grad at beta (j = 0, t = 20, l = 20) : 27.24 is 0.00.\n",
      "grad at beta (j = 0, t = 20, l = 21) : 27.23 is -0.00.\n",
      "grad at beta (j = 0, t = 20, l = 22) : 3.37 is 0.00.\n",
      "grad at beta (j = 0, t = 20, l = 23) : 1.38 is -0.00.\n",
      "grad at beta (j = 0, t = 20, l = 24) : 6.53 is -0.00.\n",
      "grad at beta (j = 0, t = 20, l = 25) : 4.79 is 0.00.\n",
      "grad at beta (j = 0, t = 20, l = 26) : 0.60 is -0.00.\n",
      "grad at beta (j = 0, t = 20, l = 27) : -7.16 is 0.00.\n",
      "grad at beta (j = 0, t = 20, l = 28) : 18.20 is 0.00.\n",
      "grad at beta (j = 0, t = 20, l = 29) : 0.33 is -0.00.\n",
      "grad at beta (j = 0, t = 21, l = 0) : -9.44 is -0.00.\n",
      "grad at beta (j = 0, t = 21, l = 1) : -17.50 is -0.00.\n",
      "grad at beta (j = 0, t = 21, l = 2) : -69.98 is -0.00.\n",
      "grad at beta (j = 0, t = 21, l = 3) : -100.37 is -0.00.\n",
      "grad at beta (j = 0, t = 21, l = 4) : -141.00 is 0.00.\n",
      "grad at beta (j = 0, t = 21, l = 5) : -48.55 is 0.00.\n",
      "grad at beta (j = 0, t = 21, l = 6) : -9.56 is -0.00.\n",
      "grad at beta (j = 0, t = 21, l = 7) : -116.07 is -0.00.\n",
      "grad at beta (j = 0, t = 21, l = 8) : 65.42 is 0.00.\n",
      "grad at beta (j = 0, t = 21, l = 9) : 30.42 is 0.00.\n",
      "grad at beta (j = 0, t = 21, l = 10) : 1.98 is 0.00.\n",
      "grad at beta (j = 0, t = 21, l = 11) : 1.16 is 0.00.\n",
      "grad at beta (j = 0, t = 21, l = 12) : 0.18 is -0.00.\n",
      "grad at beta (j = 0, t = 21, l = 13) : -1.44 is -0.00.\n",
      "grad at beta (j = 0, t = 21, l = 14) : -0.53 is 0.00.\n",
      "grad at beta (j = 0, t = 21, l = 15) : -2.76 is -0.00.\n",
      "grad at beta (j = 0, t = 21, l = 16) : 1.43 is 0.00.\n",
      "grad at beta (j = 0, t = 21, l = 17) : 1.42 is 0.00.\n",
      "grad at beta (j = 0, t = 21, l = 18) : -1.13 is 0.00.\n",
      "grad at beta (j = 0, t = 21, l = 19) : 0.02 is -0.00.\n",
      "grad at beta (j = 0, t = 21, l = 20) : 1.15 is -0.00.\n",
      "grad at beta (j = 0, t = 21, l = 21) : 0.63 is 0.00.\n",
      "grad at beta (j = 0, t = 21, l = 22) : -0.27 is -0.00.\n",
      "grad at beta (j = 0, t = 21, l = 23) : -1.36 is 0.00.\n",
      "grad at beta (j = 0, t = 21, l = 24) : 4.04 is 0.00.\n",
      "grad at beta (j = 0, t = 21, l = 25) : 3.38 is -0.00.\n",
      "grad at beta (j = 0, t = 21, l = 26) : 5.84 is 0.00.\n",
      "grad at beta (j = 0, t = 21, l = 27) : 4.52 is -0.00.\n",
      "grad at beta (j = 0, t = 21, l = 28) : 10.67 is 0.00.\n",
      "grad at beta (j = 0, t = 21, l = 29) : 13.24 is -0.00.\n",
      "grad at beta (j = 0, t = 22, l = 0) : -35.74 is 0.00.\n",
      "grad at beta (j = 0, t = 22, l = 1) : -1.36 is -0.00.\n",
      "grad at beta (j = 0, t = 22, l = 2) : -149.35 is 0.00.\n",
      "grad at beta (j = 0, t = 22, l = 3) : -154.93 is 0.00.\n",
      "grad at beta (j = 0, t = 22, l = 4) : -46.51 is -0.00.\n",
      "grad at beta (j = 0, t = 22, l = 5) : -5.26 is -0.00.\n",
      "grad at beta (j = 0, t = 22, l = 6) : -99.73 is 0.00.\n",
      "grad at beta (j = 0, t = 22, l = 7) : -4.11 is -0.00.\n",
      "grad at beta (j = 0, t = 22, l = 8) : -13.93 is 0.00.\n",
      "grad at beta (j = 0, t = 22, l = 9) : 80.09 is -0.00.\n",
      "grad at beta (j = 0, t = 22, l = 10) : 4.04 is 0.00.\n",
      "grad at beta (j = 0, t = 22, l = 11) : 2.28 is -0.00.\n",
      "grad at beta (j = 0, t = 22, l = 12) : 0.40 is -0.00.\n",
      "grad at beta (j = 0, t = 22, l = 13) : 1.50 is -0.00.\n",
      "grad at beta (j = 0, t = 22, l = 14) : -1.21 is -0.00.\n",
      "grad at beta (j = 0, t = 22, l = 15) : 2.80 is 0.00.\n",
      "grad at beta (j = 0, t = 22, l = 16) : 5.62 is 0.00.\n",
      "grad at beta (j = 0, t = 22, l = 17) : -0.18 is -0.00.\n",
      "grad at beta (j = 0, t = 22, l = 18) : -0.44 is 0.00.\n",
      "grad at beta (j = 0, t = 22, l = 19) : 0.88 is 0.00.\n",
      "grad at beta (j = 0, t = 22, l = 20) : -1.32 is -0.00.\n",
      "grad at beta (j = 0, t = 22, l = 21) : 0.50 is 0.00.\n",
      "grad at beta (j = 0, t = 22, l = 22) : -1.74 is -0.00.\n",
      "grad at beta (j = 0, t = 22, l = 23) : -0.02 is 0.00.\n",
      "grad at beta (j = 0, t = 22, l = 24) : -1.66 is -0.00.\n",
      "grad at beta (j = 0, t = 22, l = 25) : -1.16 is 0.00.\n",
      "grad at beta (j = 0, t = 22, l = 26) : -0.34 is -0.00.\n",
      "grad at beta (j = 0, t = 22, l = 27) : -0.02 is 0.00.\n",
      "grad at beta (j = 0, t = 22, l = 28) : 1.75 is 0.00.\n",
      "grad at beta (j = 0, t = 22, l = 29) : 6.45 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 0) : -42.95 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 1) : 2.47 is -0.00.\n",
      "grad at beta (j = 0, t = 23, l = 2) : -228.59 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 3) : -150.14 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 4) : -54.87 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 5) : -244.47 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 6) : 27.07 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 7) : -1.38 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 8) : -40.36 is -0.00.\n",
      "grad at beta (j = 0, t = 23, l = 9) : 79.31 is -0.00.\n",
      "grad at beta (j = 0, t = 23, l = 10) : 6.44 is -0.00.\n",
      "grad at beta (j = 0, t = 23, l = 11) : 3.48 is -0.00.\n",
      "grad at beta (j = 0, t = 23, l = 12) : 0.35 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 13) : -0.35 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 14) : -0.32 is -0.00.\n",
      "grad at beta (j = 0, t = 23, l = 15) : 2.31 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 16) : 0.04 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 17) : 0.88 is -0.00.\n",
      "grad at beta (j = 0, t = 23, l = 18) : -0.49 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 19) : -0.90 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 20) : -0.65 is -0.00.\n",
      "grad at beta (j = 0, t = 23, l = 21) : -1.05 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 22) : -1.84 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 23) : -2.41 is -0.00.\n",
      "grad at beta (j = 0, t = 23, l = 24) : -4.48 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 25) : -0.20 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 26) : 4.97 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 27) : 0.76 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 28) : 1.74 is 0.00.\n",
      "grad at beta (j = 0, t = 23, l = 29) : 2.60 is -0.00.\n",
      "grad at beta (j = 0, t = 24, l = 0) : -46.40 is -0.00.\n",
      "grad at beta (j = 0, t = 24, l = 1) : -53.72 is 0.00.\n",
      "grad at beta (j = 0, t = 24, l = 2) : -161.91 is -0.00.\n",
      "grad at beta (j = 0, t = 24, l = 3) : -92.56 is 0.00.\n",
      "grad at beta (j = 0, t = 24, l = 4) : -26.63 is 0.00.\n",
      "grad at beta (j = 0, t = 24, l = 5) : -91.63 is -0.00.\n",
      "grad at beta (j = 0, t = 24, l = 6) : -81.55 is -0.00.\n",
      "grad at beta (j = 0, t = 24, l = 7) : -62.50 is 0.00.\n",
      "grad at beta (j = 0, t = 24, l = 8) : -55.30 is 0.00.\n",
      "grad at beta (j = 0, t = 24, l = 9) : 134.28 is 0.00.\n",
      "grad at beta (j = 0, t = 24, l = 10) : 7.03 is -0.00.\n",
      "grad at beta (j = 0, t = 24, l = 11) : 2.24 is 0.00.\n",
      "grad at beta (j = 0, t = 24, l = 12) : 0.99 is -0.00.\n",
      "grad at beta (j = 0, t = 24, l = 13) : 2.24 is 0.00.\n",
      "grad at beta (j = 0, t = 24, l = 14) : 0.95 is 0.00.\n",
      "grad at beta (j = 0, t = 24, l = 15) : 0.67 is -0.00.\n",
      "grad at beta (j = 0, t = 24, l = 16) : 2.17 is 0.00.\n",
      "grad at beta (j = 0, t = 24, l = 17) : 4.79 is 0.00.\n",
      "grad at beta (j = 0, t = 24, l = 18) : 4.39 is -0.00.\n",
      "grad at beta (j = 0, t = 24, l = 19) : 1.21 is -0.00.\n",
      "grad at beta (j = 0, t = 24, l = 20) : 1.66 is -0.00.\n",
      "grad at beta (j = 0, t = 24, l = 21) : 9.96 is 0.00.\n",
      "grad at beta (j = 0, t = 24, l = 22) : 0.08 is -0.00.\n",
      "grad at beta (j = 0, t = 24, l = 23) : 1.67 is 0.00.\n",
      "grad at beta (j = 0, t = 24, l = 24) : 15.17 is 0.00.\n",
      "grad at beta (j = 0, t = 24, l = 25) : -0.49 is 0.00.\n",
      "grad at beta (j = 0, t = 24, l = 26) : 6.49 is -0.00.\n",
      "grad at beta (j = 0, t = 24, l = 27) : -0.63 is -0.00.\n",
      "grad at beta (j = 0, t = 24, l = 28) : -0.56 is -0.00.\n",
      "grad at beta (j = 0, t = 24, l = 29) : 0.72 is -0.00.\n",
      "grad at beta (j = 0, t = 25, l = 0) : -26.72 is -0.00.\n",
      "grad at beta (j = 0, t = 25, l = 1) : -10.89 is -0.00.\n",
      "grad at beta (j = 0, t = 25, l = 2) : -6.52 is -0.00.\n",
      "grad at beta (j = 0, t = 25, l = 3) : -15.58 is 0.00.\n",
      "grad at beta (j = 0, t = 25, l = 4) : -82.70 is 0.00.\n",
      "grad at beta (j = 0, t = 25, l = 5) : -45.18 is -0.00.\n",
      "grad at beta (j = 0, t = 25, l = 6) : 114.82 is 0.00.\n",
      "grad at beta (j = 0, t = 25, l = 7) : -24.70 is 0.00.\n",
      "grad at beta (j = 0, t = 25, l = 8) : 12.82 is -0.00.\n",
      "grad at beta (j = 0, t = 25, l = 9) : -22.90 is 0.00.\n",
      "grad at beta (j = 0, t = 25, l = 10) : -1.61 is 0.00.\n",
      "grad at beta (j = 0, t = 25, l = 11) : -0.65 is -0.00.\n",
      "grad at beta (j = 0, t = 25, l = 12) : -0.33 is 0.00.\n",
      "grad at beta (j = 0, t = 25, l = 13) : -1.75 is -0.00.\n",
      "grad at beta (j = 0, t = 25, l = 14) : -3.06 is -0.00.\n",
      "grad at beta (j = 0, t = 25, l = 15) : -1.36 is -0.00.\n",
      "grad at beta (j = 0, t = 25, l = 16) : 0.74 is 0.00.\n",
      "grad at beta (j = 0, t = 25, l = 17) : 0.72 is -0.00.\n",
      "grad at beta (j = 0, t = 25, l = 18) : 0.20 is 0.00.\n",
      "grad at beta (j = 0, t = 25, l = 19) : 0.64 is -0.00.\n",
      "grad at beta (j = 0, t = 25, l = 20) : -1.11 is 0.00.\n",
      "grad at beta (j = 0, t = 25, l = 21) : 0.20 is -0.00.\n",
      "grad at beta (j = 0, t = 25, l = 22) : -1.42 is 0.00.\n",
      "grad at beta (j = 0, t = 25, l = 23) : -1.64 is -0.00.\n",
      "grad at beta (j = 0, t = 25, l = 24) : 0.23 is -0.00.\n",
      "grad at beta (j = 0, t = 25, l = 25) : -0.32 is 0.00.\n",
      "grad at beta (j = 0, t = 25, l = 26) : -1.39 is -0.00.\n",
      "grad at beta (j = 0, t = 25, l = 27) : -0.26 is -0.00.\n",
      "grad at beta (j = 0, t = 25, l = 28) : -1.62 is 0.00.\n",
      "grad at beta (j = 0, t = 25, l = 29) : 0.33 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 0) : 8.40 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 1) : 5.57 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 2) : -56.53 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 3) : -71.73 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 4) : -22.14 is -0.00.\n",
      "grad at beta (j = 0, t = 26, l = 5) : -67.06 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 6) : 25.74 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 7) : 64.60 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 8) : 99.50 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 9) : 3.67 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 10) : -0.97 is -0.00.\n",
      "grad at beta (j = 0, t = 26, l = 11) : -0.86 is -0.00.\n",
      "grad at beta (j = 0, t = 26, l = 12) : -0.03 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 13) : 1.43 is -0.00.\n",
      "grad at beta (j = 0, t = 26, l = 14) : -0.48 is -0.00.\n",
      "grad at beta (j = 0, t = 26, l = 15) : 0.84 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 16) : 0.43 is -0.00.\n",
      "grad at beta (j = 0, t = 26, l = 17) : 1.52 is -0.00.\n",
      "grad at beta (j = 0, t = 26, l = 18) : 2.05 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 19) : 1.78 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 20) : -1.10 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 21) : -1.01 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 22) : -0.71 is -0.00.\n",
      "grad at beta (j = 0, t = 26, l = 23) : 0.45 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 24) : 1.01 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 25) : 1.39 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 26) : -0.54 is -0.00.\n",
      "grad at beta (j = 0, t = 26, l = 27) : -1.12 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 28) : 0.30 is 0.00.\n",
      "grad at beta (j = 0, t = 26, l = 29) : 3.06 is 0.00.\n",
      "grad at beta (j = 0, t = 27, l = 0) : -76.12 is 0.00.\n",
      "grad at beta (j = 0, t = 27, l = 1) : -15.78 is -0.00.\n",
      "grad at beta (j = 0, t = 27, l = 2) : -53.96 is 0.00.\n",
      "grad at beta (j = 0, t = 27, l = 3) : -191.78 is -0.00.\n",
      "grad at beta (j = 0, t = 27, l = 4) : -80.95 is -0.00.\n",
      "grad at beta (j = 0, t = 27, l = 5) : 61.59 is -0.00.\n",
      "grad at beta (j = 0, t = 27, l = 6) : 7.91 is 0.00.\n",
      "grad at beta (j = 0, t = 27, l = 7) : -52.03 is -0.00.\n",
      "grad at beta (j = 0, t = 27, l = 8) : 10.83 is -0.00.\n",
      "grad at beta (j = 0, t = 27, l = 9) : 1.57 is -0.00.\n",
      "grad at beta (j = 0, t = 27, l = 10) : 1.18 is 0.00.\n",
      "grad at beta (j = 0, t = 27, l = 11) : 0.15 is 0.00.\n",
      "grad at beta (j = 0, t = 27, l = 12) : 0.92 is 0.00.\n",
      "grad at beta (j = 0, t = 27, l = 13) : 1.64 is -0.00.\n",
      "grad at beta (j = 0, t = 27, l = 14) : -0.11 is 0.00.\n",
      "grad at beta (j = 0, t = 27, l = 15) : 1.02 is -0.00.\n",
      "grad at beta (j = 0, t = 27, l = 16) : -1.47 is -0.00.\n",
      "grad at beta (j = 0, t = 27, l = 17) : -1.08 is -0.00.\n",
      "grad at beta (j = 0, t = 27, l = 18) : -1.15 is -0.00.\n",
      "grad at beta (j = 0, t = 27, l = 19) : -1.94 is -0.00.\n",
      "grad at beta (j = 0, t = 27, l = 20) : -3.05 is -0.00.\n",
      "grad at beta (j = 0, t = 27, l = 21) : -4.97 is -0.00.\n",
      "grad at beta (j = 0, t = 27, l = 22) : -0.86 is 0.00.\n",
      "grad at beta (j = 0, t = 27, l = 23) : -0.90 is 0.00.\n",
      "grad at beta (j = 0, t = 27, l = 24) : 1.79 is 0.00.\n",
      "grad at beta (j = 0, t = 27, l = 25) : 0.01 is -0.00.\n",
      "grad at beta (j = 0, t = 27, l = 26) : 2.42 is 0.00.\n",
      "grad at beta (j = 0, t = 27, l = 27) : -1.53 is 0.00.\n",
      "grad at beta (j = 0, t = 27, l = 28) : 3.47 is 0.00.\n",
      "grad at beta (j = 0, t = 27, l = 29) : 7.70 is -0.00.\n",
      "grad at beta (j = 0, t = 28, l = 0) : -20.93 is -0.00.\n",
      "grad at beta (j = 0, t = 28, l = 1) : 31.02 is 0.00.\n",
      "grad at beta (j = 0, t = 28, l = 2) : -21.16 is 0.00.\n",
      "grad at beta (j = 0, t = 28, l = 3) : -22.06 is -0.00.\n",
      "grad at beta (j = 0, t = 28, l = 4) : -241.39 is -0.00.\n",
      "grad at beta (j = 0, t = 28, l = 5) : -165.19 is -0.00.\n",
      "grad at beta (j = 0, t = 28, l = 6) : 71.42 is 0.00.\n",
      "grad at beta (j = 0, t = 28, l = 7) : 7.40 is 0.00.\n",
      "grad at beta (j = 0, t = 28, l = 8) : 155.38 is 0.00.\n",
      "grad at beta (j = 0, t = 28, l = 9) : -12.16 is -0.00.\n",
      "grad at beta (j = 0, t = 28, l = 10) : -1.90 is -0.00.\n",
      "grad at beta (j = 0, t = 28, l = 11) : 0.42 is 0.00.\n",
      "grad at beta (j = 0, t = 28, l = 12) : -0.25 is -0.00.\n",
      "grad at beta (j = 0, t = 28, l = 13) : 0.16 is -0.00.\n",
      "grad at beta (j = 0, t = 28, l = 14) : 1.11 is 0.00.\n",
      "grad at beta (j = 0, t = 28, l = 15) : 0.53 is -0.00.\n",
      "grad at beta (j = 0, t = 28, l = 16) : -0.41 is 0.00.\n",
      "grad at beta (j = 0, t = 28, l = 17) : 0.82 is 0.00.\n",
      "grad at beta (j = 0, t = 28, l = 18) : 1.09 is 0.00.\n",
      "grad at beta (j = 0, t = 28, l = 19) : -2.23 is 0.00.\n",
      "grad at beta (j = 0, t = 28, l = 20) : 0.09 is -0.00.\n",
      "grad at beta (j = 0, t = 28, l = 21) : -0.19 is -0.00.\n",
      "grad at beta (j = 0, t = 28, l = 22) : 2.65 is 0.00.\n",
      "grad at beta (j = 0, t = 28, l = 23) : 6.03 is -0.00.\n",
      "grad at beta (j = 0, t = 28, l = 24) : 3.33 is -0.00.\n",
      "grad at beta (j = 0, t = 28, l = 25) : 6.80 is 0.00.\n",
      "grad at beta (j = 0, t = 28, l = 26) : 5.32 is -0.00.\n",
      "grad at beta (j = 0, t = 28, l = 27) : -0.16 is -0.00.\n",
      "grad at beta (j = 0, t = 28, l = 28) : 4.10 is 0.00.\n",
      "grad at beta (j = 0, t = 28, l = 29) : 1.29 is 0.00.\n",
      "grad at beta (j = 0, t = 29, l = 0) : -4.42 is 0.00.\n",
      "grad at beta (j = 0, t = 29, l = 1) : -7.83 is 0.00.\n",
      "grad at beta (j = 0, t = 29, l = 2) : -11.62 is 0.00.\n",
      "grad at beta (j = 0, t = 29, l = 3) : -19.74 is 0.00.\n",
      "grad at beta (j = 0, t = 29, l = 4) : -23.34 is 0.00.\n",
      "grad at beta (j = 0, t = 29, l = 5) : -25.86 is 0.00.\n",
      "grad at beta (j = 0, t = 29, l = 6) : -17.92 is -0.00.\n",
      "grad at beta (j = 0, t = 29, l = 7) : -38.53 is 0.00.\n",
      "grad at beta (j = 0, t = 29, l = 8) : 15.05 is -0.00.\n",
      "grad at beta (j = 0, t = 29, l = 9) : 16.48 is -0.00.\n",
      "grad at beta (j = 0, t = 29, l = 10) : -0.50 is 0.00.\n",
      "grad at beta (j = 0, t = 29, l = 11) : 0.97 is 0.00.\n",
      "grad at beta (j = 0, t = 29, l = 12) : 2.38 is 0.00.\n",
      "grad at beta (j = 0, t = 29, l = 13) : 1.76 is 0.00.\n",
      "grad at beta (j = 0, t = 29, l = 14) : -0.08 is -0.00.\n",
      "grad at beta (j = 0, t = 29, l = 15) : 7.44 is 0.00.\n",
      "grad at beta (j = 0, t = 29, l = 16) : 4.49 is -0.00.\n",
      "grad at beta (j = 0, t = 29, l = 17) : 0.06 is -0.00.\n",
      "grad at beta (j = 0, t = 29, l = 18) : 0.22 is -0.00.\n",
      "grad at beta (j = 0, t = 29, l = 19) : -0.78 is 0.00.\n",
      "grad at beta (j = 0, t = 29, l = 20) : -1.62 is 0.00.\n",
      "grad at beta (j = 0, t = 29, l = 21) : 1.07 is 0.00.\n",
      "grad at beta (j = 0, t = 29, l = 22) : 1.33 is -0.00.\n",
      "grad at beta (j = 0, t = 29, l = 23) : -2.53 is -0.00.\n",
      "grad at beta (j = 0, t = 29, l = 24) : -2.25 is 0.00.\n",
      "grad at beta (j = 0, t = 29, l = 25) : -1.04 is -0.00.\n",
      "grad at beta (j = 0, t = 29, l = 26) : 0.58 is -0.00.\n",
      "grad at beta (j = 0, t = 29, l = 27) : 0.82 is 0.00.\n",
      "grad at beta (j = 0, t = 29, l = 28) : 0.69 is -0.00.\n",
      "grad at beta (j = 0, t = 29, l = 29) : -0.46 is -0.00.\n",
      "grad at beta (j = 1, t = 0, l = 0) : -153.64 is 0.00.\n",
      "grad at beta (j = 1, t = 0, l = 1) : -81.37 is 0.00.\n",
      "grad at beta (j = 1, t = 0, l = 2) : -715.15 is -0.00.\n",
      "grad at beta (j = 1, t = 0, l = 3) : -352.41 is -0.00.\n",
      "grad at beta (j = 1, t = 0, l = 4) : -20.17 is -0.00.\n",
      "grad at beta (j = 1, t = 0, l = 5) : -236.32 is -0.00.\n",
      "grad at beta (j = 1, t = 0, l = 6) : 7.17 is 0.00.\n",
      "grad at beta (j = 1, t = 0, l = 7) : 8.47 is -0.00.\n",
      "grad at beta (j = 1, t = 0, l = 8) : 29.64 is -0.00.\n",
      "grad at beta (j = 1, t = 0, l = 9) : 38.47 is -0.00.\n",
      "grad at beta (j = 1, t = 0, l = 10) : 3.07 is -0.00.\n",
      "grad at beta (j = 1, t = 0, l = 11) : 0.22 is 0.00.\n",
      "grad at beta (j = 1, t = 0, l = 12) : 0.78 is 0.00.\n",
      "grad at beta (j = 1, t = 0, l = 13) : 1.91 is -0.00.\n",
      "grad at beta (j = 1, t = 0, l = 14) : 1.95 is 0.00.\n",
      "grad at beta (j = 1, t = 0, l = 15) : 1.85 is -0.00.\n",
      "grad at beta (j = 1, t = 0, l = 16) : -0.19 is 0.00.\n",
      "grad at beta (j = 1, t = 0, l = 17) : -1.64 is 0.00.\n",
      "grad at beta (j = 1, t = 0, l = 18) : -1.82 is 0.00.\n",
      "grad at beta (j = 1, t = 0, l = 19) : -5.56 is 0.00.\n",
      "grad at beta (j = 1, t = 0, l = 20) : -2.13 is 0.00.\n",
      "grad at beta (j = 1, t = 0, l = 21) : -3.35 is -0.00.\n",
      "grad at beta (j = 1, t = 0, l = 22) : -7.25 is -0.00.\n",
      "grad at beta (j = 1, t = 0, l = 23) : -8.75 is -0.00.\n",
      "grad at beta (j = 1, t = 0, l = 24) : -8.24 is -0.00.\n",
      "grad at beta (j = 1, t = 0, l = 25) : 5.17 is -0.00.\n",
      "grad at beta (j = 1, t = 0, l = 26) : 11.32 is 0.00.\n",
      "grad at beta (j = 1, t = 0, l = 27) : 0.69 is -0.00.\n",
      "grad at beta (j = 1, t = 0, l = 28) : 2.14 is -0.00.\n",
      "grad at beta (j = 1, t = 0, l = 29) : 9.14 is 0.00.\n",
      "grad at beta (j = 1, t = 1, l = 0) : -130.96 is -0.00.\n",
      "grad at beta (j = 1, t = 1, l = 1) : -35.60 is 0.00.\n",
      "grad at beta (j = 1, t = 1, l = 2) : -240.23 is 0.00.\n",
      "grad at beta (j = 1, t = 1, l = 3) : -104.78 is -0.00.\n",
      "grad at beta (j = 1, t = 1, l = 4) : -202.09 is 0.00.\n",
      "grad at beta (j = 1, t = 1, l = 5) : -287.72 is -0.00.\n",
      "grad at beta (j = 1, t = 1, l = 6) : -87.02 is -0.00.\n",
      "grad at beta (j = 1, t = 1, l = 7) : -113.59 is -0.00.\n",
      "grad at beta (j = 1, t = 1, l = 8) : 36.43 is 0.00.\n",
      "grad at beta (j = 1, t = 1, l = 9) : 146.39 is 0.00.\n",
      "grad at beta (j = 1, t = 1, l = 10) : 10.03 is 0.00.\n",
      "grad at beta (j = 1, t = 1, l = 11) : 3.32 is 0.00.\n",
      "grad at beta (j = 1, t = 1, l = 12) : 1.99 is -0.00.\n",
      "grad at beta (j = 1, t = 1, l = 13) : -0.59 is -0.00.\n",
      "grad at beta (j = 1, t = 1, l = 14) : -0.29 is 0.00.\n",
      "grad at beta (j = 1, t = 1, l = 15) : -3.70 is 0.00.\n",
      "grad at beta (j = 1, t = 1, l = 16) : 0.75 is 0.00.\n",
      "grad at beta (j = 1, t = 1, l = 17) : 0.53 is 0.00.\n",
      "grad at beta (j = 1, t = 1, l = 18) : -1.59 is 0.00.\n",
      "grad at beta (j = 1, t = 1, l = 19) : -2.02 is 0.00.\n",
      "grad at beta (j = 1, t = 1, l = 20) : 0.18 is -0.00.\n",
      "grad at beta (j = 1, t = 1, l = 21) : -0.42 is -0.00.\n",
      "grad at beta (j = 1, t = 1, l = 22) : 0.55 is 0.00.\n",
      "grad at beta (j = 1, t = 1, l = 23) : -0.75 is -0.00.\n",
      "grad at beta (j = 1, t = 1, l = 24) : 0.18 is -0.00.\n",
      "grad at beta (j = 1, t = 1, l = 25) : 2.05 is 0.00.\n",
      "grad at beta (j = 1, t = 1, l = 26) : -1.12 is 0.00.\n",
      "grad at beta (j = 1, t = 1, l = 27) : -1.64 is 0.00.\n",
      "grad at beta (j = 1, t = 1, l = 28) : -0.02 is 0.00.\n",
      "grad at beta (j = 1, t = 1, l = 29) : 1.30 is 0.00.\n",
      "grad at beta (j = 1, t = 2, l = 0) : -67.11 is 0.00.\n",
      "grad at beta (j = 1, t = 2, l = 1) : -27.79 is -0.00.\n",
      "grad at beta (j = 1, t = 2, l = 2) : -437.21 is -0.00.\n",
      "grad at beta (j = 1, t = 2, l = 3) : -6.73 is 0.00.\n",
      "grad at beta (j = 1, t = 2, l = 4) : -225.35 is 0.00.\n",
      "grad at beta (j = 1, t = 2, l = 5) : -525.25 is -0.00.\n",
      "grad at beta (j = 1, t = 2, l = 6) : 213.80 is -0.00.\n",
      "grad at beta (j = 1, t = 2, l = 7) : 112.85 is -0.00.\n",
      "grad at beta (j = 1, t = 2, l = 8) : 478.04 is -0.00.\n",
      "grad at beta (j = 1, t = 2, l = 9) : -92.91 is 0.00.\n",
      "grad at beta (j = 1, t = 2, l = 10) : -1.01 is -0.00.\n",
      "grad at beta (j = 1, t = 2, l = 11) : -1.66 is 0.00.\n",
      "grad at beta (j = 1, t = 2, l = 12) : 1.50 is 0.00.\n",
      "grad at beta (j = 1, t = 2, l = 13) : -0.29 is 0.00.\n",
      "grad at beta (j = 1, t = 2, l = 14) : -0.01 is 0.00.\n",
      "grad at beta (j = 1, t = 2, l = 15) : -0.69 is 0.00.\n",
      "grad at beta (j = 1, t = 2, l = 16) : -0.60 is 0.00.\n",
      "grad at beta (j = 1, t = 2, l = 17) : -7.26 is -0.00.\n",
      "grad at beta (j = 1, t = 2, l = 18) : -2.58 is 0.00.\n",
      "grad at beta (j = 1, t = 2, l = 19) : -4.29 is 0.00.\n",
      "grad at beta (j = 1, t = 2, l = 20) : 7.18 is 0.00.\n",
      "grad at beta (j = 1, t = 2, l = 21) : 1.88 is -0.00.\n",
      "grad at beta (j = 1, t = 2, l = 22) : -0.51 is 0.00.\n",
      "grad at beta (j = 1, t = 2, l = 23) : -0.05 is 0.00.\n",
      "grad at beta (j = 1, t = 2, l = 24) : -1.48 is -0.00.\n",
      "grad at beta (j = 1, t = 2, l = 25) : 1.28 is -0.00.\n",
      "grad at beta (j = 1, t = 2, l = 26) : -1.34 is -0.00.\n",
      "grad at beta (j = 1, t = 2, l = 27) : -5.72 is 0.00.\n",
      "grad at beta (j = 1, t = 2, l = 28) : -0.02 is -0.00.\n",
      "grad at beta (j = 1, t = 2, l = 29) : 5.79 is 0.00.\n",
      "grad at beta (j = 1, t = 3, l = 0) : -75.54 is -0.00.\n",
      "grad at beta (j = 1, t = 3, l = 1) : -1470.63 is -0.00.\n",
      "grad at beta (j = 1, t = 3, l = 2) : -344.15 is -0.00.\n",
      "grad at beta (j = 1, t = 3, l = 3) : -17.17 is 0.00.\n",
      "grad at beta (j = 1, t = 3, l = 4) : -323.62 is 0.00.\n",
      "grad at beta (j = 1, t = 3, l = 5) : -241.17 is -0.00.\n",
      "grad at beta (j = 1, t = 3, l = 6) : 139.92 is 0.00.\n",
      "grad at beta (j = 1, t = 3, l = 7) : -28.67 is -0.00.\n",
      "grad at beta (j = 1, t = 3, l = 8) : 4.70 is -0.00.\n",
      "grad at beta (j = 1, t = 3, l = 9) : -11.86 is -0.00.\n",
      "grad at beta (j = 1, t = 3, l = 10) : 1.99 is -0.00.\n",
      "grad at beta (j = 1, t = 3, l = 11) : 0.64 is -0.00.\n",
      "grad at beta (j = 1, t = 3, l = 12) : -0.64 is -0.00.\n",
      "grad at beta (j = 1, t = 3, l = 13) : -0.35 is -0.00.\n",
      "grad at beta (j = 1, t = 3, l = 14) : 0.37 is 0.00.\n",
      "grad at beta (j = 1, t = 3, l = 15) : -2.10 is -0.00.\n",
      "grad at beta (j = 1, t = 3, l = 16) : -2.27 is -0.00.\n",
      "grad at beta (j = 1, t = 3, l = 17) : -6.56 is 0.00.\n",
      "grad at beta (j = 1, t = 3, l = 18) : -12.45 is 0.00.\n",
      "grad at beta (j = 1, t = 3, l = 19) : -18.41 is -0.00.\n",
      "grad at beta (j = 1, t = 3, l = 20) : -23.78 is 0.00.\n",
      "grad at beta (j = 1, t = 3, l = 21) : -2.31 is -0.00.\n",
      "grad at beta (j = 1, t = 3, l = 22) : -9.94 is 0.00.\n",
      "grad at beta (j = 1, t = 3, l = 23) : -0.86 is 0.00.\n",
      "grad at beta (j = 1, t = 3, l = 24) : 1.61 is 0.00.\n",
      "grad at beta (j = 1, t = 3, l = 25) : -1.82 is -0.00.\n",
      "grad at beta (j = 1, t = 3, l = 26) : 8.59 is -0.00.\n",
      "grad at beta (j = 1, t = 3, l = 27) : 3.69 is 0.00.\n",
      "grad at beta (j = 1, t = 3, l = 28) : 43.12 is -0.00.\n",
      "grad at beta (j = 1, t = 3, l = 29) : 23.53 is 0.00.\n",
      "grad at beta (j = 1, t = 4, l = 0) : -394.00 is 0.00.\n",
      "grad at beta (j = 1, t = 4, l = 1) : -224.87 is -0.00.\n",
      "grad at beta (j = 1, t = 4, l = 2) : -575.49 is 0.00.\n",
      "grad at beta (j = 1, t = 4, l = 3) : -375.38 is 0.00.\n",
      "grad at beta (j = 1, t = 4, l = 4) : -1822.51 is 0.00.\n",
      "grad at beta (j = 1, t = 4, l = 5) : -1916.19 is 0.00.\n",
      "grad at beta (j = 1, t = 4, l = 6) : -334.98 is -0.00.\n",
      "grad at beta (j = 1, t = 4, l = 7) : -2174.49 is -0.00.\n",
      "grad at beta (j = 1, t = 4, l = 8) : 114.58 is -0.00.\n",
      "grad at beta (j = 1, t = 4, l = 9) : 663.31 is 0.00.\n",
      "grad at beta (j = 1, t = 4, l = 10) : 44.12 is -0.00.\n",
      "grad at beta (j = 1, t = 4, l = 11) : 19.14 is 0.00.\n",
      "grad at beta (j = 1, t = 4, l = 12) : 21.06 is 0.00.\n",
      "grad at beta (j = 1, t = 4, l = 13) : 12.09 is 0.00.\n",
      "grad at beta (j = 1, t = 4, l = 14) : 5.79 is -0.00.\n",
      "grad at beta (j = 1, t = 4, l = 15) : 8.24 is -0.00.\n",
      "grad at beta (j = 1, t = 4, l = 16) : 17.00 is -0.00.\n",
      "grad at beta (j = 1, t = 4, l = 17) : 29.49 is 0.00.\n",
      "grad at beta (j = 1, t = 4, l = 18) : 11.40 is 0.00.\n",
      "grad at beta (j = 1, t = 4, l = 19) : 0.17 is -0.00.\n",
      "grad at beta (j = 1, t = 4, l = 20) : -7.43 is 0.00.\n",
      "grad at beta (j = 1, t = 4, l = 21) : 0.68 is -0.00.\n",
      "grad at beta (j = 1, t = 4, l = 22) : 0.09 is 0.00.\n",
      "grad at beta (j = 1, t = 4, l = 23) : 8.81 is -0.00.\n",
      "grad at beta (j = 1, t = 4, l = 24) : 0.16 is 0.00.\n",
      "grad at beta (j = 1, t = 4, l = 25) : 37.11 is 0.00.\n",
      "grad at beta (j = 1, t = 4, l = 26) : 93.96 is 0.00.\n",
      "grad at beta (j = 1, t = 4, l = 27) : 57.53 is 0.00.\n",
      "grad at beta (j = 1, t = 4, l = 28) : 90.69 is 0.00.\n",
      "grad at beta (j = 1, t = 4, l = 29) : 48.04 is -0.00.\n",
      "grad at beta (j = 1, t = 5, l = 0) : -74.12 is 0.00.\n",
      "grad at beta (j = 1, t = 5, l = 1) : 110.56 is 0.00.\n",
      "grad at beta (j = 1, t = 5, l = 2) : 3.12 is -0.00.\n",
      "grad at beta (j = 1, t = 5, l = 3) : -151.74 is 0.00.\n",
      "grad at beta (j = 1, t = 5, l = 4) : -262.95 is -0.00.\n",
      "grad at beta (j = 1, t = 5, l = 5) : -252.26 is 0.00.\n",
      "grad at beta (j = 1, t = 5, l = 6) : 446.98 is 0.00.\n",
      "grad at beta (j = 1, t = 5, l = 7) : -29.07 is 0.00.\n",
      "grad at beta (j = 1, t = 5, l = 8) : 63.60 is 0.00.\n",
      "grad at beta (j = 1, t = 5, l = 9) : -38.06 is -0.00.\n",
      "grad at beta (j = 1, t = 5, l = 10) : -0.52 is -0.00.\n",
      "grad at beta (j = 1, t = 5, l = 11) : -0.81 is -0.00.\n",
      "grad at beta (j = 1, t = 5, l = 12) : -1.32 is -0.00.\n",
      "grad at beta (j = 1, t = 5, l = 13) : -0.16 is -0.00.\n",
      "grad at beta (j = 1, t = 5, l = 14) : -0.49 is -0.00.\n",
      "grad at beta (j = 1, t = 5, l = 15) : 0.28 is -0.00.\n",
      "grad at beta (j = 1, t = 5, l = 16) : -2.45 is -0.00.\n",
      "grad at beta (j = 1, t = 5, l = 17) : -2.64 is 0.00.\n",
      "grad at beta (j = 1, t = 5, l = 18) : -0.34 is -0.00.\n",
      "grad at beta (j = 1, t = 5, l = 19) : -0.65 is -0.00.\n",
      "grad at beta (j = 1, t = 5, l = 20) : -0.51 is 0.00.\n",
      "grad at beta (j = 1, t = 5, l = 21) : 0.78 is -0.00.\n",
      "grad at beta (j = 1, t = 5, l = 22) : 0.67 is -0.00.\n",
      "grad at beta (j = 1, t = 5, l = 23) : 0.13 is -0.00.\n",
      "grad at beta (j = 1, t = 5, l = 24) : -0.08 is -0.00.\n",
      "grad at beta (j = 1, t = 5, l = 25) : 0.09 is -0.00.\n",
      "grad at beta (j = 1, t = 5, l = 26) : 0.29 is 0.00.\n",
      "grad at beta (j = 1, t = 5, l = 27) : -0.91 is -0.00.\n",
      "grad at beta (j = 1, t = 5, l = 28) : -2.64 is -0.00.\n",
      "grad at beta (j = 1, t = 5, l = 29) : 1.15 is 0.00.\n",
      "grad at beta (j = 1, t = 6, l = 0) : -119.61 is 0.00.\n",
      "grad at beta (j = 1, t = 6, l = 1) : -271.51 is 0.00.\n",
      "grad at beta (j = 1, t = 6, l = 2) : -302.23 is 0.00.\n",
      "grad at beta (j = 1, t = 6, l = 3) : -382.92 is 0.00.\n",
      "grad at beta (j = 1, t = 6, l = 4) : -253.38 is 0.00.\n",
      "grad at beta (j = 1, t = 6, l = 5) : -68.15 is -0.00.\n",
      "grad at beta (j = 1, t = 6, l = 6) : 115.90 is 0.00.\n",
      "grad at beta (j = 1, t = 6, l = 7) : -0.54 is -0.00.\n",
      "grad at beta (j = 1, t = 6, l = 8) : 169.22 is 0.00.\n",
      "grad at beta (j = 1, t = 6, l = 9) : 19.00 is 0.00.\n",
      "grad at beta (j = 1, t = 6, l = 10) : -1.89 is 0.00.\n",
      "grad at beta (j = 1, t = 6, l = 11) : -0.10 is 0.00.\n",
      "grad at beta (j = 1, t = 6, l = 12) : 3.38 is 0.00.\n",
      "grad at beta (j = 1, t = 6, l = 13) : -0.16 is -0.00.\n",
      "grad at beta (j = 1, t = 6, l = 14) : 0.10 is -0.00.\n",
      "grad at beta (j = 1, t = 6, l = 15) : 0.16 is -0.00.\n",
      "grad at beta (j = 1, t = 6, l = 16) : -1.98 is -0.00.\n",
      "grad at beta (j = 1, t = 6, l = 17) : -1.07 is -0.00.\n",
      "grad at beta (j = 1, t = 6, l = 18) : -2.37 is 0.00.\n",
      "grad at beta (j = 1, t = 6, l = 19) : -2.68 is -0.00.\n",
      "grad at beta (j = 1, t = 6, l = 20) : -5.34 is 0.00.\n",
      "grad at beta (j = 1, t = 6, l = 21) : -4.64 is 0.00.\n",
      "grad at beta (j = 1, t = 6, l = 22) : -2.97 is -0.00.\n",
      "grad at beta (j = 1, t = 6, l = 23) : -3.45 is 0.00.\n",
      "grad at beta (j = 1, t = 6, l = 24) : -1.34 is 0.00.\n",
      "grad at beta (j = 1, t = 6, l = 25) : 2.81 is -0.00.\n",
      "grad at beta (j = 1, t = 6, l = 26) : 5.39 is -0.00.\n",
      "grad at beta (j = 1, t = 6, l = 27) : 3.32 is 0.00.\n",
      "grad at beta (j = 1, t = 6, l = 28) : 9.78 is -0.00.\n",
      "grad at beta (j = 1, t = 6, l = 29) : 10.72 is -0.00.\n",
      "grad at beta (j = 1, t = 7, l = 0) : -157.94 is -0.00.\n",
      "grad at beta (j = 1, t = 7, l = 1) : -65.36 is -0.00.\n",
      "grad at beta (j = 1, t = 7, l = 2) : -324.52 is 0.00.\n",
      "grad at beta (j = 1, t = 7, l = 3) : -261.02 is -0.00.\n",
      "grad at beta (j = 1, t = 7, l = 4) : -584.27 is -0.00.\n",
      "grad at beta (j = 1, t = 7, l = 5) : -154.73 is 0.00.\n",
      "grad at beta (j = 1, t = 7, l = 6) : 131.78 is -0.00.\n",
      "grad at beta (j = 1, t = 7, l = 7) : -62.99 is 0.00.\n",
      "grad at beta (j = 1, t = 7, l = 8) : 195.65 is 0.00.\n",
      "grad at beta (j = 1, t = 7, l = 9) : 8.58 is -0.00.\n",
      "grad at beta (j = 1, t = 7, l = 10) : -0.72 is 0.00.\n",
      "grad at beta (j = 1, t = 7, l = 11) : 1.26 is 0.00.\n",
      "grad at beta (j = 1, t = 7, l = 12) : 0.29 is 0.00.\n",
      "grad at beta (j = 1, t = 7, l = 13) : 1.79 is 0.00.\n",
      "grad at beta (j = 1, t = 7, l = 14) : -1.08 is 0.00.\n",
      "grad at beta (j = 1, t = 7, l = 15) : -2.29 is 0.00.\n",
      "grad at beta (j = 1, t = 7, l = 16) : -0.96 is 0.00.\n",
      "grad at beta (j = 1, t = 7, l = 17) : 0.12 is -0.00.\n",
      "grad at beta (j = 1, t = 7, l = 18) : -3.09 is 0.00.\n",
      "grad at beta (j = 1, t = 7, l = 19) : -1.70 is -0.00.\n",
      "grad at beta (j = 1, t = 7, l = 20) : -1.07 is 0.00.\n",
      "grad at beta (j = 1, t = 7, l = 21) : 2.33 is 0.00.\n",
      "grad at beta (j = 1, t = 7, l = 22) : -0.88 is -0.00.\n",
      "grad at beta (j = 1, t = 7, l = 23) : -0.19 is 0.00.\n",
      "grad at beta (j = 1, t = 7, l = 24) : 2.82 is -0.00.\n",
      "grad at beta (j = 1, t = 7, l = 25) : 11.74 is -0.00.\n",
      "grad at beta (j = 1, t = 7, l = 26) : 3.50 is 0.00.\n",
      "grad at beta (j = 1, t = 7, l = 27) : 0.54 is -0.00.\n",
      "grad at beta (j = 1, t = 7, l = 28) : 1.52 is -0.00.\n",
      "grad at beta (j = 1, t = 7, l = 29) : 9.32 is -0.00.\n",
      "grad at beta (j = 1, t = 8, l = 0) : -57.90 is -0.00.\n",
      "grad at beta (j = 1, t = 8, l = 1) : -34.12 is -0.00.\n",
      "grad at beta (j = 1, t = 8, l = 2) : -155.49 is -0.00.\n",
      "grad at beta (j = 1, t = 8, l = 3) : -175.82 is -0.00.\n",
      "grad at beta (j = 1, t = 8, l = 4) : -168.23 is 0.00.\n",
      "grad at beta (j = 1, t = 8, l = 5) : -97.97 is -0.00.\n",
      "grad at beta (j = 1, t = 8, l = 6) : 30.31 is 0.00.\n",
      "grad at beta (j = 1, t = 8, l = 7) : 51.57 is 0.00.\n",
      "grad at beta (j = 1, t = 8, l = 8) : 155.73 is -0.00.\n",
      "grad at beta (j = 1, t = 8, l = 9) : 4.52 is 0.00.\n",
      "grad at beta (j = 1, t = 8, l = 10) : -1.51 is -0.00.\n",
      "grad at beta (j = 1, t = 8, l = 11) : 1.97 is 0.00.\n",
      "grad at beta (j = 1, t = 8, l = 12) : 1.58 is -0.00.\n",
      "grad at beta (j = 1, t = 8, l = 13) : -1.15 is -0.00.\n",
      "grad at beta (j = 1, t = 8, l = 14) : -1.12 is 0.00.\n",
      "grad at beta (j = 1, t = 8, l = 15) : 1.41 is -0.00.\n",
      "grad at beta (j = 1, t = 8, l = 16) : 1.43 is -0.00.\n",
      "grad at beta (j = 1, t = 8, l = 17) : -0.25 is -0.00.\n",
      "grad at beta (j = 1, t = 8, l = 18) : 1.52 is -0.00.\n",
      "grad at beta (j = 1, t = 8, l = 19) : -1.08 is -0.00.\n",
      "grad at beta (j = 1, t = 8, l = 20) : -0.46 is -0.00.\n",
      "grad at beta (j = 1, t = 8, l = 21) : -0.78 is 0.00.\n",
      "grad at beta (j = 1, t = 8, l = 22) : 0.11 is 0.00.\n",
      "grad at beta (j = 1, t = 8, l = 23) : 0.72 is 0.00.\n",
      "grad at beta (j = 1, t = 8, l = 24) : 2.41 is -0.00.\n",
      "grad at beta (j = 1, t = 8, l = 25) : 0.42 is 0.00.\n",
      "grad at beta (j = 1, t = 8, l = 26) : 1.82 is 0.00.\n",
      "grad at beta (j = 1, t = 8, l = 27) : 0.16 is -0.00.\n",
      "grad at beta (j = 1, t = 8, l = 28) : 3.73 is 0.00.\n",
      "grad at beta (j = 1, t = 8, l = 29) : 2.37 is -0.00.\n",
      "grad at beta (j = 1, t = 9, l = 0) : -233.98 is 0.00.\n",
      "grad at beta (j = 1, t = 9, l = 1) : -2.32 is 0.00.\n",
      "grad at beta (j = 1, t = 9, l = 2) : -468.99 is -0.00.\n",
      "grad at beta (j = 1, t = 9, l = 3) : -126.13 is -0.00.\n",
      "grad at beta (j = 1, t = 9, l = 4) : -876.42 is 0.00.\n",
      "grad at beta (j = 1, t = 9, l = 5) : -38.56 is 0.00.\n",
      "grad at beta (j = 1, t = 9, l = 6) : -445.90 is -0.00.\n",
      "grad at beta (j = 1, t = 9, l = 7) : -996.25 is -0.00.\n",
      "grad at beta (j = 1, t = 9, l = 8) : 5.58 is -0.00.\n",
      "grad at beta (j = 1, t = 9, l = 9) : 337.56 is -0.00.\n",
      "grad at beta (j = 1, t = 9, l = 10) : 21.50 is -0.00.\n",
      "grad at beta (j = 1, t = 9, l = 11) : 1.57 is 0.00.\n",
      "grad at beta (j = 1, t = 9, l = 12) : 8.73 is -0.00.\n",
      "grad at beta (j = 1, t = 9, l = 13) : 10.62 is 0.00.\n",
      "grad at beta (j = 1, t = 9, l = 14) : 2.03 is 0.00.\n",
      "grad at beta (j = 1, t = 9, l = 15) : -4.33 is -0.00.\n",
      "grad at beta (j = 1, t = 9, l = 16) : -2.05 is 0.00.\n",
      "grad at beta (j = 1, t = 9, l = 17) : 0.32 is -0.00.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "enc_r, enc_lam, enc_b, enc_beta, enc_mu, enc_cov, enc_elbo = cavi(\n",
    "                                                    s=s, \n",
    "                                                    y_k=y_k, \n",
    "                                                    means=gmm.means_, \n",
    "                                                    covs=gmm.covariances_, \n",
    "                                                    K=len(train_idx), \n",
    "                                                    T=n_time_bins, \n",
    "                                                    C=len(gmm.means_), \n",
    "                                                    D=train_trials.shape[1], \n",
    "                                                    k_idxs=k_idxs,\n",
    "                                                    t_idxs=t_idxs, \n",
    "                                                    max_iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdbe2e3-fd39-40bb-aa51-70d8975dfd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(enc_elbo)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('ELBO')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a82a7b-7155-4ada-92f7-4b70166abfdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
