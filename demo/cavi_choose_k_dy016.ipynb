{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde4fb9b-5728-474f-8dcb-4508d495d296",
   "metadata": {},
   "source": [
    "In this notebook, we compare 2 ways to choose $K$ using 5-fold CV:\n",
    "\n",
    "1. $K$ = \\{# of channels\\} or $\\frac{1}{2} \\times$ \\{# of channels\\}\n",
    "\n",
    "2. For each CV fold, preserve a validation set and a test set. Choose $K$ that yields the best decoding results on the validation set. Then use the training set to train the model, and evaluate on the test set. \n",
    "\n",
    "We compare the following baselines:\n",
    "\n",
    "1. KS all units\n",
    "\n",
    "2. KS good units\n",
    "\n",
    "3. Thresholded units\n",
    "\n",
    "4. MoG\n",
    "\n",
    "5. CAVI + MoG ($K \\propto$ \\{# of channels\\})\n",
    "\n",
    "6. CAVI + MoG ($K$ selected by validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ebdad3-7f3f-451d-a165-0d0667bb52e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from clusterless import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f96b5d-bba6-4c4a-93f2-bc2f88625773",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 666\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706227bd-74c4-4c7a-b7e0-4c45083be3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 20\n",
    "MEDIUM_SIZE = 25\n",
    "BIGGER_SIZE = 30\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)         \n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     \n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE) \n",
    "plt.rc('axes', linewidth = 1.5)\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)   \n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)   \n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)   \n",
    "plt.rc('figure', titlesize=MEDIUM_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c038a907-339e-4646-a910-57da1eced34b",
   "metadata": {},
   "source": [
    "#### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5533c467-2921-4c27-9de2-3b33b4cad9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = 'dab512bd-a02d-4c1f-8dbc-9155a163efc0'\n",
    "rootpath = '/mnt/3TB/yizi/Downloads/ONE/openalyx.internationalbrainlab.org'\n",
    "trial_data_path = rootpath + '/danlab/Subjects/DY_016/2020-09-12/001/alf'\n",
    "neural_data_path = '/mnt/3TB/yizi/danlab/Subjects/DY_016'\n",
    "behavior_data_path = rootpath + '/paper_repro_ephys_data/figure9_10/original_data'\n",
    "save_path = '../saved_results/danlab/Subjects/DY_016/cavi_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606d92e-d9bc-4aa0-a0f5-8558d61e0a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = [roi.lower() for roi in ['PO', 'LP', 'DG', 'CA1', 'VIS']]\n",
    "roi = rois[2]\n",
    "print(roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287afebe-5cf2-4a5d-ba42-744577e22f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsorted_trials, stim_on_times, np1_channel_map = preprocess.load_neural_data(\n",
    "    pid=pid, \n",
    "    trial_data_path=trial_data_path,\n",
    "    neural_data_path=neural_data_path,\n",
    "    behavior_data_path=behavior_data_path,\n",
    "    keep_active_trials=True, \n",
    "    roi=roi,\n",
    "    # roi='all',\n",
    "    kilosort=False,\n",
    "    triage=False\n",
    ")\n",
    "\n",
    "behave_dict = preprocess.load_behaviors_data(behavior_data_path, pid)\n",
    "choices, stimuli, transformed_stimuli, one_hot_stimuli, enc_categories, rewards, priors = preprocess.preprocess_static_behaviors(behave_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b047a0ab-1534-4edc-9704-47fa92a3d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, data, y, stim_on_times, np1_channel_map, n_t_bins=30):\n",
    "        self.data = data\n",
    "        self.y = y\n",
    "        self.stim_on_times = stim_on_times\n",
    "        self.np1_channel_map = np1_channel_map\n",
    "        self.n_t_bins = n_t_bins\n",
    "        self.n_trials = stim_on_times.shape[0]\n",
    "        self.n_channels = np1_channel_map.shape[0]\n",
    "        self.t_binning = np.arange(0, 1.5, step = (1.5 - 0) / n_t_bins)\n",
    "        self.rand_trial_ids = np.arange(self.n_trials)\n",
    "        \n",
    "        # allocate unsorted data into trials\n",
    "        self.trial_ids = []\n",
    "        self.t_ids = []\n",
    "        self.trials = []\n",
    "        self.t_bins = []\n",
    "        for k in range(self.n_trials):\n",
    "            mask = np.logical_and(data[:,0] >= stim_on_times[k] - 0.5,\n",
    "                                  data[:,0] <= stim_on_times[k] + 1)\n",
    "            trial = data[mask,:]\n",
    "            trial[:,0] = trial[:,0] - trial[:,0].min()\n",
    "            t_bins = np.digitize(trial[:,0], self.t_binning, right = False) - 1\n",
    "            t_bin_lst = []\n",
    "            for t in range(self.n_t_bins):\n",
    "                t_bin = trial[t_bins == t,1:]\n",
    "                self.trial_ids.append(np.ones_like(t_bin[:,0]) * k)\n",
    "                self.t_ids.append(np.ones_like(t_bin[:,0]) * t)\n",
    "                t_bin_lst.append(t_bin)\n",
    "            self.trials.append(t_bin_lst)\n",
    "    \n",
    "    \n",
    "    def split_train_val_test(self, train_ids, val_ids, test_ids):\n",
    "        \n",
    "        self.train_ids = self.rand_trial_ids[train_ids]\n",
    "        self.val_ids = self.rand_trial_ids[val_ids]\n",
    "        self.test_ids = self.rand_trial_ids[test_ids]\n",
    "        self.y_train = self.y[self.train_ids]\n",
    "        self.y_val = self.y[self.val_ids]\n",
    "        self.y_test = self.y[self.test_ids]\n",
    "        \n",
    "        trial_ids = np.concatenate(self.trial_ids)\n",
    "        t_ids = np.concatenate(self.t_ids)\n",
    "        trials = np.concatenate(np.concatenate(self.trials))\n",
    "\n",
    "        train_mask = np.sum([trial_ids == idx for idx in self.train_ids], axis=0).astype(bool)\n",
    "        val_mask = np.sum([trial_ids == idx for idx in self.val_ids], axis=0).astype(bool)\n",
    "        test_mask = np.sum([trial_ids == idx for idx in self.test_ids], axis=0).astype(bool)\n",
    "        train_trial_ids, val_trial_ids, test_trial_ids = trial_ids[train_mask], trial_ids[val_mask], trial_ids[test_mask]\n",
    "        train_t_ids, val_t_ids, test_t_ids = t_ids[train_mask], t_ids[val_mask], t_ids[test_mask]\n",
    "        train_trials, val_trials, test_trials = trials[train_mask], trials[val_mask], trials[test_mask]\n",
    "        \n",
    "        return train_trials, train_trial_ids, train_t_ids, \\\n",
    "               val_trials, val_trial_ids, val_t_ids, \\\n",
    "               test_trials, test_trial_ids, test_t_ids\n",
    "    \n",
    "    \n",
    "    def compute_lambda(self, gmm):\n",
    "        C = len(gmm.means_)\n",
    "        lambdas = []\n",
    "        for k in self.train_ids:\n",
    "            lam_lst = []\n",
    "            for t in range(self.n_t_bins):\n",
    "                lam = np.zeros((C, 2))\n",
    "                t_bin = self.trials[k][t]\n",
    "                if len(t_bin) > 0:\n",
    "                    cluster_ids = gmm.predict(t_bin)\n",
    "                    for j in range(C):\n",
    "                        if self.y[k] == 0:\n",
    "                            lam[j, 0] = np.sum(cluster_ids == j)\n",
    "                        else:\n",
    "                            lam[j, 1] = np.sum(cluster_ids == j)\n",
    "                lam_lst.append(lam)\n",
    "            lambdas.append(lam_lst)\n",
    "        n_left, n_right = np.sum(self.y_train == 0), np.sum(self.y_train == 1)\n",
    "        p = n_right / (n_right + n_left)\n",
    "        lambdas = ( np.array(lambdas).sum(0) / np.array([n_left, n_right]) ).transpose(1,0,2)\n",
    "        return lambdas, p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b75edb-f996-4bb9-a145-d86dc1604186",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(data = np.concatenate(unsorted_trials)[:,[0,2,3,4]], \n",
    "                         y = choices.argmax(1), \n",
    "                         stim_on_times = stim_on_times, \n",
    "                         np1_channel_map = np1_channel_map, \n",
    "                         n_t_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3078c8c6-64fe-4fd2-a84d-55cc05376cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "kf_train_ids = []; kf_val_ids = []; kf_test_ids = []\n",
    "for i, (train_ids, test_ids) in enumerate(kf.split(data_loader.y)):\n",
    "    kf_train_ids.append(train_ids)\n",
    "    kf_val_ids.append(test_ids[:int(.5*len(test_ids))])\n",
    "    kf_test_ids.append(test_ids[int(.5*len(test_ids)):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4853676-ed70-4fdc-94f7-8ae0e87e2da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "train_trials, train_trial_ids, train_t_ids, \\\n",
    "val_trials, val_trial_ids, val_t_ids, \\\n",
    "test_trials, test_trial_ids, test_t_ids = data_loader.split_train_test(\n",
    "    train_ids = kf_train_ids[i], \n",
    "    val_ids = kf_val_ids[i],\n",
    "    test_ids = kf_test_ids[i]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920c968-9da0-43a0-90f8-41612fddd2ff",
   "metadata": {},
   "source": [
    "#### CAVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c5aea1-539e-433a-a8a0-5fe394e0059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_log(x, minval=1e-10):\n",
    "    return torch.log(x + minval)\n",
    "\n",
    "def safe_divide(x, y):\n",
    "    return torch.clip(x / y, min = 0, max = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e02315-f5aa-4c22-b6ca-1e6a2b21f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAVI():\n",
    "    def __init__(self, init_mu, init_cov, init_lam, \n",
    "                 train_k_ids, train_t_ids, test_k_ids, test_t_ids):\n",
    "        \n",
    "        self.train_K = len(train_k_ids)\n",
    "        self.test_K = len(test_k_ids)\n",
    "        self.T = len(train_t_ids)\n",
    "        self.C = init_mu.shape[0]\n",
    "        self.D = init_mu.shape[1]\n",
    "        self.init_mu = init_mu\n",
    "        self.init_cov = init_cov\n",
    "        self.init_lam = init_lam\n",
    "        self.train_k_ids = train_k_ids\n",
    "        self.train_t_ids = train_t_ids\n",
    "        self.test_k_ids = test_k_ids\n",
    "        self.test_t_ids = test_t_ids\n",
    "        \n",
    "    \n",
    "    def _compute_normal_log_dens(self, s, mu, cov, safe_cov=None):\n",
    "        \n",
    "        log_dens = []\n",
    "        for j in range(self.C):\n",
    "            try:\n",
    "                log_dens.append(\n",
    "                    torch.tensor(\n",
    "                        multivariate_normal.logpdf(s, mu[j], cov[j])\n",
    "                    )\n",
    "                )\n",
    "            except np.linalg.LinAlgError:\n",
    "                # error occurs when cov is not PSD\n",
    "                # use initial cov as a replacement to ensure numerical stability\n",
    "                # need a better solution \n",
    "                log_dens.append(\n",
    "                    torch.tensor(\n",
    "                        multivariate_normal.logpdf(s, mu[j], safe_cov[j])\n",
    "                    )\n",
    "                )\n",
    "                print(f'cov {j} is not PSD.')\n",
    "        return torch.vstack(log_dens).T # (*, C)\n",
    "    \n",
    "    \n",
    "    def _compute_enc_elbo(self, r, y, log_dens, norm_lam):\n",
    "        \n",
    "        elbo_1 = torch.sum(torch.tensor(\n",
    "            [torch.einsum('i,i->', r[:,j], log_dens[:,j]) for j in range(self.C)]\n",
    "        ))\n",
    "\n",
    "        elbo_2 = torch.tensor(\n",
    "            [ torch.einsum('ij,il,j->', r[self.train_t_ids[t]], \n",
    "                           y[self.train_t_ids[t]], norm_lam[:,t,1]) +\n",
    "              torch.einsum('ij,il,j->', r[self.train_t_ids[t]], \n",
    "                           1-y[self.train_t_ids[t]], norm_lam[:,t,0]) for t in range(self.T) ]\n",
    "            ).sum()\n",
    "\n",
    "        elbo_3 = - torch.einsum('ij,ij->', safe_log(r), r)\n",
    "\n",
    "        elbo = elbo_1 + elbo_2 + elbo_3 \n",
    "\n",
    "        return elbo\n",
    "    \n",
    "    \n",
    "    def _compute_dec_elbo(self, r, log_dens, norm_lam, nu, nu_k, p):\n",
    "        \n",
    "        elbo_1 = torch.sum(torch.tensor(\n",
    "            [torch.einsum('i,i->', r[:,j], log_dens[:,j]) for j in range(self.C)]\n",
    "        ))\n",
    "\n",
    "        elbo_2 = torch.tensor(\n",
    "            [ torch.einsum('ij,il,j->', r[self.test_t_ids[t]], \n",
    "                           nu[self.test_t_ids[t]], norm_lam[:,t,1]) +\n",
    "              torch.einsum('ij,il,j->', r[self.test_t_ids[t]], \n",
    "                           1-nu[self.test_t_ids[t]], norm_lam[:,t,0]) for t in range(self.T) ]\n",
    "            ).sum()\n",
    "\n",
    "        elbo_3 = torch.sum(nu_k * safe_log(p) + (1-nu_k) * safe_log(1-p))\n",
    "\n",
    "        elbo_4 = - torch.einsum('ij,ij->', safe_log(r), r)\n",
    "\n",
    "        elbo_5 = - torch.sum(safe_log(nu_k) * nu_k)\n",
    "\n",
    "        elbo = elbo_1 + elbo_2 + elbo_3 + elbo_4 + elbo_5\n",
    "\n",
    "        return elbo\n",
    "    \n",
    "    \n",
    "    def _encode_e_step(self, r, y, log_dens, norm_lam):\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            r[self.train_t_ids[t]] = torch.exp( log_dens[self.train_t_ids[t]] + \\\n",
    "                      torch.einsum('il,j->ij', y[self.train_t_ids[t]], norm_lam[:,t,1]) + \\\n",
    "                      torch.einsum('il,j->ij', 1-y[self.train_t_ids[t]], norm_lam[:,t,0])\n",
    "            )\n",
    "            r[self.train_t_ids[t]] = torch.einsum('ij,i->ij', r[self.train_t_ids[t]], 1/r[self.train_t_ids[t]].sum(1))\n",
    "        return r\n",
    "        \n",
    "    \n",
    "    def _encode_m_step(self, s, r, y, mu, lam):\n",
    "        \n",
    "        for j in range(self.C):\n",
    "            no_j_idx = torch.cat([torch.arange(j), torch.arange(j+1, self.C)])\n",
    "            lam_sum_no_j = lam[no_j_idx,:,:].sum(0)\n",
    "            for t in range(self.T):\n",
    "                num1 = torch.einsum('i,il,->', r[self.train_t_ids[t],j], y[self.train_t_ids[t]], lam_sum_no_j[t,1])\n",
    "                denom1 = np.einsum('ij,il->', r[self.train_t_ids[t]][:,no_j_idx], y[self.train_t_ids[t]])\n",
    "                num0 = torch.einsum('i,il,->', r[self.train_t_ids[t],j], 1-y[self.train_t_ids[t]], lam_sum_no_j[t,0])\n",
    "                denom0 = np.einsum('ij,il->', r[self.train_t_ids[t]][:,no_j_idx], 1-y[self.train_t_ids[t]])\n",
    "                lam[j,t,1], lam[j,t,0] = num1 / denom1, num0 / denom0\n",
    "        norm_lam = safe_log(lam) - safe_log(lam.sum(0))\n",
    "\n",
    "        norm = r.sum(0)\n",
    "        mu = torch.einsum('j,ij,ip->jp', 1/norm, r, s)\n",
    "        cov = [torch.einsum(\n",
    "            ',i,ip,id->pd', 1/norm[j], r[:,j], s-mu[j], s-mu[j] ) for j in range(self.C)]\n",
    "        \n",
    "        return mu, cov, lam, norm_lam\n",
    "    \n",
    "    \n",
    "    def _decode_e_step(self, r, log_dens, norm_lam, nu, nu_k, p):\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            r[self.test_t_ids[t]] = torch.exp( log_dens[self.test_t_ids[t]] + \\\n",
    "                      torch.einsum('il,j->ij', nu[self.test_t_ids[t]], norm_lam[:,t,1]) + \\\n",
    "                      torch.einsum('il,j->ij', 1-nu[self.test_t_ids[t]], norm_lam[:,t,0])\n",
    "            )\n",
    "            r[self.test_t_ids[t]] = torch.einsum('ij,i->ij', r[self.test_t_ids[t]], 1/r[self.test_t_ids[t]].sum(1))\n",
    "        \n",
    "        for k in range(self.test_K):\n",
    "            y_tilde0, y_tilde1 = safe_log(1-p), safe_log(p)\n",
    "            for t in range(self.T):\n",
    "                k_t_ids = np.intersect1d(self.test_k_ids[k], self.test_t_ids[t])\n",
    "                y_tilde0 += torch.einsum('ij,j->', r[k_t_ids], norm_lam[:,t,0])\n",
    "                y_tilde1 += torch.einsum('ij,j->', r[k_t_ids], norm_lam[:,t,1])\n",
    "            # y_tilde explode to 0 after exp(); need offset to ensure y_tilde stay in range\n",
    "            # offset = 1. / (torch.min(torch.tensor([y_tilde0, y_tilde1])) / -745.) \n",
    "            offset = 1. / (torch.min(torch.tensor([y_tilde0, y_tilde1])) / -250.)\n",
    "            y_tilde0, y_tilde1 = torch.exp(y_tilde0 * offset), torch.exp(y_tilde1 * offset)\n",
    "            nu_k[k] = safe_divide(y_tilde1, y_tilde0 + y_tilde1)\n",
    "            nu[self.test_k_ids[k]] = nu_k[k]\n",
    "            \n",
    "        return r, nu, nu_k\n",
    "    \n",
    "    \n",
    "    def _decode_m_step(self, s, r, nu_k, mu):\n",
    "        \n",
    "        p = nu_k.sum() / self.test_K\n",
    "    \n",
    "        norm = r.sum(0)\n",
    "        mu = torch.einsum('j,ij,ip->jp', 1/norm, r, s)\n",
    "        cov = [torch.einsum(\n",
    "            ',i,ip,id->pd', 1/norm[j], r[:,j], s-mu[j], s-mu[j]) for j in range(self.C)]\n",
    "        \n",
    "        return p, mu, cov\n",
    "    \n",
    "    \n",
    "    def encode(self, s, y, max_iter=20, eps=1e-6):\n",
    "        # initialize\n",
    "        r = torch.ones((s.shape[0], self.C)) / self.C\n",
    "        lam = self.init_lam.clone()\n",
    "        mu, cov = self.init_mu.clone(), self.init_cov.clone()\n",
    "        norm_lam = safe_log(lam) - safe_log(lam.sum(0))\n",
    "        \n",
    "        # compute initial elbo\n",
    "        log_dens = self._compute_normal_log_dens(s, mu, cov, safe_cov=self.init_cov)\n",
    "        elbo = self._compute_enc_elbo(r, y, log_dens, norm_lam)\n",
    "        convergence = 1.\n",
    "        elbos = [elbo]\n",
    "        print(f'initial elbo: {elbos[-1]:.2f}')\n",
    "        \n",
    "        it = 1\n",
    "        while convergence > eps or convergence < 0: \n",
    "            # E step\n",
    "            r = self._encode_e_step(r, y, log_dens, norm_lam)\n",
    "            \n",
    "            # M step\n",
    "            mu, cov, lam, norm_lam = self._encode_m_step(s, r, y, mu, lam)\n",
    "            \n",
    "            # compute new elbo\n",
    "            log_dens = self._compute_normal_log_dens(s, mu, cov, safe_cov=self.init_cov)\n",
    "            elbo = self._compute_enc_elbo(r, y, log_dens, norm_lam)\n",
    "            elbos.append(elbo)\n",
    "            convergence = elbos[-1] - elbos[-2]\n",
    "\n",
    "            print(f'iter: {it} elbo: {elbos[-1]:.2f}.')\n",
    "            it +=1 \n",
    "            if it > max_iter: \n",
    "                print('reached max iter allowed.')\n",
    "                break\n",
    "\n",
    "        if abs(convergence) <= eps:\n",
    "            print('converged.')\n",
    "            \n",
    "        return r, lam, mu, torch.stack(cov), elbos\n",
    "    \n",
    "    \n",
    "    def decode(self, s, init_p, init_mu, init_cov, init_lam, \n",
    "                test_k_ids, test_ids, max_iter=20, eps=1e-6):\n",
    "        # initialize\n",
    "        p = init_p.clone()\n",
    "        r = torch.ones((s.shape[0], self.C)) / self.C\n",
    "        mu, cov = init_mu.clone(), init_cov.clone()\n",
    "        lam = init_lam.clone()\n",
    "        norm_lam = safe_log(lam) - safe_log(lam.sum(0))\n",
    "        nu_k = torch.rand(self.test_K)\n",
    "        nu = torch.zeros(s.shape[0])\n",
    "        for k in range(self.test_K):\n",
    "            nu[test_k_ids == test_ids[k]] = nu_k[k]\n",
    "        nu = nu.reshape(-1,1)\n",
    "        \n",
    "        # compute initial elbo\n",
    "        log_dens = self._compute_normal_log_dens(s, mu, cov, safe_cov=init_cov)\n",
    "        elbo = self._compute_dec_elbo(r, log_dens, norm_lam, nu, nu_k, p)\n",
    "        convergence = 1.\n",
    "        elbos = [elbo]\n",
    "        print(f'initial elbo: {elbos[-1]:.2f}')\n",
    "        \n",
    "        it = 1\n",
    "        while convergence > eps or convergence < 0:\n",
    "            # E step\n",
    "            r, nu, nu_k = self._decode_e_step(r, log_dens, norm_lam, nu, nu_k, p)\n",
    "            \n",
    "            # M step\n",
    "            p, mu, cov = self._decode_m_step(s, r, nu_k, mu)\n",
    "            \n",
    "            # compute new elbo\n",
    "            log_dens = self._compute_normal_log_dens(s, mu, cov, safe_cov=init_cov)\n",
    "            elbo = self._compute_dec_elbo(r, log_dens, norm_lam, nu, nu_k, p)\n",
    "            elbos.append(elbo)\n",
    "            convergence = elbos[-1] - elbos[-2]\n",
    "\n",
    "            print(f'iter: {it} elbo: {elbos[-1]:.2f}.')\n",
    "            it +=1 \n",
    "            if it > max_iter: \n",
    "                print('reached max iter allowed.')\n",
    "                break\n",
    "\n",
    "        if abs(convergence) <= eps:\n",
    "            print('converged.')\n",
    "    \n",
    "        return r, nu_k, mu, torch.stack(cov), p, elbos\n",
    "    \n",
    "    \n",
    "    def eval_performance(self, nu_k, y_test):\n",
    "        acc = accuracy_score(y_test, 1. * ( nu_k > .5 ))\n",
    "        auc = roc_auc_score(y_test, nu_k)\n",
    "        print(f'decoding accuracy is {acc:.2f}')\n",
    "        print(f'decoding auc is {auc:.2f}')\n",
    "        return acc, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960bc2f9-7aab-41da-a2b5-e4bdfc2fc924",
   "metadata": {},
   "source": [
    "#### thresholded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5937d275-fe68-465d-915c-f2c35e9990d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = stim_on_times.shape[0]\n",
    "unsorted = np.vstack([unsorted_trials[i] for i in np.arange(n_trials)]) \n",
    "spike_times = unsorted[:,0]\n",
    "spike_channels = unsorted[:,1]\n",
    "spike_features = unsorted[:,2:]\n",
    "\n",
    "thresholded_neural_data = preprocess.compute_neural_activity(\n",
    "    (spike_times, spike_channels),\n",
    "    stim_on_times,\n",
    "    'thresholded', \n",
    "    n_time_bins=10,\n",
    "    # regional=True\n",
    "    regional=False\n",
    ")\n",
    "print(f'thresholded neural data shape: {thresholded_neural_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f65b52-1a0d-4d76-a189-72e8c9491e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = thresholded_neural_data[data_loader.train_ids].reshape(-1, thresholded_neural_data.shape[1]*thresholded_neural_data.shape[-1])\n",
    "x_test = thresholded_neural_data[data_loader.test_ids].reshape(-1, thresholded_neural_data.shape[1]*thresholded_neural_data.shape[-1])\n",
    "decoder = LogisticRegression(random_state=seed, solver='liblinear').fit(x_train, data_loader.y_train)\n",
    "probs = decoder.predict_proba(x_test)\n",
    "threshold_preds = probs.argmax(1)\n",
    "acc = accuracy_score(data_loader.y_test, threshold_preds)\n",
    "auc = roc_auc_score(choices[data_loader.test_ids], probs)\n",
    "print(f'{acc:.2f}')\n",
    "print(f'{auc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59948802-58ef-4e2b-aaa0-6b33063c58c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_all_accs = []\n",
    "thresh_all_aucs = []\n",
    "thresh_po_accs = []\n",
    "thresh_po_aucs = []\n",
    "thresh_lp_accs = []\n",
    "thresh_lp_aucs = []\n",
    "thresh_dg_accs = []\n",
    "thresh_dg_aucs = []\n",
    "thresh_ca1_accs = []\n",
    "thresh_ca1_aucs = []\n",
    "thresh_vis_accs = []\n",
    "thresh_vis_aucs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95164168-40d3-4597-94fa-c1a20b612072",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'all acc: {np.mean(thresh_all_accs):.2f} auc: {np.mean(thresh_all_aucs):.2f}')\n",
    "print(f'po acc: {np.mean(thresh_po_accs):.2f} auc: {np.mean(thresh_po_aucs):.2f}')\n",
    "print(f'lp acc: {np.mean(thresh_lp_accs):.2f} auc: {np.mean(thresh_lp_aucs):.2f}')\n",
    "print(f'dg acc: {np.mean(thresh_dg_accs):.2f} auc: {np.mean(thresh_dg_aucs):.2f}')\n",
    "print(f'ca1 acc: {np.mean(thresh_ca1_accs):.2f} auc: {np.mean(thresh_ca1_aucs):.2f}')\n",
    "print(f'vis acc: {np.mean(thresh_vis_accs):.2f} auc: {np.mean(thresh_vis_aucs):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91eff0a-4128-4df8-bfa6-24391d66b4a1",
   "metadata": {},
   "source": [
    "#### choose $K$ using validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0538d5-2d64-4636-90a5-e9dd3eb2214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C = \n",
    "T = data_loader.n_t_bins\n",
    "y_train = torch.zeros(train_trials.shape[0])\n",
    "for k_idx, k in enumerate(data_loader.train_ids):\n",
    "    y_train[train_trial_ids == k] = data_loader.y_train[k_idx]\n",
    "y_train = y_train.reshape(-1,1)\n",
    "\n",
    "val_accs = []\n",
    "val_aucs = []\n",
    "for c in np.arange(10, C, 10):\n",
    "    gmm = GaussianMixture(n_components = c, \n",
    "                      covariance_type = 'full', \n",
    "                      init_params = 'kmeans',\n",
    "                      verbose=1)\n",
    "    gmm.fit(train_trials)\n",
    "    \n",
    "    lambdas, p = data_loader.compute_lambda(gmm)\n",
    "    \n",
    "    cavi = CAVI(\n",
    "        init_mu = torch.tensor(gmm.means_), \n",
    "        init_cov = torch.tensor(gmm.covariances_), \n",
    "        init_lam = torch.tensor(lambdas), \n",
    "        train_k_ids = [torch.argwhere(torch.tensor(train_trial_ids) == k).reshape(-1) for k in data_loader.train_ids], \n",
    "        train_t_ids = [torch.argwhere(torch.tensor(train_t_ids) == t).reshape(-1) for t in range(T)],\n",
    "        test_k_ids = [torch.argwhere(torch.tensor(val_trial_ids) == k).reshape(-1) for k in data_loader.val_ids],\n",
    "        test_t_ids = [torch.argwhere(torch.tensor(val_t_ids) == t).reshape(-1) for t in range(T)]\n",
    "    )\n",
    "    \n",
    "    enc_r, enc_lam, enc_mu, enc_cov, enc_elbo = cavi.encode(\n",
    "        s = torch.tensor(train_trials),\n",
    "        y = y_train, max_iter=30)\n",
    "    \n",
    "    dec_r, dec_nu, dec_mu, dec_cov, dec_p, dec_elbos = cavi.decode(\n",
    "        s = torch.tensor(val_trials),\n",
    "        init_p = torch.tensor([p]), \n",
    "        init_mu = enc_mu, \n",
    "        init_cov = enc_cov, \n",
    "        init_lam = enc_lam, \n",
    "        test_k_ids = val_trial_ids, \n",
    "        test_ids = data_loader.val_ids, \n",
    "        max_iter=10\n",
    "    )\n",
    "    \n",
    "    acc, auc = cavi.eval_performance(dec_nu, data_loader.y_val)\n",
    "    val_accs.append(acc)\n",
    "    val_aucs.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be22ee2-dd29-4405-a786-6f47d2154ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.arange(10, C, 10)[np.array(val_accs).argmax()]\n",
    "gmm = GaussianMixture(n_components = C, \n",
    "                  covariance_type = 'full', \n",
    "                  init_params = 'kmeans',\n",
    "                  verbose=1)\n",
    "gmm.fit(train_trials)\n",
    "\n",
    "lambdas, p = data_loader.compute_lambda(gmm)\n",
    "\n",
    "cavi = CAVI(\n",
    "    init_mu = torch.tensor(gmm.means_), \n",
    "    init_cov = torch.tensor(gmm.covariances_), \n",
    "    init_lam = torch.tensor(lambdas), \n",
    "    train_k_ids = [torch.argwhere(torch.tensor(train_trial_ids) == k).reshape(-1) for k in data_loader.train_ids], \n",
    "    train_t_ids = [torch.argwhere(torch.tensor(train_t_ids) == t).reshape(-1) for t in range(T)],\n",
    "    test_k_ids = [torch.argwhere(torch.tensor(test_trial_ids) == k).reshape(-1) for k in data_loader.test_ids],\n",
    "    test_t_ids = [torch.argwhere(torch.tensor(test_t_ids) == t).reshape(-1) for t in range(T)]\n",
    ")\n",
    "\n",
    "enc_r, enc_lam, enc_mu, enc_cov, enc_elbo = cavi.encode(\n",
    "    s = torch.tensor(train_trials),\n",
    "    y = y_train, max_iter=30)\n",
    "\n",
    "dec_r, dec_nu, dec_mu, dec_cov, dec_p, dec_elbos = cavi.decode(\n",
    "    s = torch.tensor(test_trials),\n",
    "    init_p = torch.tensor([p]), \n",
    "    init_mu = enc_mu, \n",
    "    init_cov = enc_cov, \n",
    "    init_lam = enc_lam, \n",
    "    test_k_ids = test_trial_ids, \n",
    "    test_ids = data_loader.test_ids, \n",
    "    max_iter=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42639ef9-be2c-42d5-8bb3-f03d8fd90bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding MoG \n",
    "enc_pi = enc_lam / enc_lam.sum(0) \n",
    "y_pred = 1 * (dec_nu > .5)\n",
    "train_feat = [data_loader.trials[k] for k in data_loader.train_ids]\n",
    "test_feat = [data_loader.trials[k] for k in data_loader.test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db97414-a0c2-4c92-9e5b-53a4571e23d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_train = np.zeros((len(data_loader.train_ids), data_loader.n_t_bins, C))\n",
    "for k in range(len(data_loader.train_ids)):\n",
    "    for t in range(data_loader.n_t_bins):\n",
    "        modulated_gmm =  GaussianMixture(n_components=C, covariance_type='full')\n",
    "        modulated_gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(dec_cov))\n",
    "        modulated_gmm.weights_ = enc_pi[:, t, data_loader.y_train[k]].numpy()\n",
    "        modulated_gmm.means_ = dec_mu.numpy()\n",
    "        modulated_gmm.covariances_ = dec_cov.numpy()\n",
    "        if len(train_feat[k][t]) > 0:\n",
    "            W_train[k,t,:] = modulated_gmm.predict_proba(train_feat[k][t]).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c52ba-cf67-400c-a9c9-61d6041abced",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_test = np.zeros((len(data_loader.test_ids), data_loader.n_t_bins, C))\n",
    "for k in range(len(data_loader.test_ids)):\n",
    "    for t in range(data_loader.n_t_bins):\n",
    "        modulated_gmm =  GaussianMixture(n_components=C, covariance_type='full')\n",
    "        modulated_gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(dec_cov))\n",
    "        modulated_gmm.weights_ = enc_pi[:, t, y_pred[k]].numpy()\n",
    "        modulated_gmm.means_ = dec_mu.numpy()\n",
    "        modulated_gmm.covariances_ = dec_cov.numpy()\n",
    "        if len(test_feat[k][t]) > 0:\n",
    "            W_test[k,t,:] = modulated_gmm.predict_proba(test_feat[k][t]).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9fa50b-1fab-4da7-ae59-8f0f7749160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = W_train.reshape(-1, W_train.shape[1]*W_train.shape[-1])\n",
    "x_test = W_test.reshape(-1, W_test.shape[1]*W_test.shape[-1])\n",
    "decoder = LogisticRegression(random_state=seed, solver='liblinear').fit(x_train, data_loader.y_train)\n",
    "probs = decoder.predict_proba(x_test)\n",
    "preds = probs.argmax(1)\n",
    "acc = accuracy_score(data_loader.y_test, preds)\n",
    "auc = roc_auc_score(choices[data_loader.test_ids], probs)\n",
    "print(f'{acc:.2f}')\n",
    "print(f'{auc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe1c0f-f955-48fd-b50c-4a454328185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xval_init_enc_mog_all_accs = []\n",
    "xval_init_enc_mog_all_aucs = []\n",
    "xval_init_enc_mog_po_accs = []\n",
    "xval_init_enc_mog_po_aucs = []\n",
    "xval_init_enc_mog_lp_accs = []\n",
    "xval_init_enc_mog_lp_aucs = []\n",
    "xval_init_enc_mog_dg_accs = []\n",
    "xval_init_enc_mog_dg_aucs = []\n",
    "xval_init_enc_mog_ca1_accs = []\n",
    "xval_init_enc_mog_ca1_aucs = []\n",
    "xval_init_enc_mog_vis_accs = []\n",
    "xval_init_enc_mog_vis_aucs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea580af-32b5-4152-bb58-1db72ce4e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'all acc: {np.mean(xval_init_enc_mog_all_accs):.2f} auc: {np.mean(xval_init_enc_mog_all_aucs):.2f}')\n",
    "print(f'po acc: {np.mean(xval_init_enc_mog_po_accs):.2f} auc: {np.mean(xval_init_enc_mog_po_aucs):.2f}')\n",
    "print(f'lp acc: {np.mean(xval_init_enc_mog_lp_accs):.2f} auc: {np.mean(xval_init_enc_mog_lp_aucs):.2f}')\n",
    "print(f'dg acc: {np.mean(xval_init_enc_mog_dg_accs):.2f} auc: {np.mean(xval_init_enc_mog_dg_aucs):.2f}')\n",
    "print(f'ca1 acc: {np.mean(xval_init_enc_mog_ca1_accs):.2f} auc: {np.mean(xval_init_enc_mog_ca1_aucs):.2f}')\n",
    "print(f'vis acc: {np.mean(xval_init_enc_mog_vis_accs):.2f} auc: {np.mean(xval_init_enc_mog_vis_aucs):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d29bba-2fe3-4161-950d-007222ba2745",
   "metadata": {},
   "source": [
    "#### choose $K \\propto$ \\{# of channels\\} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81593a3-836f-484a-8027-ef28708f1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C = number of channels\n",
    "gmm = GaussianMixture(n_components = C, \n",
    "                      covariance_type = 'full', \n",
    "                      init_params = 'kmeans',\n",
    "                      verbose=1)\n",
    "gmm.fit(train_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29814ebb-aa3d-4972-b233-cbdb7cb9bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas, p = data_loader.compute_lambda(gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9798599f-2c55-4886-9120-e0c43c935884",
   "metadata": {},
   "outputs": [],
   "source": [
    "cavi = CAVI(\n",
    "    init_mu = torch.tensor(gmm.means_), \n",
    "    init_cov = torch.tensor(gmm.covariances_), \n",
    "    init_lam = torch.tensor(lambdas), \n",
    "    train_k_ids = [torch.argwhere(torch.tensor(train_trial_ids) == k).reshape(-1) for k in data_loader.train_ids], \n",
    "    train_t_ids = [torch.argwhere(torch.tensor(train_t_ids) == t).reshape(-1) for t in range(T)],\n",
    "    test_k_ids = [torch.argwhere(torch.tensor(test_trial_ids) == k).reshape(-1) for k in data_loader.test_ids],\n",
    "    test_t_ids = [torch.argwhere(torch.tensor(test_t_ids) == t).reshape(-1) for t in range(T)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d494861d-1cb0-4954-afd5-ea37ede98ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "enc_r, enc_lam, enc_mu, enc_cov, enc_elbo = cavi.encode(\n",
    "    s = torch.tensor(train_trials),\n",
    "    y = y_train, max_iter=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04680595-acff-4324-9c52-dd586f2142e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_init_cavi_all_accs = []\n",
    "channel_init_cavi_all_aucs = []\n",
    "channel_init_cavi_po_accs = []\n",
    "channel_init_cavi_po_aucs = []\n",
    "channel_init_cavi_lp_accs = []\n",
    "channel_init_cavi_lp_aucs = []\n",
    "channel_init_cavi_dg_accs = []\n",
    "channel_init_cavi_dg_aucs = []\n",
    "channel_init_cavi_ca1_accs = []\n",
    "channel_init_cavi_ca1_aucs = []\n",
    "channel_init_cavi_vis_accs = []\n",
    "channel_init_cavi_vis_aucs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1664a7-20ce-4d5c-b300-2ec3e227e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'all acc: {np.mean(channel_init_cavi_all_accs):.2f} auc: {np.mean(channel_init_cavi_all_aucs):.2f}')\n",
    "print(f'po acc: {np.mean(channel_init_cavi_po_accs):.2f} auc: {np.mean(channel_init_cavi_po_aucs):.2f}')\n",
    "print(f'lp acc: {np.mean(channel_init_cavi_lp_accs):.2f} auc: {np.mean(channel_init_cavi_lp_aucs):.2f}')\n",
    "print(f'dg acc: {np.mean(channel_init_cavi_dg_accs):.2f} auc: {np.mean(channel_init_cavi_dg_aucs):.2f}')\n",
    "print(f'ca1 acc: {np.mean(channel_init_cavi_ca1_accs):.2f} auc: {np.mean(channel_init_cavi_ca1_aucs):.2f}')\n",
    "print(f'vis acc: {np.mean(channel_init_cavi_vis_accs):.2f} auc: {np.mean(channel_init_cavi_vis_aucs):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7d8a7-2839-4ddc-b85b-363662f358d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding MoG \n",
    "enc_pi = enc_lam / enc_lam.sum(0) \n",
    "train_feat = [data_loader.trials[k] for k in data_loader.train_ids]\n",
    "test_feat = [data_loader.trials[k] for k in data_loader.test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de53ccef-0ded-4106-a4a6-71269f81e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_train = np.zeros((len(data_loader.train_ids), data_loader.n_t_bins, C))\n",
    "for k in range(len(data_loader.train_ids)):\n",
    "    for t in range(data_loader.n_t_bins):\n",
    "        modulated_gmm =  GaussianMixture(n_components=C, covariance_type='full')\n",
    "        modulated_gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(enc_cov))\n",
    "        modulated_gmm.weights_ = enc_pi[:, t, data_loader.y_train[k]].numpy()\n",
    "        modulated_gmm.means_ = enc_mu.numpy()\n",
    "        modulated_gmm.covariances_ = enc_cov.numpy()\n",
    "        if len(train_feat[k][t]) > 0:\n",
    "            W_train[k,t,:] = modulated_gmm.predict_proba(train_feat[k][t]).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c087a8e-7364-45de-94f3-21b4e77d4aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_test = np.zeros((len(data_loader.test_ids), data_loader.n_t_bins, C))\n",
    "for k in range(len(data_loader.test_ids)):\n",
    "    for t in range(data_loader.n_t_bins):\n",
    "        modulated_gmm =  GaussianMixture(n_components=C, covariance_type='full')\n",
    "        modulated_gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(enc_cov))\n",
    "        modulated_gmm.weights_ = enc_pi[:, t, threshold_preds[k]].numpy()\n",
    "        modulated_gmm.means_ = enc_mu.numpy()\n",
    "        modulated_gmm.covariances_ = enc_cov.numpy()\n",
    "        if len(test_feat[k][t]) > 0:\n",
    "            W_test[k,t,:] = modulated_gmm.predict_proba(test_feat[k][t]).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d9f73a-e079-4eca-88e4-d8d98b01cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = W_train.reshape(-1, W_train.shape[1]*W_train.shape[-1])\n",
    "x_test = W_test.reshape(-1, W_test.shape[1]*W_test.shape[-1])\n",
    "decoder = LogisticRegression(random_state=seed, solver='liblinear').fit(x_train, data_loader.y_train)\n",
    "probs = decoder.predict_proba(x_test)\n",
    "preds = probs.argmax(1)\n",
    "acc = accuracy_score(data_loader.y_test, preds)\n",
    "auc = roc_auc_score(choices[data_loader.test_ids], probs)\n",
    "print(f'{acc:.2f}')\n",
    "print(f'{auc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fd6c77-1bcd-4d36-ad6b-5bd5245b3544",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_init_enc_mog_all_accs = []\n",
    "channel_init_enc_mog_all_aucs = []\n",
    "channel_init_enc_mog_po_accs = []\n",
    "channel_init_enc_mog_po_aucs = []\n",
    "channel_init_enc_mog_lp_accs = []\n",
    "channel_init_enc_mog_lp_aucs = []\n",
    "channel_init_enc_mog_dg_accs = []\n",
    "channel_init_enc_mog_dg_aucs = []\n",
    "channel_init_enc_mog_ca1_accs = []\n",
    "channel_init_enc_mog_ca1_aucs = []\n",
    "channel_init_enc_mog_vis_accs = []\n",
    "channel_init_enc_mog_vis_aucs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee192d5-0d47-4bc9-82eb-4ed9105883fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'all acc: {np.mean(channel_init_enc_mog_all_accs):.2f} auc: {np.mean(channel_init_enc_mog_all_aucs):.2f}')\n",
    "print(f'po acc: {np.mean(channel_init_enc_mog_po_accs):.2f} auc: {np.mean(channel_init_enc_mog_po_aucs):.2f}')\n",
    "print(f'lp acc: {np.mean(channel_init_enc_mog_lp_accs):.2f} auc: {np.mean(channel_init_enc_mog_lp_aucs):.2f}')\n",
    "print(f'dg acc: {np.mean(channel_init_enc_mog_dg_accs):.2f} auc: {np.mean(channel_init_enc_mog_dg_aucs):.2f}')\n",
    "print(f'ca1 acc: {np.mean(channel_init_enc_mog_ca1_accs):.2f} auc: {np.mean(channel_init_enc_mog_ca1_aucs):.2f}')\n",
    "print(f'vis acc: {np.mean(channel_init_enc_mog_vis_accs):.2f} auc: {np.mean(channel_init_enc_mog_vis_aucs):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d925ed8-23ae-4015-8985-4afa181d4d0f",
   "metadata": {},
   "source": [
    "#### post split-merge MoG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd207f8-632e-4c12-ace5-c4903e40a797",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_name = '../pretrained/danlab/Subjects/DY_016/post_merge_gmm'\n",
    "means = np.load(gmm_name + '_means.npy')\n",
    "covar = np.load(gmm_name + '_covariances.npy')\n",
    "loaded_gmm = GaussianMixture(n_components=len(means), covariance_type='full')\n",
    "loaded_gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(covar))\n",
    "loaded_gmm.weights_ = np.load(gmm_name + '_weights.npy')\n",
    "loaded_gmm.means_ = means\n",
    "loaded_gmm.covariances_ = covar\n",
    "\n",
    "spike_labels = loaded_gmm.predict(spike_features)\n",
    "spike_probs = loaded_gmm.predict_proba(spike_features)\n",
    "\n",
    "clusterless_neural_data = preprocess.compute_neural_activity(\n",
    "    (spike_times_unsorted, spike_labels, spike_probs),\n",
    "    stim_on_times,\n",
    "    'clusterless', \n",
    "    n_time_bins=30,\n",
    "    regional=True\n",
    ")\n",
    "print(f'clusterless neural data shape: {clusterless_neural_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096cd8ec-e8fa-489e-8b53-1e7b339f6d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = clusterless_neural_data.reshape(-1, clusterless_neural_data.shape[1]*clusterless_neural_data.shape[-1])\n",
    "x_test = clusterless_neural_data.reshape(-1, clusterless_neural_data.shape[1]*clusterless_neural_data.shape[-1])\n",
    "decoder = LogisticRegression(random_state=seed, solver='liblinear').fit(x_train, data_loader.y_train)\n",
    "probs = decoder.predict_proba(x_test)\n",
    "preds = probs.argmax(1)\n",
    "acc = accuracy_score(data_loader.y_test, preds)\n",
    "auc = roc_auc_score(choices[data_loader.test_ids], probs)\n",
    "print(f'{acc:.2f}')\n",
    "print(f'{auc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d82f1-2b24-4dca-803c-38017e71dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "mog_all_accs = []\n",
    "mog_all_aucs = []\n",
    "mog_po_accs = []\n",
    "mog_po_aucs = []\n",
    "mog_lp_accs = []\n",
    "mog_lp_aucs = []\n",
    "mog_dg_accs = []\n",
    "mog_dg_aucs = []\n",
    "mog_ca1_accs = []\n",
    "mog_ca1_aucs = []\n",
    "mog_vis_accs = []\n",
    "mog_vis_aucs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a52196-3732-4e32-be7c-f88fa0cfa8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'all acc: {np.mean(mog_all_accs):.2f} auc: {np.mean(mog_all_aucs):.2f}')\n",
    "print(f'po acc: {np.mean(mog_po_accs):.2f} auc: {np.mean(mog_po_aucs):.2f}')\n",
    "print(f'lp acc: {np.mean(mog_lp_accs):.2f} auc: {np.mean(mog_lp_aucs):.2f}')\n",
    "print(f'dg acc: {np.mean(mog_dg_accs):.2f} auc: {np.mean(mog_dg_aucs):.2f}')\n",
    "print(f'ca1 acc: {np.mean(mog_ca1_accs):.2f} auc: {np.mean(mog_ca1_aucs):.2f}')\n",
    "print(f'vis acc: {np.mean(mog_vis_accs):.2f} auc: {np.mean(mog_vis_aucs):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090da85-ce22-4783-9a67-2c8fae0f3395",
   "metadata": {},
   "source": [
    "#### KS all and good units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1888adc-4d22-4960-bafd-768dd32b6436",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_trials, good_sorted_trials, unsorted_trials, stim_on_times, np1_channel_map= preprocess.load_neural_data(\n",
    "    pid=pid, \n",
    "    trial_data_path=trial_data_path,\n",
    "    neural_data_path=neural_data_path,\n",
    "    behavior_data_path=behavior_data_path,\n",
    "    keep_active_trials=True, \n",
    "    # roi=roi,\n",
    "    roi='all',\n",
    "    kilosort=True,\n",
    "    triage=False,\n",
    "    good_units=True,\n",
    "    thresholding=True\n",
    ")\n",
    "\n",
    "n_trials = stim_on_times.shape[0]\n",
    "sorted = np.vstack([sorted_trials[i] for i in np.arange(n_trials)]) \n",
    "unsorted = np.vstack([unsorted_trials[i] for i in np.arange(n_trials)])\n",
    "spike_times = sorted[:,0]\n",
    "spike_clusters = sorted[:,1]\n",
    "spike_channels = unsorted[:,1]\n",
    "\n",
    "sorted_neural_data = preprocess.compute_neural_activity(\n",
    "    (spike_times, spike_clusters),\n",
    "    stim_on_times,\n",
    "    'sorted', \n",
    "    n_time_bins=10,\n",
    "    # regional=True\n",
    "    regional=False\n",
    ")\n",
    "print(f'sorted neural data shape: {sorted_neural_data.shape}')\n",
    "\n",
    "good_sorted = np.vstack([good_sorted_trials[i] for i in np.arange(n_trials)]) \n",
    "spike_times = good_sorted[:,0]\n",
    "spike_clusters = good_sorted[:,1]\n",
    "\n",
    "good_sorted_neural_data = preprocess.compute_neural_activity(\n",
    "    (spike_times, spike_clusters),\n",
    "    stim_on_times,\n",
    "    'sorted', \n",
    "    n_time_bins=10,\n",
    "    # regional=True\n",
    "    regional=False\n",
    ")\n",
    "print(f'good sorted neural data shape: {good_sorted_neural_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd99c0-85b0-4fe7-96c1-4379ef17ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sorted_neural_data[data_loader.train_ids].reshape(-1, sorted_neural_data.shape[1]*sorted_neural_data.shape[-1])\n",
    "x_test = sorted_neural_data[data_loader.test_ids].reshape(-1, sorted_neural_data.shape[1]*sorted_neural_data.shape[-1])\n",
    "decoder = LogisticRegression(random_state=seed, solver='liblinear').fit(x_train, data_loader.y_train)\n",
    "probs = decoder.predict_proba(x_test)\n",
    "preds = probs.argmax(1)\n",
    "acc = accuracy_score(data_loader.y_test, preds)\n",
    "auc = roc_auc_score(choices[data_loader.test_ids], probs)\n",
    "print(f'{acc:.2f}')\n",
    "print(f'{auc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491c0f2e-06d5-43a9-ac38-94e3dc67f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_all_accs = []\n",
    "ks_all_aucs = []\n",
    "ks_po_accs = []\n",
    "ks_po_aucs = []\n",
    "ks_lp_accs = []\n",
    "ks_lp_aucs = []\n",
    "ks_dg_accs = []\n",
    "ks_dg_aucs = []\n",
    "ks_ca1_accs = []\n",
    "ks_ca1_aucs = []\n",
    "ks_vis_accs = []\n",
    "ks_vis_aucs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc215ee2-43fd-4cde-81ad-63c864b46e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'all acc: {np.mean(ks_all_accs):.2f} auc: {np.mean(ks_all_aucs):.2f}')\n",
    "print(f'po acc: {np.mean(ks_po_accs):.2f} auc: {np.mean(ks_po_aucs):.2f}')\n",
    "print(f'lp acc: {np.mean(ks_lp_accs):.2f} auc: {np.mean(ks_lp_aucs):.2f}')\n",
    "print(f'dg acc: {np.mean(ks_dg_accs):.2f} auc: {np.mean(ks_dg_aucs):.2f}')\n",
    "print(f'ca1 acc: {np.mean(ks_ca1_accs):.2f} auc: {np.mean(ks_ca1_aucs):.2f}')\n",
    "print(f'vis acc: {np.mean(ks_vis_accs):.2f} auc: {np.mean(ks_vis_aucs):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0764980e-adf4-4cfb-b06b-fcd7a8cd6ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = good_sorted_neural_data[data_loader.train_ids].reshape(-1, good_sorted_neural_data.shape[1]*good_sorted_neural_data.shape[-1])\n",
    "x_test = good_sorted_neural_data[data_loader.test_ids].reshape(-1, good_sorted_neural_data.shape[1]*good_sorted_neural_data.shape[-1])\n",
    "decoder = LogisticRegression(random_state=seed, solver='liblinear').fit(x_train, data_loader.y_train)\n",
    "probs = decoder.predict_proba(x_test)\n",
    "preds = probs.argmax(1)\n",
    "acc = accuracy_score(data_loader.y_test, preds)\n",
    "auc = roc_auc_score(choices[data_loader.test_ids], probs)\n",
    "print(f'{acc:.2f}')\n",
    "print(f'{auc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99146dc-fb8b-494f-934e-8e5d82b333f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_all_accs = []\n",
    "good_all_aucs = []\n",
    "good_po_accs = []\n",
    "good_po_aucs = []\n",
    "good_lp_accs = []\n",
    "good_lp_aucs = []\n",
    "good_dg_accs = []\n",
    "good_dg_aucs = []\n",
    "good_ca1_accs = []\n",
    "good_ca1_aucs = []\n",
    "good_vis_accs = []\n",
    "good_vis_aucs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2be8045-5596-4cdc-b02e-73a4d91e2e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'all acc: {np.mean(good_all_accs):.2f} auc: {np.mean(good_all_aucs):.2f}')\n",
    "print(f'po acc: {np.mean(good_po_accs):.2f} auc: {np.mean(good_po_aucs):.2f}')\n",
    "print(f'po acc: {np.mean(good_lp_accs):.2f} auc: {np.mean(good_lp_aucs):.2f}')\n",
    "print(f'dg acc: {np.mean(good_dg_accs):.2f} auc: {np.mean(good_dg_aucs):.2f}')\n",
    "print(f'ca1 acc: {np.mean(good_ca1_accs):.2f} auc: {np.mean(good_ca1_aucs):.2f}')\n",
    "print(f'vis acc: {np.mean(good_vis_accs):.2f} auc: {np.mean(good_vis_aucs):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea606755-3bd4-45d2-96db-3d7a0129872b",
   "metadata": {},
   "source": [
    "#### plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe6ace5-86d4-4fd0-8260-1a8ad0a8ca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "dy09_good_units = np.array([\n",
    " \n",
    "])\n",
    "\n",
    "dy09_sorted = np.array([\n",
    "\n",
    "])\n",
    "\n",
    "dy09_thresholded = np.array([\n",
    "  \n",
    "])\n",
    "\n",
    "\n",
    "dy09_mog = np.array([\n",
    " \n",
    "])\n",
    "\n",
    "dy09_channel_init_cavi = np.array([\n",
    " \n",
    "])\n",
    "\n",
    "dy09_xval_init_cavi = np.array([\n",
    " \n",
    "])\n",
    "\n",
    "\n",
    "dy16_good_units = np.array([\n",
    " \n",
    "])\n",
    "\n",
    "dy16_sorted = np.array([\n",
    "\n",
    "])\n",
    "\n",
    "dy16_thresholded = np.array([\n",
    "  \n",
    "])\n",
    "\n",
    "\n",
    "dy16_mog = np.array([\n",
    " \n",
    "])\n",
    "\n",
    "dy16_channel_init_cavi = np.array([\n",
    " \n",
    "])\n",
    "\n",
    "dy16_xval_init_cavi = np.array([\n",
    " \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9fdc9d-6791-49e1-b270-6301290705a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(24, 12))\n",
    "\n",
    "ticks = ['all', 'po', 'lp', 'dg', 'ca1', 'vis']\n",
    "\n",
    "mins, maxs, means, stds = dy16_good_units.min(1), dy16_good_units.max(1), dy16_good_units.mean(1), dy16_good_units.std(1)\n",
    "ax.errorbar(np.arange(len(ticks))*2-.2, means, stds, \n",
    "                 fmt='ok', ecolor='goldenrod', lw=8, label='\"good\" units')\n",
    "ax.errorbar(np.arange(len(ticks))*2-.2, means, [means - mins, maxs - means],\n",
    "                 fmt='ok', ecolor='gray', lw=4)\n",
    "\n",
    "mins, maxs, means, stds = dy09_good_units.min(1), dy09_good_units.max(1), dy09_good_units.mean(1), dy09_good_units.std(1)\n",
    "ax.errorbar(np.arange(len(ticks))*2-.8, means, stds, \n",
    "                 fmt='ok', ecolor='goldenrod', lw=8)\n",
    "ax.errorbar(np.arange(len(ticks))*2-.8, means, [means - mins, maxs - means],\n",
    "                 fmt='ok', ecolor='gray', lw=4)\n",
    "\n",
    "mins, maxs, means, stds = dy16_sorted.min(1), dy16_sorted.max(1), dy16_sorted.mean(1), dy16_sorted.std(1)\n",
    "ax.errorbar(np.arange(len(ticks))*2-.1, means, stds, \n",
    "                 fmt='ok', ecolor='royalblue', lw=8, label='sorted')\n",
    "ax.errorbar(np.arange(len(ticks))*2-.1, means, [means - mins, maxs - means],\n",
    "                 fmt='ok', ecolor='gray', lw=4)\n",
    "\n",
    "mins, maxs, means, stds = dy09_sorted.min(1), dy09_sorted.max(1), dy09_sorted.mean(1), dy09_sorted.std(1)\n",
    "ax.errorbar(np.arange(len(ticks))*2-.7, means, stds, \n",
    "                 fmt='ok', ecolor='royalblue', lw=8)\n",
    "ax.errorbar(np.arange(len(ticks))*2-.7, means, [means - mins, maxs - means],\n",
    "                 fmt='ok', ecolor='gray', lw=4)\n",
    "\n",
    "mins, maxs, means, stds = dy16_thresholded.min(1), dy16_thresholded.max(1), dy16_thresholded.mean(1), dy16_thresholded.std(1)\n",
    "ax.errorbar(np.arange(len(ticks))*2, means, stds, \n",
    "             fmt='ok', ecolor='teal', lw=8, label='thresholded')\n",
    "ax.errorbar(np.arange(len(ticks))*2, means, [means - mins, maxs - means],\n",
    "                 fmt='.k', ecolor='gray', lw=4)\n",
    "\n",
    "mins, maxs, means, stds = dy09_thresholded.min(1), dy09_thresholded.max(1), dy09_thresholded.mean(1), dy09_thresholded.std(1)\n",
    "ax.errorbar(np.arange(len(ticks))*2-.6, means, stds, \n",
    "             fmt='ok', ecolor='teal', lw=8)\n",
    "ax.errorbar(np.arange(len(ticks))*2-.6, means, [means - mins, maxs - means],\n",
    "                 fmt='.k', ecolor='gray', lw=4)\n",
    "\n",
    "mins, maxs, means, stds = dy16_mog.min(1), dy16_mog.max(1), dy16_mog.mean(1), dy16_mog.std(1)\n",
    "ax.errorbar(np.arange(len(ticks))*2+.1, means, stds, \n",
    "             fmt='ok', ecolor='coral', lw=8, label='cavi+MoG')\n",
    "ax.errorbar(np.arange(len(ticks))*2+.1, means, [means - mins, maxs - means],\n",
    "                 fmt='ok', ecolor='gray', lw=4)\n",
    "\n",
    "mins, maxs, means, stds = dy09_mog.min(1), dy09_mog.max(1), dy09_mog.mean(1), dy09_mog.std(1)\n",
    "ax.errorbar(np.arange(len(ticks))*2-.5, means, stds, \n",
    "             fmt='ok', ecolor='coral', lw=8)\n",
    "ax.errorbar(np.arange(len(ticks))*2-.5, means, [means - mins, maxs - means],\n",
    "                 fmt='ok', ecolor='gray', lw=4)\n",
    "\n",
    "mins, maxs, means, stds = dy16_channel_init_cavi.min(1), dy16_channel_init_cavi.max(1), \n",
    "                          dy16_channel_init_cavi.mean(1), dy16_channel_init_cavi.std(1)\n",
    "ax.errorbar(np.arange(len(ticks))*2+.2, means, stds, \n",
    "             fmt='ok', ecolor='coral', lw=8, label='cavi+MoG')\n",
    "ax.errorbar(np.arange(len(ticks))*2+.2, means, [means - mins, maxs - means],\n",
    "                 fmt='ok', ecolor='gray', lw=4)\n",
    "\n",
    "mins, maxs, means, stds = dy09_channel_init_cavi.min(1), dy09_channel_init_cavi.max(1), \n",
    "                          dy09_channel_init_cavi.mean(1), dy09_channel_init_cavi.std(1)\n",
    "ax.errorbar(np.arange(len(ticks))*2-.4, means, stds, \n",
    "             fmt='ok', ecolor='coral', lw=8)\n",
    "ax.errorbar(np.arange(len(ticks))*2-.4, means, [means - mins, maxs - means],\n",
    "                 fmt='ok', ecolor='gray', lw=4)\n",
    "\n",
    "mins, maxs, means, stds = dy16_xval_init_cavi.min(1), dy16_xval_init_cavi.max(1), \n",
    "                          dy16_xval_init_cavi.mean(1), dy16_xval_init_cavi.std(1)\n",
    "ax.errorbar(np.arange(len(ticks))*2+.3, means, stds, \n",
    "             fmt='ok', ecolor='coral', lw=8, label='cavi+MoG')\n",
    "ax.errorbar(np.arange(len(ticks))*2+.3, means, [means - mins, maxs - means],\n",
    "                 fmt='ok', ecolor='gray', lw=4)\n",
    "\n",
    "mins, maxs, means, stds = dy09_xval_init_cavi.min(1), dy09_xval_init_cavi.max(1), \n",
    "                          dy09_xval_init_cavi.mean(1), dy09_xval_init_cavi.std(1)\n",
    "ax.errorbar(np.arange(len(ticks))*2-.3, means, stds, \n",
    "             fmt='ok', ecolor='coral', lw=8)\n",
    "ax.errorbar(np.arange(len(ticks))*2-.3, means, [means - mins, maxs - means],\n",
    "                 fmt='ok', ecolor='gray', lw=4)\n",
    "\n",
    "# ax.legend(loc='lower left')\n",
    "# ax.set_xticks([-.25, 1.75, 3.75, 5.75, 7.75, 9.75], ticks)\n",
    "ax.set_ylabel('accuracy')\n",
    "# ax.text(-1.15, 1.04, 'visual decision')\n",
    "\n",
    "# ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.08),\n",
    "#           ncol=4, fancybox=False, shadow=False, frameon=False)\n",
    "\n",
    "# ax.axvspan(-.9, -.35, facecolor='lavender')\n",
    "# ax.axvspan(-.35, .2, facecolor='whitesmoke')\n",
    "# ax.axvspan(1.1, 1.65, facecolor='lavender')\n",
    "# ax.axvspan(1.65, 2.2, facecolor='whitesmoke')\n",
    "# ax.axvspan(3.1, 3.65, facecolor='lavender')\n",
    "# ax.axvspan(3.65, 4.2, facecolor='whitesmoke')\n",
    "# ax.axvspan(5.1, 5.65, facecolor='lavender')\n",
    "# ax.axvspan(5.65, 6.2, facecolor='whitesmoke')\n",
    "# ax.axvspan(7.1, 7.65, facecolor='lavender')\n",
    "# ax.axvspan(7.65, 8.2, facecolor='whitesmoke')\n",
    "# ax.axvspan(9.1, 9.65, facecolor='lavender')\n",
    "# ax.axvspan(9.65, 10.2, facecolor='whitesmoke')\n",
    "\n",
    "# ax.text(8.98, 1.04, 'DY-016', bbox={'facecolor': 'lavender', 'edgecolor':'none', 'pad':4})\n",
    "# ax.text(9.77, 1.04, 'DY-009', bbox={'facecolor': 'whitesmoke', 'edgecolor':'none', 'pad':4})\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('compare_decoders.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a33d8d4-8275-4a3a-8678-f3a66b12d46b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
