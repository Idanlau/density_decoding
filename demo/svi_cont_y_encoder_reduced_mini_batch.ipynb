{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a3a3c6-c029-486d-9a9b-d3bc1cf7c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import logsumexp\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.distributions as D\n",
    "\n",
    "import isosplit\n",
    "\n",
    "from clusterless import preprocess\n",
    "from clusterless import decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c83eddd-64ec-44da-91f6-be3c8880cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 666\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "804affc9-ffa2-4b40-98c1-e33a5fc928db",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 15\n",
    "MEDIUM_SIZE = 20\n",
    "BIGGER_SIZE = 25\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)         \n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     \n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE) \n",
    "plt.rc('axes', linewidth = 1.5)\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)   \n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)   \n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)   \n",
    "plt.rc('figure', titlesize=MEDIUM_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779810ad-6236-439a-9c42-920935cc363c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cd1c562-85c3-4d92-98da-b1ae930d9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = 'febb430e-2d50-4f83-87a0-b5ffbb9a4943'\n",
    "rootpath = '/mnt/3TB/yizi/Downloads/ONE/openalyx.internationalbrainlab.org'\n",
    "trial_data_path = rootpath + '/danlab/Subjects/DY_009/2020-02-27/001/alf'\n",
    "neural_data_path = '/mnt/3TB/yizi/danlab/Subjects/DY_009'\n",
    "behavior_data_path = rootpath + '/paper_repro_ephys_data/figure9_10/original_data'\n",
    "save_path = '../saved_results/danlab/Subjects/DY_009'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f1f05f-1250-40a9-8b16-88469fef6e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = 'vis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2d94ef8-0d09-4bb5-9342-3179e7c0ba04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pid: febb430e-2d50-4f83-87a0-b5ffbb9a4943\n",
      "eid: db4df448-e449-4a6f-a0e7-288711e7a75a\n",
      "found 85 good ibl units ..\n",
      "1st trial stim on time: 40.81, last trial stim on time 2252.10\n",
      "found 29 neurons in region vis ...\n",
      "found 40 channels in region vis ...\n"
     ]
    }
   ],
   "source": [
    "sorted_trials, good_sorted_trials, unsorted_trials, stim_on_times, np1_channel_map= preprocess.load_neural_data(\n",
    "    pid=pid, \n",
    "    trial_data_path=trial_data_path,\n",
    "    neural_data_path=neural_data_path,\n",
    "    behavior_data_path=behavior_data_path,\n",
    "    keep_active_trials=True, \n",
    "    # roi='all',\n",
    "    roi = roi,\n",
    "    kilosort=True,\n",
    "    triage=False,\n",
    "    good_units=True,\n",
    "    thresholding=True\n",
    ")\n",
    "\n",
    "behave_dict = preprocess.load_behaviors_data(behavior_data_path, pid)\n",
    "motion_energy, wheel_velocity, wheel_speed, paw_speed, nose_speed, pupil_diameter = preprocess.preprocess_dynamic_behaviors(behave_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7da5cbb8-4cf4-45c4-a61e-97c97bf63423",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, data, y, stim_on_times, np1_channel_map, n_t_bins=30):\n",
    "        self.data = data\n",
    "        self.y = y\n",
    "        self.stim_on_times = stim_on_times\n",
    "        self.np1_channel_map = np1_channel_map\n",
    "        self.n_t_bins = n_t_bins\n",
    "        self.n_trials = stim_on_times.shape[0]\n",
    "        self.n_channels = np1_channel_map.shape[0]\n",
    "        self.t_binning = np.arange(0, 1.5, step = (1.5 - 0) / n_t_bins)\n",
    "        self.rand_trial_ids = np.arange(self.n_trials)\n",
    "        \n",
    "        # allocate unsorted data into trials\n",
    "        self.trial_ids = []\n",
    "        self.t_ids = []\n",
    "        self.trials = []\n",
    "        self.t_bins = []\n",
    "        for k in range(self.n_trials):\n",
    "            mask = np.logical_and(data[:,0] >= stim_on_times[k] - 0.5,\n",
    "                                  data[:,0] <= stim_on_times[k] + 1)\n",
    "            trial = data[mask,:]\n",
    "            trial[:,0] = trial[:,0] - trial[:,0].min()\n",
    "            t_bins = np.digitize(trial[:,0], self.t_binning, right = False) - 1\n",
    "            t_bin_lst = []\n",
    "            for t in range(self.n_t_bins):\n",
    "                t_bin = trial[t_bins == t,1:]\n",
    "                self.trial_ids.append(np.ones_like(t_bin[:,0]) * k)\n",
    "                self.t_ids.append(np.ones_like(t_bin[:,0]) * t)\n",
    "                t_bin_lst.append(t_bin)\n",
    "            self.trials.append(t_bin_lst)\n",
    "    \n",
    "    \n",
    "    def split_train_test(self, train_ids, test_ids):\n",
    "        \n",
    "        self.train_ids = self.rand_trial_ids[train_ids]\n",
    "        self.test_ids = self.rand_trial_ids[test_ids]\n",
    "        self.y_train = self.y[self.train_ids]\n",
    "        self.y_test = self.y[self.test_ids]\n",
    "        \n",
    "        trial_ids = np.concatenate(self.trial_ids)\n",
    "        t_ids = np.concatenate(self.t_ids)\n",
    "        trials = np.concatenate(np.concatenate(self.trials))\n",
    "\n",
    "        train_mask = np.sum([trial_ids == idx for idx in self.train_ids], axis=0).astype(bool)\n",
    "        test_mask = np.sum([trial_ids == idx for idx in self.test_ids], axis=0).astype(bool)\n",
    "        train_trial_ids, test_trial_ids = trial_ids[train_mask], trial_ids[test_mask]\n",
    "        train_t_ids, test_t_ids = t_ids[train_mask], t_ids[test_mask]\n",
    "        train_trials, test_trials = trials[train_mask], trials[test_mask]\n",
    "        \n",
    "        return train_trials, train_trial_ids, train_t_ids, \\\n",
    "               test_trials, test_trial_ids, test_t_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a170e94-7388-43ae-8367-f680c532cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(data = np.concatenate(unsorted_trials)[:,[0,1,2,3,4]], \n",
    "                         y = wheel_velocity, \n",
    "                         stim_on_times = stim_on_times, \n",
    "                         np1_channel_map = np1_channel_map, \n",
    "                         n_t_bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5a9abac-d7de-40fc-ac4a-27aaaa18da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "# kf = KFold(n_splits=5, shuffle=False)\n",
    "kf_train_ids = []; kf_test_ids = []\n",
    "for i, (train_ids, test_ids) in enumerate(kf.split(data_loader.y)):\n",
    "    kf_train_ids.append(train_ids)\n",
    "    kf_test_ids.append(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b5143bd-bd3d-4f59-b420-f83402c51a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "train_trials, train_trial_ids, train_t_ids, \\\n",
    "test_trials, test_trial_ids, test_t_ids = data_loader.split_train_test(\n",
    "    train_ids = kf_train_ids[i], test_ids = kf_test_ids[i]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c30d5815-3716-476f-96eb-fb91968e7935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel 344.0 has 1 modes ...\n",
      "channel 345.0 has 3 modes ...\n",
      "channel 346.0 has 1 modes ...\n",
      "channel 349.0 has 3 modes ...\n",
      "channel 350.0 has 2 modes ...\n",
      "channel 354.0 has 1 modes ...\n",
      "channel 355.0 has 1 modes ...\n",
      "channel 358.0 has 1 modes ...\n",
      "channel 361.0 has 1 modes ...\n",
      "channel 362.0 has 1 modes ...\n",
      "channel 365.0 has 1 modes ...\n",
      "channel 366.0 has 1 modes ...\n",
      "channel 369.0 has 1 modes ...\n",
      "channel 373.0 has 1 modes ...\n"
     ]
    }
   ],
   "source": [
    "sub_weights_lst = []\n",
    "sub_means_lst = []\n",
    "sub_covs_lst = []\n",
    "\n",
    "all_trials = np.vstack([train_trials, test_trials])\n",
    "for channel in np.unique(all_trials[:,0]):\n",
    "    sub_s = all_trials[all_trials[:,0] == channel, 1:]\n",
    "    \n",
    "    if sub_s.shape[0] > 10:\n",
    "        try:\n",
    "            isosplit_labels = isosplit.isosplit(sub_s.T, K_init=20, min_cluster_size=10,\n",
    "                                                whiten_cluster_pairs=1, refine_clusters=1)\n",
    "        except AssertionError:\n",
    "            continue\n",
    "        except ValueError:\n",
    "            continue\n",
    "    elif sub_s.shape[0] < 2:\n",
    "        continue\n",
    "    else:\n",
    "        sub_gmm = GaussianMixture(n_components=1, \n",
    "                              covariance_type='full',\n",
    "                              init_params='k-means++', \n",
    "                              verbose=0)\n",
    "        sub_gmm.fit(sub_s)\n",
    "        sub_labels = sub_gmm.predict(sub_s)\n",
    "        sub_weights = len(sub_labels)/len(all_trials)\n",
    "        sub_weights_lst.append(sub_weights)\n",
    "        sub_means_lst.append(sub_gmm.means_)\n",
    "        sub_covs_lst.append(sub_gmm.covariances_)\n",
    "        continue\n",
    "    \n",
    "    n_splits = np.unique(isosplit_labels).shape[0]\n",
    "    print(f'channel {channel} has {n_splits} modes ...')\n",
    "    \n",
    "    if n_splits == 1: \n",
    "        sub_gmm = GaussianMixture(n_components=1, \n",
    "                              covariance_type='full',\n",
    "                              init_params='k-means++', \n",
    "                              verbose=0)\n",
    "        sub_gmm.fit(sub_s)\n",
    "        sub_labels = sub_gmm.predict(sub_s)\n",
    "        sub_weights = len(sub_labels)/len(all_trials)\n",
    "        sub_weights_lst.append(sub_weights)\n",
    "        sub_means_lst.append(sub_gmm.means_)\n",
    "        sub_covs_lst.append(sub_gmm.covariances_)\n",
    "    else:\n",
    "        for label in np.arange(n_splits):\n",
    "            mask = isosplit_labels == label\n",
    "            sub_gmm = GaussianMixture(n_components=1, \n",
    "                              covariance_type='full',\n",
    "                              init_params='k-means++', \n",
    "                              verbose=0)\n",
    "            sub_gmm.fit(sub_s[mask])\n",
    "            sub_labels = sub_gmm.predict(sub_s[mask])\n",
    "            sub_weights = len(sub_labels)/len(all_trials)\n",
    "            sub_weights_lst.append(sub_weights)\n",
    "            sub_means_lst.append(sub_gmm.means_)\n",
    "            sub_covs_lst.append(sub_gmm.covariances_)\n",
    "            \n",
    "sub_weights = np.hstack(sub_weights_lst)\n",
    "sub_means = np.vstack(sub_means_lst)\n",
    "sub_covs = np.vstack(sub_covs_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60ab6f4f-8f41-44fe-9d71-caf0ec38132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_s_lst = []\n",
    "# sub_weights_lst = []\n",
    "# sub_means_lst = []\n",
    "# sub_covs_lst = []\n",
    "# for channel in np.unique(train_trials[:,0]):\n",
    "#     sub_s = train_trials[train_trials[:,0] == channel, 1:]\n",
    "#     sub_s_lst.append(sub_s)\n",
    "#     if len(sub_s) > 1:\n",
    "#         sub_gmm = GaussianMixture(n_components=1, \n",
    "#                           covariance_type='full',\n",
    "#                           init_params='k-means++', verbose=0)\n",
    "#         sub_gmm.fit(sub_s)\n",
    "#         sub_labels = sub_gmm.predict(sub_s)\n",
    "#         sub_weights = len(sub_s)/len(train_trials)\n",
    "#         sub_weights_lst.append(sub_weights)\n",
    "#         sub_means_lst.append(sub_gmm.means_)\n",
    "#         sub_covs_lst.append(sub_gmm.covariances_)\n",
    "        \n",
    "# sub_weights = np.hstack(sub_weights_lst)\n",
    "# sub_means = np.vstack(sub_means_lst)\n",
    "# sub_covs = np.vstack(sub_covs_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c71ad14-8a99-432f-bcc3-784062f90886",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=len(sub_weights), covariance_type='full', init_params='k-means++')\n",
    "gmm.weights_ = sub_weights\n",
    "gmm.means_ = sub_means\n",
    "gmm.covariances_ = sub_covs\n",
    "gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sub_covs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7484aa0b-7a3d-482a-ae88-0ba05c2ca98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmm = GaussianMixture(n_components=100, \n",
    "#                       covariance_type='full', \n",
    "#                       init_params='k-means++',\n",
    "#                       verbose=0)\n",
    "# gmm.fit(train_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ea3b25d-4b85-4a19-850c-e2f17ac7b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.tensor(train_trials[:,1:])\n",
    "y = torch.tensor(data_loader.y)\n",
    "ks = torch.tensor(train_trial_ids)\n",
    "ts = torch.tensor(train_t_ids)\n",
    "\n",
    "Nk = len(data_loader.train_ids)\n",
    "Nt = data_loader.n_t_bins\n",
    "Nc = gmm.means_.shape[0]\n",
    "Nd = gmm.means_.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eabf2359-e97b-4156-b9d5-b4deb23146e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b54db6c-22c2-4ac2-914a-3080c28a3f91",
   "metadata": {},
   "source": [
    "#### CAVI-SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "492ce76e-fa9c-468e-a939-4f42136ff111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_log(x, minval=1e-10):\n",
    "    return torch.log(x + minval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f1111f2-813f-490f-9320-11305244d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAVI(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, Nk, Nt, Nc, Nd, init_means, init_covs):\n",
    "        super(CAVI, self).__init__()\n",
    "        self.Nk = Nk\n",
    "        self.Nt = Nt\n",
    "        self.Nc = Nc\n",
    "        self.Nd = Nd\n",
    "        self.ks = ks\n",
    "        self.ts = ts\n",
    "        \n",
    "        # initialize variables for variational distribution\n",
    "        self.means = torch.nn.Parameter(torch.tensor(init_means), requires_grad=False)\n",
    "        self.covs = torch.nn.Parameter(torch.tensor(init_covs), requires_grad=False)\n",
    "        self.bs = torch.nn.Parameter(torch.randn((Nc)))\n",
    "        self.betas = torch.nn.Parameter(torch.randn((Nc, Nt)))\n",
    "\n",
    "        \n",
    "    def forward(self, s, y, ks, ts):\n",
    "        \n",
    "        # compute log-lambdas\n",
    "        log_lambdas = torch.zeros((self.Nk, self.Nc, self.Nt))\n",
    "        for k in range(self.Nk):\n",
    "            for t in range(self.Nt):\n",
    "                log_lambdas[k,:,t] = self.bs + self.betas[:,t] * y[k][t]\n",
    "                \n",
    "        \n",
    "        # compute mixing proportions \n",
    "        log_pis = log_lambdas - torch.logsumexp(log_lambdas, 1)[:,None,:]\n",
    "        \n",
    "        # compute log-likelihood\n",
    "        ll = torch.zeros((s.shape[0], self.Nc))\n",
    "        for j in range(self.Nc):\n",
    "            ll[:,j] = D.multivariate_normal.MultivariateNormal(\n",
    "                            loc=self.means[j], \n",
    "                            covariance_matrix=self.covs[j]\n",
    "                        ).log_prob(s)\n",
    "            \n",
    "        \n",
    "        # order of update is: E step -> compute ELBO -> M step\n",
    "        # E step\n",
    "        r = torch.zeros((s.shape[0], self.Nc))\n",
    "        for k in range(self.Nk):\n",
    "            for t in range(self.Nt):\n",
    "                k_t_idx = torch.logical_and(ks == torch.unique(ks).int()[k], ts == t)\n",
    "                r[k_t_idx] = torch.exp( ll[k_t_idx] + log_pis[k,:,t] )\n",
    "                r[k_t_idx] = r[k_t_idx] / r[k_t_idx].sum(1)[:,None]\n",
    "                \n",
    "                    \n",
    "        # compute ELBO\n",
    "        elbo_1 = 0; elbo_2 = 0; elbo_3 = 0\n",
    "        elbo = 0\n",
    "        for k in range(self.Nk):\n",
    "            for t in range(self.Nt):\n",
    "                k_t_idx = torch.logical_and(ks == torch.unique(ks).int()[k], ts == t)\n",
    "                elbo_1 += torch.sum( r[k_t_idx] * ll[k_t_idx] )\n",
    "                elbo_2 += torch.sum( r[k_t_idx] * log_pis[k,:,t] )\n",
    "                elbo_3 -= torch.sum( r[k_t_idx] * safe_log(r[k_t_idx]) )\n",
    "                \n",
    "                \n",
    "        # M step is done via back propagation\n",
    "                \n",
    "        return elbo_1 + elbo_2 + elbo_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ebe5bce-0f81-4b88-91af-5ebc6f6b5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 6\n",
    "batch_size = 1\n",
    "batch_ids = list(zip(*(iter(data_loader.train_ids),) * batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff3b3c74-68d0-44f3-8a79-b019d37793bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cavi = CAVI(batch_size, Nt, Nc, Nd, gmm.means_, gmm.covariances_)\n",
    "optim = torch.optim.Adam(cavi.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b92bc-5dfa-4b9c-a2bf-c6d30a8a07f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1 batch 100\n",
      "iter: 1 total elbo: -295900.53\n",
      "iter: 2 batch 100\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_iter = 100\n",
    "elbos = []\n",
    "N = s.shape[0]\n",
    "for i in range(max_iter):\n",
    "    tot_elbo = 0\n",
    "    for n, batch_idx in enumerate(batch_ids): \n",
    "        mask = torch.logical_and(ks >= batch_idx[0], ks <= batch_idx[-1])\n",
    "        batch_s = s[mask]\n",
    "        batch_y = y[list(batch_idx)]\n",
    "        batch_ks = ks[mask]\n",
    "        batch_ts = ts[mask]\n",
    "        batch_elbo = cavi(batch_s, batch_y, batch_ks, batch_ts)\n",
    "        tot_elbo += batch_elbo\n",
    "        loss = - batch_elbo\n",
    "        loss.backward()\n",
    "        if (n+1) % 100 == 0:\n",
    "            print(f'iter: {i+1} batch {n+1}')\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    print(f'iter: {i+1} total elbo: {tot_elbo:.2f}')\n",
    "    elbos.append(tot_elbo.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82095003-f54d-4b03-a4ee-15642116a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbos = [elbo for elbo in elbos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3079c2c6-e52f-411c-90e7-d3aa801e1201",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,2))\n",
    "plt.plot(elbos)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"ELBO\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb15cc9-efcb-45f8-a35e-e83f6eeca259",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lambdas = torch.zeros((Nk, Nc, Nt))\n",
    "for k in range(Nk):\n",
    "    for t in range(Nt):\n",
    "        log_lambdas[k,:,t] = cavi.bs + cavi.betas[:,t] * y[k][t]\n",
    "\n",
    "log_pis = log_lambdas - torch.logsumexp(log_lambdas, 1)[:,None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3332c075-0d43-4e7e-b58e-550a25717447",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "plt.imshow(torch.exp(log_pis.mean(0)).detach().numpy(), \n",
    "           aspect='auto', cmap='cubehelix')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d830d7-8ec4-4fd5-82a5-32e83a71b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "plt.plot(cavi.betas[-1,:].detach().numpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b889923-398e-4bcf-9ae7-5c68ac3fcc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmm.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccef1f4-77da-4d5b-9c55-484ea773e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cavi.means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578669c5-2879-4329-874d-fc0741d4df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_y_enc_res = {\n",
    "    'bs': cavi.bs,\n",
    "    'betas': cavi.betas,\n",
    "    'means': cavi.means,\n",
    "    'covs': cavi.covs\n",
    "}\n",
    "np.save(save_path + f'dy009_cont_y_enc_res_c{len(cavi.means)}.npy', cont_y_enc_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b034a422-b578-437c-8995-ce9f257baace",
   "metadata": {},
   "source": [
    "#### MoG only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f69e95-d913-4d46-a2c7-ca8da37bf755",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trials = np.concatenate(np.concatenate(data_loader.trials))[:,1:]\n",
    "spike_times = data_loader.data[:,0]\n",
    "\n",
    "spike_labels = []\n",
    "spike_probs = []\n",
    "spike_labels.extend(gmm.predict(all_trials))\n",
    "spike_probs.extend(gmm.predict_proba(all_trials))\n",
    "spike_labels = np.array(spike_labels)\n",
    "spike_probs = np.array(spike_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d23492-d63b-409b-b40e-e9604b044406",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_gmm = preprocess.compute_neural_activity(\n",
    "    (spike_times, spike_labels, spike_probs),\n",
    "    data_loader.stim_on_times,\n",
    "    'clusterless', \n",
    "    n_time_bins=data_loader.n_t_bins\n",
    ")\n",
    "print(enc_gmm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65719e8c-79f5-4c0f-992d-40bba4944f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_loader.train_ids\n",
    "test = data_loader.test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f1551-d910-4273-abe4-eeb92f4aaa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = enc_gmm.reshape(-1, enc_gmm.shape[1] * enc_gmm.shape[2])[train]\n",
    "x_test = enc_gmm.reshape(-1, enc_gmm.shape[1] * enc_gmm.shape[2])[test]\n",
    "y_train = data_loader.y[train]\n",
    "\n",
    "ridge = Ridge(alpha=2000)\n",
    "ridge.fit(x_train, y_train)\n",
    "y_hat = ridge.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f6003d-9275-42c0-9935-3a7c35c25d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R2 = {r2_score(data_loader.y[test], y_hat):.3f}')\n",
    "print(f'MSE = {mean_squared_error(data_loader.y[test], y_hat):.3f}')\n",
    "print(f'corr = {pearsonr(data_loader.y[test].flatten(), y_hat.flatten()).statistic:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dc4738-c384-4c1d-989e-abc745f800cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, ExpSineSquared, RationalQuadratic, RBF\n",
    "\n",
    "long_term_trend_kernel = 50.0**2 * RBF(length_scale=50.0)\n",
    "seasonal_kernel = (\n",
    "    2.0**2\n",
    "    * RBF(length_scale=100.0)\n",
    "    * ExpSineSquared(length_scale=1.0, periodicity=1.0, periodicity_bounds=\"fixed\")\n",
    ")\n",
    "# irregularities_kernel = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
    "# noise_kernel = 0.1**2 * RBF(length_scale=0.1) + WhiteKernel(\n",
    "#     noise_level=0.1**2, noise_level_bounds=(1e-5, 1e5)\n",
    "# )\n",
    "kernel = (\n",
    "    long_term_trend_kernel #+ seasonal_kernel + noise_kernel #+ irregularities_kernel \n",
    ")\n",
    "gaussian_process = GaussianProcessRegressor(kernel=kernel, alpha=1e-10)\n",
    "start_time = time.time()\n",
    "gaussian_process.fit(x_train, y_train)\n",
    "print(\n",
    "    f\"Time for GaussianProcessRegressor fitting: {time.time() - start_time:.3f} seconds\"\n",
    ")\n",
    "gp_pred = gaussian_process.predict(x_test)\n",
    "print(f'R2 = {r2_score(data_loader.y[test], gp_pred):.3f}')\n",
    "print(f'MSE = {mean_squared_error(data_loader.y[test], gp_pred):.3f}')\n",
    "print(f'corr = {pearsonr(data_loader.y[test].flatten(), gp_pred.flatten()).statistic:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba48647-cb49-46d8-87c6-a09b25787c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 2))\n",
    "plt.plot(data_loader.y[test].flatten()[:200], c='gray', linestyle='dashed', label='observed');\n",
    "plt.plot(gp_pred.flatten()[:200], c='blue', alpha=.6, label='predicted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc7666-b82a-4f9d-acb1-697baa81c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_enc_gmm, half_window_size, n_windows = decoder.sliding_window(\n",
    "    enc_gmm, \n",
    "    data_loader.n_trials,\n",
    "    window_size = 7\n",
    ")\n",
    "windowed_y = data_loader.y[:,half_window_size:n_windows].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c561ff5a-a9fb-418a-ac9b-ac4abdb9fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_by_trial = windowed_enc_gmm.reshape((data_loader.n_trials, -1))\n",
    "y_by_trial = windowed_y.reshape((data_loader.n_trials, -1))\n",
    "x_train, x_test = x_by_trial[train], x_by_trial[test]\n",
    "y_train, y_test = y_by_trial[train], y_by_trial[test]\n",
    "\n",
    "x_train = x_train.reshape((-1, windowed_enc_gmm.shape[1]))\n",
    "x_test = x_test.reshape((-1, windowed_enc_gmm.shape[1]))\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "ridge = Ridge(alpha=10000)\n",
    "ridge.fit(x_train, y_train)\n",
    "y_pred = ridge.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e9930-8dc2-4efa-a9eb-11b2cab4f3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R2 = {r2_score(y_test, y_pred):.3f}')\n",
    "print(f'MSE = {mean_squared_error(y_test, y_pred):.3f}')\n",
    "print(f'corr = {pearsonr(y_test, y_pred).statistic:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c608674f-fb36-487b-bd91-9897e9b3928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 2))\n",
    "plt.plot(y_test[:200], c='gray', linestyle='dashed', label='observed');\n",
    "plt.plot(y_pred[:200], c='blue', alpha=.6, label='predicted');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa711a18-f5f6-47df-ae74-8ddb19984825",
   "metadata": {},
   "source": [
    "#### encoding MoG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2347405d-b07e-425c-b017-b77fffe13df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = stim_on_times.shape[0]\n",
    "unsorted = np.vstack([unsorted_trials[i] for i in np.arange(n_trials)]) \n",
    "spike_times = unsorted[:,0]\n",
    "spike_channels = unsorted[:,1]\n",
    "spike_features = unsorted[:,2:]\n",
    "\n",
    "thresholded_neural_data = preprocess.compute_neural_activity(\n",
    "    (spike_times, spike_channels),\n",
    "    stim_on_times,\n",
    "    'thresholded', \n",
    "    n_time_bins=30,\n",
    "    regional=True\n",
    ")\n",
    "print(f'thresholded neural data shape: {thresholded_neural_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400ff52-7891-4d6e-997e-debbfd1029d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = thresholded_neural_data.reshape(-1, thresholded_neural_data.shape[1] * thresholded_neural_data.shape[2])[train]\n",
    "x_test = thresholded_neural_data.reshape(-1, thresholded_neural_data.shape[1] * thresholded_neural_data.shape[2])[test]\n",
    "y_train = data_loader.y[train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ea665d-c06d-41da-9076-6af349a16789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, ExpSineSquared, RationalQuadratic, RBF\n",
    "\n",
    "long_term_trend_kernel = 50.0**2 * RBF(length_scale=50.0)\n",
    "seasonal_kernel = (\n",
    "    2.0**2\n",
    "    * RBF(length_scale=100.0)\n",
    "    * ExpSineSquared(length_scale=1.0, periodicity=1.0, periodicity_bounds=\"fixed\")\n",
    ")\n",
    "# irregularities_kernel = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
    "# noise_kernel = 0.1**2 * RBF(length_scale=0.1) + WhiteKernel(\n",
    "#     noise_level=0.1**2, noise_level_bounds=(1e-5, 1e5)\n",
    "# )\n",
    "kernel = (\n",
    "    long_term_trend_kernel #+ seasonal_kernel #+ irregularities_kernel + noise_kernel\n",
    ")\n",
    "gaussian_process = GaussianProcessRegressor(kernel=kernel, alpha=1e-2)\n",
    "start_time = time.time()\n",
    "gaussian_process.fit(x_train, y_train)\n",
    "print(\n",
    "    f\"Time for GaussianProcessRegressor fitting: {time.time() - start_time:.3f} seconds\"\n",
    ")\n",
    "gp_pred = gaussian_process.predict(x_test)\n",
    "print(f'R2 = {r2_score(data_loader.y[test], gp_pred):.3f}')\n",
    "print(f'MSE = {mean_squared_error(data_loader.y[test], gp_pred):.3f}')\n",
    "print(f'corr = {pearsonr(data_loader.y[test].flatten(), gp_pred.flatten()).statistic:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041a9b9-e572-489f-a6fd-2711934828cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 2))\n",
    "plt.plot(data_loader.y[test].flatten()[:200], c='gray', linestyle='dashed', label='observed');\n",
    "plt.plot(gp_pred.flatten()[:200], c='blue', alpha=.6, label='predicted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120fd514-c7ef-46d5-950d-591f338bd06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=2000)\n",
    "ridge.fit(x_train, y_train)\n",
    "y_hat = ridge.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0f307-295d-4ed0-8e1d-5a06a1d7e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R2 = {r2_score(data_loader.y[test], y_hat):.3f}')\n",
    "print(f'MSE = {mean_squared_error(data_loader.y[test], y_hat):.3f}')\n",
    "print(f'corr = {pearsonr(data_loader.y[test].flatten(), y_hat.flatten()).statistic:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc3bc5f-fc96-4b28-b1b1-cf389639229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 2))\n",
    "plt.plot(data_loader.y[test].flatten()[:200], c='gray', linestyle='dashed', label='observed');\n",
    "plt.plot(y_hat.flatten()[:200], c='blue', alpha=.6, label='predicted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cc72c6-3c63-4051-9878-218b81ea4dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lambdas_hat = np.zeros((data_loader.n_trials, Nc, Nt))\n",
    "for k in range(len(train)):\n",
    "    for t in range(Nt):\n",
    "        log_lambdas_hat[k,:,t] = cavi.bs.detach().numpy() + cavi.betas[:,t].detach().numpy() * y_train[k][t]\n",
    "\n",
    "for k in range(len(test)):\n",
    "    for t in range(Nt):\n",
    "        log_lambdas_hat[k,:,t] = cavi.bs.detach().numpy() + cavi.betas[:,t].detach().numpy() * y_hat[k][t]\n",
    "\n",
    "log_pis_hat = log_lambdas_hat - logsumexp(log_lambdas_hat, 1)[:,None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa90ad-7fdd-4a10-adca-a8131fd30120",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_pis = np.exp(log_pis_hat)\n",
    "enc_means = cavi.means.detach().numpy()\n",
    "enc_covs = cavi.covs.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b59f9a-3ddc-4e55-b1c0-0326f76c705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_all = np.zeros((data_loader.n_trials, Nc, Nt))\n",
    "\n",
    "for k in range(enc_all.shape[0]):\n",
    "    for t in range(Nt):\n",
    "        enc_gmm =  GaussianMixture(n_components=Nc, covariance_type='full')\n",
    "        enc_gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(enc_covs))\n",
    "        enc_gmm.weights_ = enc_pis[k,:,t]\n",
    "        # enc_gmm.weights_ = enc_pis[:,:,t].mean(0)\n",
    "        enc_gmm.means_ = enc_means\n",
    "        enc_gmm.covariances_ = enc_covs\n",
    "        if len(data_loader.trials[k][t]) > 0:\n",
    "            enc_all[k,:,t] = enc_gmm.predict_proba(data_loader.trials[k][t][:,1:]).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373d8438-aca9-4f33-880a-5d6f6afce3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "plt.imshow(enc_all.mean(0), aspect='auto', cmap='cubehelix')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3fcf2-eea4-4f54-b2c1-e24ed84c9ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = enc_all.reshape(-1, enc_all.shape[1] * enc_all.shape[2])[train]\n",
    "x_test = enc_all.reshape(-1, enc_all.shape[1] * enc_all.shape[2])[test]\n",
    "y_train = data_loader.y[train]\n",
    "\n",
    "ridge = Ridge(alpha=2000)\n",
    "ridge.fit(x_train, y_train)\n",
    "y_hat = ridge.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6ba34-af7d-495f-8ecd-75d35325bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R2 = {r2_score(data_loader.y[test], y_hat):.3f}')\n",
    "print(f'MSE = {mean_squared_error(data_loader.y[test], y_hat):.3f}')\n",
    "print(f'corr = {pearsonr(data_loader.y[test].flatten(), y_hat.flatten()).statistic:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c10cb-c84f-4909-8d05-5b0048e632bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, ExpSineSquared, RationalQuadratic, RBF\n",
    "\n",
    "long_term_trend_kernel = 50.0**2 * RBF(length_scale=50.0)\n",
    "seasonal_kernel = (\n",
    "    2.0**2\n",
    "    * RBF(length_scale=100.0)\n",
    "    * ExpSineSquared(length_scale=1.0, periodicity=1.0, periodicity_bounds=\"fixed\")\n",
    ")\n",
    "# irregularities_kernel = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
    "# noise_kernel = 0.1**2 * RBF(length_scale=0.1) + WhiteKernel(\n",
    "#     noise_level=0.1**2, noise_level_bounds=(1e-5, 1e5)\n",
    "# )\n",
    "kernel = (\n",
    "    long_term_trend_kernel #+ seasonal_kernel + noise_kernel #+ irregularities_kernel \n",
    ")\n",
    "gaussian_process = GaussianProcessRegressor(kernel=kernel, alpha=1e-10)\n",
    "start_time = time.time()\n",
    "gaussian_process.fit(x_train, y_train)\n",
    "print(\n",
    "    f\"Time for GaussianProcessRegressor fitting: {time.time() - start_time:.3f} seconds\"\n",
    ")\n",
    "gp_pred = gaussian_process.predict(x_test)\n",
    "print(f'R2 = {r2_score(data_loader.y[test], gp_pred):.3f}')\n",
    "print(f'MSE = {mean_squared_error(data_loader.y[test], gp_pred):.3f}')\n",
    "print(f'corr = {pearsonr(data_loader.y[test].flatten(), gp_pred.flatten()).statistic:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17c8586-ab67-4aee-8437-f683d7105e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 2))\n",
    "plt.plot(data_loader.y[test].flatten()[:200], c='gray', linestyle='dashed', label='observed');\n",
    "plt.plot(gp_pred.flatten()[:200], c='blue', alpha=.6, label='predicted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7880ed-13c7-4955-ab9f-2f1fcb8b0ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_enc_all, half_window_size, n_windows = decoder.sliding_window(\n",
    "    enc_all, \n",
    "    data_loader.n_trials,\n",
    "    window_size = 7\n",
    ")\n",
    "windowed_y = data_loader.y[:,half_window_size:n_windows].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a772c029-d395-41fd-abdb-b889b325f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_by_trial = windowed_enc_all.reshape((data_loader.n_trials, -1))\n",
    "y_by_trial = windowed_y.reshape((data_loader.n_trials, -1))\n",
    "x_train, x_test = x_by_trial[train], x_by_trial[test]\n",
    "y_train, y_test = y_by_trial[train], y_by_trial[test]\n",
    "\n",
    "x_train = x_train.reshape((-1, windowed_enc_all.shape[1]))\n",
    "x_test = x_test.reshape((-1, windowed_enc_all.shape[1]))\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b116659c-83a3-4d72-8c93-dcbd2ef4a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=10000)\n",
    "ridge.fit(x_train, y_train)\n",
    "y_pred = ridge.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1566e79-5851-48c1-87a2-edb0104d3399",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R2 = {r2_score(y_test, y_pred):.3f}')\n",
    "print(f'MSE = {mean_squared_error(y_test, y_pred):.3f}')\n",
    "print(f'corr = {pearsonr(y_test, y_pred).statistic:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f5c3f-3436-4ce4-995b-cc552f1bcc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 2))\n",
    "plt.plot(y_test[:200], c='gray', linestyle='dashed', label='observed');\n",
    "plt.plot(y_pred[:200], c='blue', alpha=.6, label='predicted');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5536e0cf-ba8f-4efe-a565-a0a349f01ac7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### thresholded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c30b68-3860-4951-b448-4fa0eec07084",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_thresh, half_window_size, n_windows = decoder.sliding_window(\n",
    "    thresholded_neural_data, \n",
    "    data_loader.n_trials,\n",
    "    window_size = 7\n",
    ")\n",
    "windowed_y = data_loader.y[:,half_window_size:n_windows].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb122f-281a-4523-b70f-4c5d09939299",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_by_trial = windowed_thresh.reshape((data_loader.n_trials, -1))\n",
    "y_by_trial = windowed_y.reshape((data_loader.n_trials, -1))\n",
    "x_train, x_test = x_by_trial[train], x_by_trial[test]\n",
    "y_train, y_test = y_by_trial[train], y_by_trial[test]\n",
    "\n",
    "x_train = x_train.reshape((-1, windowed_thresh.shape[1]))\n",
    "x_test = x_test.reshape((-1, windowed_thresh.shape[1]))\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faefee3d-75ce-49da-8fed-f58f7754ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=10000)\n",
    "ridge.fit(x_train, y_train)\n",
    "y_pred = ridge.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f878ae-3c67-47d2-bb8c-dd593ddb79c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R2 = {r2_score(y_test, y_pred):.3f}')\n",
    "print(f'MSE = {mean_squared_error(y_test, y_pred):.3f}')\n",
    "print(f'corr = {pearsonr(y_test, y_pred).statistic:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6663ae66-6d1d-4486-b449-c3fc6678aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 2))\n",
    "plt.plot(y_test[:200], c='gray', linestyle='dashed', label='observed');\n",
    "plt.plot(y_pred[:200], c='blue', alpha=.6, label='predicted');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c88bb7-e74c-4279-b74f-aa3e9bd43aff",
   "metadata": {},
   "source": [
    "#### KS & good IBL units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a06582-0f91-488b-a3ab-9ef1f78412af",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = stim_on_times.shape[0]\n",
    "sorted = np.vstack([sorted_trials[i] for i in np.arange(n_trials)]) \n",
    "spike_times = sorted[:,0]\n",
    "spike_clusters = sorted[:,1]\n",
    "\n",
    "sorted_neural_data = preprocess.compute_neural_activity(\n",
    "    (spike_times, spike_clusters),\n",
    "    stim_on_times,\n",
    "    'sorted', \n",
    "    n_time_bins=30,\n",
    "    regional=True\n",
    ")\n",
    "print(f'sorted neural data shape: {sorted_neural_data.shape}')\n",
    "\n",
    "good_sorted = np.vstack([good_sorted_trials[i] for i in np.arange(n_trials)]) \n",
    "spike_times = good_sorted[:,0]\n",
    "spike_clusters = good_sorted[:,1]\n",
    "\n",
    "good_sorted_neural_data = preprocess.compute_neural_activity(\n",
    "    (spike_times, spike_clusters),\n",
    "    stim_on_times,\n",
    "    'sorted', \n",
    "    n_time_bins=30,\n",
    "    regional=True\n",
    ")\n",
    "print(f'good sorted neural data shape: {good_sorted_neural_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e018223-bf48-46c7-b868-d73d6faf0469",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sorted_neural_data.reshape(-1, sorted_neural_data.shape[1] * sorted_neural_data.shape[2])[train]\n",
    "x_test = sorted_neural_data.reshape(-1, sorted_neural_data.shape[1] * sorted_neural_data.shape[2])[test]\n",
    "y_train = data_loader.y[train]\n",
    "\n",
    "ridge = Ridge(alpha=2000)\n",
    "ridge.fit(x_train, y_train)\n",
    "y_hat = ridge.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5053a0c9-725e-4722-addd-26c0f1fd63f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R2 = {r2_score(data_loader.y[test], y_hat):.3f}')\n",
    "print(f'MSE = {mean_squared_error(data_loader.y[test], y_hat):.3f}')\n",
    "print(f'corr = {pearsonr(data_loader.y[test].flatten(), y_hat.flatten()).statistic:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b049209-7c71-4c72-85fe-54b516784485",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 2))\n",
    "plt.plot(data_loader.y[test].flatten()[:200], c='gray', linestyle='dashed', label='observed');\n",
    "plt.plot(y_hat.flatten()[:200], c='blue', alpha=.6, label='predicted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9362aa-e387-45a6-8774-90f518abcfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, ExpSineSquared, RationalQuadratic, RBF\n",
    "\n",
    "long_term_trend_kernel = 50.0**2 * RBF(length_scale=50.0)\n",
    "seasonal_kernel = (\n",
    "    2.0**2\n",
    "    * RBF(length_scale=100.0)\n",
    "    * ExpSineSquared(length_scale=1.0, periodicity=1.0, periodicity_bounds=\"fixed\")\n",
    ")\n",
    "# irregularities_kernel = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
    "noise_kernel = 0.1**2 * RBF(length_scale=0.1) + WhiteKernel(\n",
    "    noise_level=0.1**2, noise_level_bounds=(1e-5, 1e5)\n",
    ")\n",
    "kernel = (\n",
    "    long_term_trend_kernel #+ seasonal_kernel + noise_kernel #+ irregularities_kernel\n",
    ")\n",
    "gaussian_process = GaussianProcessRegressor(kernel=kernel, alpha=1e-2)\n",
    "start_time = time.time()\n",
    "gaussian_process.fit(x_train, y_train)\n",
    "print(\n",
    "    f\"Time for GaussianProcessRegressor fitting: {time.time() - start_time:.3f} seconds\"\n",
    ")\n",
    "gp_pred = gaussian_process.predict(x_test)\n",
    "print(f'R2 = {r2_score(data_loader.y[test], gp_pred):.3f}')\n",
    "print(f'MSE = {mean_squared_error(data_loader.y[test], gp_pred):.3f}')\n",
    "print(f'corr = {pearsonr(data_loader.y[test].flatten(), gp_pred.flatten()).statistic:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebc9003-3052-431d-a66a-a58f2144be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_sorted, half_window_size, n_windows = decoder.sliding_window(\n",
    "    sorted_neural_data, \n",
    "    data_loader.n_trials,\n",
    "    window_size = 7\n",
    ")\n",
    "\n",
    "windowed_good_units, half_window_size, n_windows = decoder.sliding_window(\n",
    "    good_sorted_neural_data, \n",
    "    data_loader.n_trials,\n",
    "    window_size = 7\n",
    ")\n",
    "windowed_y = data_loader.y[:,half_window_size:n_windows].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0379fe56-ed49-45ee-b5c1-09b51611b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_by_trial = windowed_sorted.reshape((data_loader.n_trials, -1))\n",
    "y_by_trial = windowed_y.reshape((data_loader.n_trials, -1))\n",
    "x_train, x_test = x_by_trial[train], x_by_trial[test]\n",
    "y_train, y_test = y_by_trial[train], y_by_trial[test]\n",
    "\n",
    "x_train = x_train.reshape((-1, windowed_sorted.shape[1]))\n",
    "x_test = x_test.reshape((-1, windowed_sorted.shape[1]))\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "ridge = Ridge(alpha=10000)\n",
    "ridge.fit(x_train, y_train)\n",
    "y_pred = ridge.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d837d7e-4b9f-46a0-b0ed-2c59e967019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R2 = {r2_score(y_test, y_pred):.3f}')\n",
    "print(f'MSE = {mean_squared_error(y_test, y_pred):.3f}')\n",
    "print(f'corr = {pearsonr(y_test, y_pred).statistic:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a610c-c17a-48c5-9670-747cebc94453",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 2))\n",
    "plt.plot(y_test[:200], c='gray', linestyle='dashed', label='observed');\n",
    "plt.plot(y_pred[:200], c='blue', alpha=.6, label='predicted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b9f2d4-e447-443b-aab9-8d30250e4eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = good_sorted_neural_data.reshape(-1, good_sorted_neural_data.shape[1] * good_sorted_neural_data.shape[2])[train]\n",
    "x_test = good_sorted_neural_data.reshape(-1, good_sorted_neural_data.shape[1] * good_sorted_neural_data.shape[2])[test]\n",
    "y_train = data_loader.y[train]\n",
    "\n",
    "ridge = Ridge(alpha=2000)\n",
    "ridge.fit(x_train, y_train)\n",
    "y_hat = ridge.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b24825a-8c8f-4597-af8d-aa650046581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R2 = {r2_score(data_loader.y[test], y_hat):.3f}')\n",
    "print(f'MSE = {mean_squared_error(data_loader.y[test], y_hat):.3f}')\n",
    "print(f'corr = {pearsonr(data_loader.y[test].flatten(), y_hat.flatten()).statistic:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a554ec-8f63-433b-8661-8530505017fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, ExpSineSquared, RationalQuadratic, RBF\n",
    "\n",
    "long_term_trend_kernel = 50.0**2 * RBF(length_scale=50.0)\n",
    "seasonal_kernel = (\n",
    "    2.0**2\n",
    "    * RBF(length_scale=100.0)\n",
    "    * ExpSineSquared(length_scale=1.0, periodicity=1.0, periodicity_bounds=\"fixed\")\n",
    ")\n",
    "# irregularities_kernel = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
    "noise_kernel = 0.1**2 * RBF(length_scale=0.1) + WhiteKernel(\n",
    "    noise_level=0.1**2, noise_level_bounds=(1e-5, 1e5)\n",
    ")\n",
    "kernel = (\n",
    "    long_term_trend_kernel #+ seasonal_kernel + noise_kernel #+ irregularities_kernel\n",
    ")\n",
    "gaussian_process = GaussianProcessRegressor(kernel=kernel, alpha=1e-2)\n",
    "start_time = time.time()\n",
    "gaussian_process.fit(x_train, y_train)\n",
    "print(\n",
    "    f\"Time for GaussianProcessRegressor fitting: {time.time() - start_time:.3f} seconds\"\n",
    ")\n",
    "gp_pred = gaussian_process.predict(x_test)\n",
    "print(f'R2 = {r2_score(data_loader.y[test], gp_pred):.3f}')\n",
    "print(f'MSE = {mean_squared_error(data_loader.y[test], gp_pred):.3f}')\n",
    "print(f'corr = {pearsonr(data_loader.y[test].flatten(), gp_pred.flatten()).statistic:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09438e8-ed72-424f-ae30-95c0a4c26b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 2))\n",
    "plt.plot(data_loader.y[test].flatten()[:200], c='gray', linestyle='dashed', label='observed');\n",
    "plt.plot(gp_pred.flatten()[:200], c='blue', alpha=.6, label='predicted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ae8bd-507c-4680-af40-bc1471a6d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_by_trial = windowed_good_units.reshape((data_loader.n_trials, -1))\n",
    "y_by_trial = windowed_y.reshape((data_loader.n_trials, -1))\n",
    "x_train, x_test = x_by_trial[train], x_by_trial[test]\n",
    "y_train, y_test = y_by_trial[train], y_by_trial[test]\n",
    "\n",
    "x_train = x_train.reshape((-1, windowed_good_units.shape[1]))\n",
    "x_test = x_test.reshape((-1, windowed_good_units.shape[1]))\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "ridge = Ridge(alpha=2000)\n",
    "ridge.fit(x_train, y_train)\n",
    "y_pred = ridge.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f2025c-2f3f-4572-aa3b-837e8e4350b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R2 = {r2_score(y_test, y_pred):.3f}')\n",
    "print(f'MSE = {mean_squared_error(y_test, y_pred):.3f}')\n",
    "print(f'corr = {pearsonr(y_test, y_pred).statistic:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30398d55-012e-4d7d-8404-b77f03f473f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 2))\n",
    "plt.plot(y_test[:200], c='gray', linestyle='dashed', label='observed');\n",
    "plt.plot(y_pred[:200], c='blue', alpha=.6, label='predicted');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793f7a4d-cfbf-4f6a-9c96-54bd7e7682ee",
   "metadata": {},
   "source": [
    "#### plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bdc0b6-d5d9-47e1-b644-07f3b573a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "po_good_units_r2 = [0.008, 0.058, 0.066, 0.046, 0.068]\n",
    "po_all_units_r2 = [0.159, 0.164, 0.119, 0.135, 0.140]\n",
    "po_thresh_r2 = [0.138, 0.123, 0.099, 0.130, 0.145]\n",
    "po_mog_r2 = [0.102, 0.048, 0.028, 0.064, 0.049]\n",
    "po_enc_mog_r2 = [0.154, 0.170, 0.104, 0.135, 0.147]\n",
    "\n",
    "po_good_units_mse = [1.685, 1.219, 1.447, 1.423, 1.535]\n",
    "po_all_units_mse = [1.175, 0.889, 1.284, 1.193, 1.253]\n",
    "po_thresh_mse = [1.248, 1.011, 1.323, 1.209, 1.216]\n",
    "po_mog_mse = [1.329, 1.281, 1.547, 1.378, 1.597]\n",
    "po_enc_mog_mse = [1.180, 0.892, 1.308, 1.191, 1.239]\n",
    "\n",
    "po_good_units_corr = []\n",
    "po_all_units_corr = []\n",
    "po_thresh_corr = []\n",
    "po_mog_corr = []\n",
    "po_enc_mog_corr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0c547f31-eeae-4e12-a457-dcbd9673c59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PO good units R2: 0.049 MSE: 1.462\n",
      "PO all units R2: 0.143 MSE: 1.159\n",
      "PO thresholded R2: 0.127 MSE: 1.201\n",
      "PO MoG R2: 0.058 MSE: 1.426\n",
      "PO encoded-MoG R2: 0.142 MSE: 1.162\n"
     ]
    }
   ],
   "source": [
    "print(f'PO good units R2: {np.mean(po_good_units_r2):.3f} MSE: {np.mean(po_good_units_mse):.3f}')\n",
    "print(f'PO all units R2: {np.mean(po_all_units_r2):.3f} MSE: {np.mean(po_all_units_mse):.3f}')\n",
    "print(f'PO thresholded R2: {np.mean(po_thresh_r2):.3f} MSE: {np.mean(po_thresh_mse):.3f}')\n",
    "print(f'PO MoG R2: {np.mean(po_mog_r2):.3f} MSE: {np.mean(po_mog_mse):.3f}')\n",
    "print(f'PO encoded-MoG R2: {np.mean(po_enc_mog_r2):.3f} MSE: {np.mean(po_enc_mog_mse):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b54aae2e-c71c-48c7-95ec-a4ea14abd218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LP good units R2: -0.067 MSE: 1.662 corr: 0.401\n",
      "LP all units R2: 0.107 MSE: 1.159 corr: 0.646\n",
      "LP thresholded R2: 0.062 MSE: 1.274 corr: 0.588\n",
      "LP MoG R2: 0.006 MSE: 1.480 corr: 0.508\n",
      "LP encoded-MoG R2: 0.078 MSE: 1.236 corr: 0.612\n"
     ]
    }
   ],
   "source": [
    "lp_good_units_r2 = [0.004, -0.079, -0.137, -0.086, -0.039]\n",
    "lp_all_units_r2 = [0.157, 0.119, 0.036, 0.086, 0.139]\n",
    "lp_thresh_r2 = [0.126, 0.069, -0.010, 0.066, 0.060]\n",
    "lp_mog_r2 = [0.053, 0.057, -0.004, -0.062, -0.016]\n",
    "lp_enc_mog_r2 = [0.151, 0.082, 0.033, 0.063, 0.060]\n",
    "\n",
    "lp_good_units_mse = [1.646, 1.638, 1.719, 1.705, 1.603]\n",
    "lp_all_units_mse = [1.122, 1.119, 1.284, 1.141, 1.127]\n",
    "lp_thresh_mse = [1.235, 1.237, 1.402, 1.236, 1.260]\n",
    "lp_mog_mse = [1.513, 1.310, 1.463, 1.559, 1.555]\n",
    "lp_enc_mog_mse = [1.136, 1.179, 1.291, 1.251, 1.321]\n",
    "\n",
    "lp_good_units_corr = [0.407, 0.316, 0.467, 0.427, 0.389]\n",
    "lp_all_units_corr = [0.667, 0.624, 0.617, 0.679, 0.645]\n",
    "lp_thresh_corr = [0.614, 0.528, 0.568, 0.646, 0.584]\n",
    "lp_mog_corr = [0.484, 0.564, 0.557, 0.505, 0.432]\n",
    "lp_enc_mog_corr = [0.662, 0.593, 0.613, 0.636, 0.554]\n",
    "\n",
    "print(f'LP good units R2: {np.mean(lp_good_units_r2):.3f} MSE: {np.mean(lp_good_units_mse):.3f} corr: {np.mean(lp_good_units_corr):.3f}')\n",
    "print(f'LP all units R2: {np.mean(lp_all_units_r2):.3f} MSE: {np.mean(lp_all_units_mse):.3f} corr: {np.mean(lp_all_units_corr):.3f}')\n",
    "print(f'LP thresholded R2: {np.mean(lp_thresh_r2):.3f} MSE: {np.mean(lp_thresh_mse):.3f} corr: {np.mean(lp_thresh_corr):.3f}')\n",
    "print(f'LP MoG R2: {np.mean(lp_mog_r2):.3f} MSE: {np.mean(lp_mog_mse):.3f} corr: {np.mean(lp_mog_corr):.3f}')\n",
    "print(f'LP encoded-MoG R2: {np.mean(lp_enc_mog_r2):.3f} MSE: {np.mean(lp_enc_mog_mse):.3f} corr: {np.mean(lp_enc_mog_corr):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7fe71d02-e297-4518-b476-e34a52bd3d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DG good units R2: -0.013 MSE: 1.703 corr: 0.385\n",
      "DG all units R2: 0.008 MSE: 1.634 corr: 0.428\n",
      "DG thresholded R2: -0.008 MSE: 1.660 corr: 0.404\n",
      "DG MoG R2: -0.032 MSE: 1.746 corr: 0.344\n",
      "DG encoded-MoG R2: 0.008 MSE: 1.616 corr: 0.431\n"
     ]
    }
   ],
   "source": [
    "dg_good_units_r2 = [0.003, 0.008, -0.016, -0.013, -0.045]\n",
    "dg_all_units_r2 = [0.019, 0.031, -0.010, 0.009, -0.011]\n",
    "dg_thresh_r2 = [-0.001, 0.014, -0.033, 0.007, -0.025]\n",
    "dg_mog_r2 = [-0.007, -0.018, -0.060, -0.023, -0.052]\n",
    "dg_enc_mog_r2 = [0.019, 0.023, -0.021, 0.012, 0.007]\n",
    "\n",
    "dg_good_units_mse = [1.770, 1.654, 1.714, 1.601, 1.778]\n",
    "dg_all_units_mse = [1.710, 1.590, 1.692, 1.537, 1.643]\n",
    "dg_thresh_mse = [1.755, 1.630, 1.727, 1.510, 1.678]\n",
    "dg_mog_mse = [1.803, 1.721, 1.813, 1.609, 1.783]\n",
    "dg_enc_mog_mse = [1.703, 1.611, 1.690, 1.509, 1.565]\n",
    "\n",
    "dg_good_units_corr = [0.324, 0.310, 0.437, 0.552, 0.303]\n",
    "dg_all_units_corr = [0.369, 0.361, 0.446, 0.579, 0.384]\n",
    "dg_thresh_corr = [0.342, 0.334, 0.415, 0.575, 0.354]\n",
    "dg_mog_corr = [0.307, 0.265, 0.362, 0.519, 0.265]\n",
    "dg_enc_mog_corr = [0.373, 0.342, 0.434, 0.579, 0.425]\n",
    "\n",
    "print(f'DG good units R2: {np.mean(dg_good_units_r2):.3f} MSE: {np.mean(dg_good_units_mse):.3f} corr: {np.mean(dg_good_units_corr):.3f}')\n",
    "print(f'DG all units R2: {np.mean(dg_all_units_r2):.3f} MSE: {np.mean(dg_all_units_mse):.3f} corr: {np.mean(dg_all_units_corr):.3f}')\n",
    "print(f'DG thresholded R2: {np.mean(dg_thresh_r2):.3f} MSE: {np.mean(dg_thresh_mse):.3f} corr: {np.mean(dg_thresh_corr):.3f}')\n",
    "print(f'DG MoG R2: {np.mean(dg_mog_r2):.3f} MSE: {np.mean(dg_mog_mse):.3f} corr: {np.mean(dg_mog_corr):.3f}')\n",
    "print(f'DG encoded-MoG R2: {np.mean(dg_enc_mog_r2):.3f} MSE: {np.mean(dg_enc_mog_mse):.3f} corr: {np.mean(dg_enc_mog_corr):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8377a60e-cd17-4872-a1af-23900c7035e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA1 good units R2: -0.035 MSE: 1.715 corr: 0.371\n",
      "CA1 all units R2: -0.010 MSE: 1.695 corr: 0.384\n",
      "CA1 thresholded R2: -0.018 MSE: 1.718 corr: 0.369\n",
      "CA1 MoG R2: -0.038 MSE: 1.783 corr: 0.323\n",
      "CA1 encoded-MoG R2: -0.011 MSE: 1.669 corr: 0.398\n"
     ]
    }
   ],
   "source": [
    "ca1_good_units_r2 = [0.010, -0.045, -0.044, -0.065, -0.032]\n",
    "ca1_all_units_r2 = [0.041, -0.047, -0.010, -0.054, 0.018]\n",
    "ca1_thresh_r2 = [0.029, -0.034, -0.016, -0.087, 0.018]\n",
    "ca1_mog_r2 = [0.008, -0.042, -0.048, -0.078, -0.031]\n",
    "ca1_enc_mog_r2 = [0.030, -0.042, -0.006, -0.053, 0.014]\n",
    "\n",
    "ca1_good_units_mse = [1.700, 1.736, 1.716, 1.715, 1.706]\n",
    "ca1_all_units_mse = [1.642, 1.767, 1.656, 1.718, 1.693]\n",
    "ca1_thresh_mse = [1.664, 1.733, 1.676, 1.814, 1.701]\n",
    "ca1_mog_mse = [1.755, 1.731, 1.811, 1.781, 1.835]\n",
    "ca1_enc_mog_mse = [1.659, 1.713, 1.623, 1.704, 1.648]\n",
    "\n",
    "ca1_good_units_corr = [0.376, 0.258, 0.423, 0.453, 0.344]\n",
    "ca1_all_units_corr = [0.417, 0.232, 0.457, 0.454, 0.360]\n",
    "ca1_thresh_corr = [0.404, 0.248, 0.445, 0.393, 0.356]\n",
    "ca1_mog_corr = [0.334, 0.237, 0.374, 0.423, 0.248]\n",
    "ca1_enc_mog_corr = [0.404, 0.270,0.467, 0.465, 0.385]\n",
    "\n",
    "print(f'CA1 good units R2: {np.mean(ca1_good_units_r2):.3f} MSE: {np.mean(ca1_good_units_mse):.3f} corr: {np.mean(ca1_good_units_corr):.3f}')\n",
    "print(f'CA1 all units R2: {np.mean(ca1_all_units_r2):.3f} MSE: {np.mean(ca1_all_units_mse):.3f} corr: {np.mean(ca1_all_units_corr):.3f}')\n",
    "print(f'CA1 thresholded R2: {np.mean(ca1_thresh_r2):.3f} MSE: {np.mean(ca1_thresh_mse):.3f} corr: {np.mean(ca1_thresh_corr):.3f}')\n",
    "print(f'CA1 MoG R2: {np.mean(ca1_mog_r2):.3f} MSE: {np.mean(ca1_mog_mse):.3f} corr: {np.mean(ca1_mog_corr):.3f}')\n",
    "print(f'CA1 encoded-MoG R2: {np.mean(ca1_enc_mog_r2):.3f} MSE: {np.mean(ca1_enc_mog_mse):.3f} corr: {np.mean(ca1_enc_mog_corr):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b2dcab-4fd2-4516-84d3-4af9a9ca0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_good_units_mse = []\n",
    "vis_all_units_mse = []\n",
    "vis_thresh_mse = []\n",
    "vis_mog_mse = []\n",
    "vis_enc_mog_mse = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c044e3-f285-4807-bbf1-2238a48ec67f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
