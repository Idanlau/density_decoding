{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a3a3c6-c029-486d-9a9b-d3bc1cf7c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import optimize\n",
    "from scipy.special import logsumexp\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "\n",
    "from clusterless import preprocess\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c83eddd-64ec-44da-91f6-be3c8880cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 666\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "804affc9-ffa2-4b40-98c1-e33a5fc928db",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 15\n",
    "MEDIUM_SIZE = 20\n",
    "BIGGER_SIZE = 25\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)         \n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     \n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE) \n",
    "plt.rc('axes', linewidth = 1.5)\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)   \n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)   \n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)   \n",
    "plt.rc('figure', titlesize=MEDIUM_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779810ad-6236-439a-9c42-920935cc363c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cd1c562-85c3-4d92-98da-b1ae930d9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = 'dab512bd-a02d-4c1f-8dbc-9155a163efc0'\n",
    "rootpath = '/mnt/3TB/yizi/Downloads/ONE/openalyx.internationalbrainlab.org'\n",
    "trial_data_path = rootpath + '/danlab/Subjects/DY_016/2020-09-12/001/alf'\n",
    "neural_data_path = '/mnt/3TB/yizi/danlab/Subjects/DY_016'\n",
    "behavior_data_path = rootpath + '/paper_repro_ephys_data/figure9_10/original_data'\n",
    "save_path = '../saved_results/danlab/Subjects/DY_016/cavi_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2d94ef8-0d09-4bb5-9342-3179e7c0ba04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pid: dab512bd-a02d-4c1f-8dbc-9155a163efc0\n",
      "eid: d23a44ef-1402-4ed7-97f5-47e9a7a504d9\n",
      "1st trial stim on time: 17.56, last trial stim on time 2310.24\n"
     ]
    }
   ],
   "source": [
    "unsorted_trials, stim_on_times, np1_channel_map = preprocess.load_neural_data(\n",
    "    pid=pid, \n",
    "    trial_data_path=trial_data_path,\n",
    "    neural_data_path=neural_data_path,\n",
    "    behavior_data_path=behavior_data_path,\n",
    "    keep_active_trials=True, \n",
    "    roi='all',\n",
    "    kilosort=False,\n",
    "    triage=False\n",
    ")\n",
    "\n",
    "behave_dict = preprocess.load_behaviors_data(behavior_data_path, pid)\n",
    "motion_energy, wheel_velocity, wheel_speed, paw_speed, nose_speed, pupil_diameter = preprocess.preprocess_dynamic_behaviors(behave_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da5cbb8-4cf4-45c4-a61e-97c97bf63423",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, data, y, stim_on_times, np1_channel_map, n_t_bins=30):\n",
    "        self.data = data\n",
    "        self.y = y\n",
    "        self.stim_on_times = stim_on_times\n",
    "        self.np1_channel_map = np1_channel_map\n",
    "        self.n_t_bins = n_t_bins\n",
    "        self.n_trials = stim_on_times.shape[0]\n",
    "        self.n_channels = np1_channel_map.shape[0]\n",
    "        self.t_binning = np.arange(0, 1.5, step = (1.5 - 0) / n_t_bins)\n",
    "        self.rand_trial_ids = np.arange(self.n_trials)\n",
    "        \n",
    "        # allocate unsorted data into trials\n",
    "        self.trial_ids = []\n",
    "        self.t_ids = []\n",
    "        self.trials = []\n",
    "        self.t_bins = []\n",
    "        for k in range(self.n_trials):\n",
    "            mask = np.logical_and(data[:,0] >= stim_on_times[k] - 0.5,\n",
    "                                  data[:,0] <= stim_on_times[k] + 1)\n",
    "            trial = data[mask,:]\n",
    "            trial[:,0] = trial[:,0] - trial[:,0].min()\n",
    "            t_bins = np.digitize(trial[:,0], self.t_binning, right = False) - 1\n",
    "            t_bin_lst = []\n",
    "            for t in range(self.n_t_bins):\n",
    "                t_bin = trial[t_bins == t,1:]\n",
    "                self.trial_ids.append(np.ones_like(t_bin[:,0]) * k)\n",
    "                self.t_ids.append(np.ones_like(t_bin[:,0]) * t)\n",
    "                t_bin_lst.append(t_bin)\n",
    "            self.trials.append(t_bin_lst)\n",
    "    \n",
    "    \n",
    "    def split_train_test(self, split=.8):\n",
    "        \n",
    "        self.train_ids = self.rand_trial_ids[:int(split * self.n_trials)]\n",
    "        self.test_ids = self.rand_trial_ids[int(split * self.n_trials):]\n",
    "        self.y_train = self.y[self.train_ids]\n",
    "        self.y_test = self.y[self.test_ids]\n",
    "        \n",
    "        trial_ids = np.concatenate(self.trial_ids)\n",
    "        t_ids = np.concatenate(self.t_ids)\n",
    "        trials = np.concatenate(np.concatenate(self.trials))\n",
    "\n",
    "        train_mask = np.sum([trial_ids == idx for idx in self.train_ids], axis=0).astype(bool)\n",
    "        test_mask = np.sum([trial_ids == idx for idx in self.test_ids], axis=0).astype(bool)\n",
    "        train_trial_ids, test_trial_ids = trial_ids[train_mask], trial_ids[test_mask]\n",
    "        train_t_ids, test_t_ids = t_ids[train_mask], t_ids[test_mask]\n",
    "        train_trials, test_trials = trials[train_mask], trials[test_mask]\n",
    "        \n",
    "        return train_trials, train_trial_ids, train_t_ids, \\\n",
    "               test_trials, test_trial_ids, test_t_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a170e94-7388-43ae-8367-f680c532cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(data = np.concatenate(unsorted_trials)[:,[0,2,3,4]], \n",
    "                         y = wheel_velocity, \n",
    "                         stim_on_times = stim_on_times, \n",
    "                         np1_channel_map = np1_channel_map, \n",
    "                         n_t_bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b5143bd-bd3d-4f59-b420-f83402c51a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "train_trials, train_trial_ids, train_t_ids, _, _, _ = data_loader.split_train_test(split=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7484aa0b-7a3d-482a-ae88-0ba05c2ca98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianMixture(init_params=&#x27;k-means++&#x27;, n_components=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianMixture</label><div class=\"sk-toggleable__content\"><pre>GaussianMixture(init_params=&#x27;k-means++&#x27;, n_components=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianMixture(init_params='k-means++', n_components=10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = 10\n",
    "gmm = GaussianMixture(n_components=C, \n",
    "                      covariance_type='full', \n",
    "                      init_params='k-means++',\n",
    "                      verbose=0)\n",
    "gmm.fit(train_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2740077e-fac5-4beb-b97c-22be863121a8",
   "metadata": {},
   "source": [
    "### ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84e309c8-3c5e-4982-9fc7-366cd0cdedfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_log(x, minval=1e-10):\n",
    "    return torch.log(x + minval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9420fec-54d7-40e3-a9db-a83cdc1ab12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_elbo(s, r, mu, cov, log_norm_lam, k_idxs, t_idxs, K, T, C):\n",
    "\n",
    "    # expected log-likelihood ( E_q(z)[log p(s,z|y)] )\n",
    "    # sum_{ktij} r_{ijt}^k * log(dN(s_i^kt; mu_j, cov_j))\n",
    "    \n",
    "    # sum_{ktij} r_{ijt}^k * ( log(lambda_jt^(k)) - log(sum_j' lambda_j't^(k)) ) \n",
    "    elbo_1 = 0; log_dens = []\n",
    "    for j in range(C):\n",
    "        log_dens.append(\n",
    "            torch.tensor(\n",
    "                multivariate_normal.logpdf(s, mu[j], cov[j])\n",
    "            )\n",
    "        )\n",
    "        elbo_1 += torch.einsum('i,i->', r[:,j], log_dens[-1])\n",
    "      \n",
    "    elbo_2 = torch.tensor([ \n",
    "                torch.sum( r[np.intersect1d(k_idxs[k], t_idxs[t])] * log_norm_lam[k,:,t] ) \\\n",
    "                for t in range(T) for k in range(K)\n",
    "             ]).sum()\n",
    "        \n",
    "    # entropy of q(z) ( E_q(z)[log q(z)] )\n",
    "    # sum_{ktij} log(r_{ijt}^k) * r_{ijt}^k\n",
    "    elbo_3 = - torch.einsum('ij,ij->', safe_log(r), r)\n",
    "    \n",
    "    elbo = elbo_1 + elbo_2 + elbo_3 \n",
    "    \n",
    "    print(f'elbo 1: {elbo_1}')\n",
    "    print(f'elbo 2: {elbo_2}')\n",
    "    print(f'elbo 3: {elbo_3}')\n",
    "    \n",
    "    return elbo, torch.vstack(log_dens).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01a0a88c-4332-4e02-bfde-b8ac6a8c2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = len(data_loader.train_ids)\n",
    "C = len(gmm.means_)\n",
    "T = data_loader.n_t_bins\n",
    "\n",
    "s = torch.tensor(train_trials)\n",
    "r = torch.ones((train_trials.shape[0], C)) / C\n",
    "train_k_ids = [torch.argwhere(torch.tensor(train_trial_ids) == k).reshape(-1) for k in range(K)]\n",
    "train_t_ids = [torch.argwhere(torch.tensor(train_t_ids) == t).reshape(-1) for t in range(T)]\n",
    "\n",
    "y_k = torch.tensor(data_loader.y_train)\n",
    "y_train = torch.zeros((train_trials.shape[0], T))\n",
    "for k in range(K):\n",
    "    y_train[torch.tensor(train_trial_ids) == k, :] = torch.tensor(data_loader.y_train[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ea3b25d-4b85-4a19-850c-e2f17ac7b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.normal(0, 1, size=(C,))\n",
    "beta = torch.normal(0, 1, size=(C,T,T))\n",
    "\n",
    "init_log_lam = torch.zeros((K, C, T))\n",
    "for k in range(K):\n",
    "    for t in range(T):\n",
    "        init_log_lam[k,:,t] = b + beta[:,t] @ data_loader.y_train[k]\n",
    "        \n",
    "log_norm_lam = init_log_lam - torch.logsumexp(init_log_lam, 1)[:,None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33541c69-0ab1-4a60-b988-e350d84cdd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elbo 1: -115223206.2406357\n",
      "elbo 2: -7911433.144862292\n",
      "elbo 3: 1368129.286682262\n",
      "tensor(-1.2177e+08)\n",
      "CPU times: user 5.9 s, sys: 188 ms, total: 6.09 s\n",
      "Wall time: 2.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "elbo, log_dens = compute_elbo(s=s, \n",
    "                    r=r, \n",
    "                    mu=gmm.means_, \n",
    "                    cov=gmm.covariances_, \n",
    "                    log_norm_lam=log_norm_lam, \n",
    "                    k_idxs=train_k_ids,\n",
    "                    t_idxs=train_t_ids,\n",
    "                    K=K,\n",
    "                    T=T,\n",
    "                    C=C)\n",
    "print(elbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbbc3cb-e6ad-4df8-b266-98f78ecb48d9",
   "metadata": {},
   "source": [
    "### Update $b_j$ and $\\beta_{jt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35239d03-ee3a-4c68-bc4f-09a6dddedd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.normal(0, 1, size=(C,)).requires_grad_(True)\n",
    "beta = torch.normal(0, 1, size=(C,T,T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc9e541-32de-4d44-b62e-8411198366f9",
   "metadata": {},
   "source": [
    "#### 1) find $b_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "310432bc-1de1-4322-a3d8-8daba74a7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_safe_grad_bj(b_j, j, b_no_j, beta, y_k, r, k_idxs, t_idxs):\n",
    "    val = 0\n",
    "    no_j = torch.cat([torch.arange(j), torch.arange(j+1,C)])\n",
    "    for k in range(len(k_idxs)):\n",
    "        for t in range(len(t_idxs)):\n",
    "            k_t_idx = np.intersect1d(k_idxs[k], t_idxs[t])\n",
    "            num = b_j + beta[j,t,:] @ y_k[k]\n",
    "            denom = torch.logsumexp(torch.hstack([b_j, b_no_j]) + \\\n",
    "                                    torch.vstack([beta[j,t], beta[no_j,t]]) @ y_k[k], 0)\n",
    "            val += torch.sum( r[k_t_idx, j] * ( 1 - torch.exp(num - denom) ) )\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f042207e-b7e4-4c8a-908c-c77e35620b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_grad_bj(b_j, j, b_no_j, beta, y_k, r, k_idxs, t_idxs):\n",
    "    val = 0\n",
    "    no_j = np.concatenate([np.arange(j), np.arange(j+1,C)])\n",
    "    for k in range(len(k_idxs)):\n",
    "        for t in range(len(t_idxs)):\n",
    "            k_t_idx = np.intersect1d(k_idxs[k], t_idxs[t])\n",
    "            num = b_j + beta[j,t,:] @ y_k[k]\n",
    "            denom = logsumexp(np.hstack([b_j, b_no_j]) + \\\n",
    "                              np.vstack([beta[j,t], beta[no_j,t]]) @ y_k[k], 0)\n",
    "            val += np.sum( r[k_t_idx, j] * ( 1 - np.exp(num - denom) ) )\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "10d0ce9d-dd13-47c2-81d8-de4c9f4c4047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56099.732700224486"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = 3\n",
    "no_j = torch.cat([torch.arange(j), torch.arange(j+1,C)])\n",
    "b_j = b[j]\n",
    "b_no_j = b[no_j]\n",
    "safe_grad_bj(b_j.numpy(), j, b_no_j.numpy(), beta.numpy(), y_k.numpy(), \n",
    "                r.numpy(), train_k_ids, train_t_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab776e-d757-4895-9981-a300d0da1377",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inputs = (j, b_no_j.numpy(), beta.numpy(), y_k.numpy(), \n",
    "          r.numpy(), train_k_ids, train_t_ids)\n",
    "sol = optimize.root(safe_grad_bj, b_j, args = inputs, method='hybr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ca460-85a3-4019-983c-280ce7542ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0e847b9-c46e-4a76-912b-f3903fcac6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_bj(j, b, beta, y_k, r, k_idxs, t_idxs, eps=1e-6, max_iter=1000):\n",
    "    b_j_new = b[j].clone()\n",
    "    no_j = torch.cat([torch.arange(j), torch.arange(j+1, len(b))])\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        f = torch_safe_grad_bj(b_j_new, j, b[no_j], beta, y_k, r, k_idxs, t_idxs)\n",
    "        if abs(f) < eps:\n",
    "            print(f'grad at b_{j} = {b_j_new:.2f} is {f.item():.2f}')\n",
    "            return b_j_new.item()\n",
    "        \n",
    "        Df = grad(outputs=f, inputs=b_j_new)[0]\n",
    "        if Df == 0:\n",
    "            print('no solution found (0 derivative).')\n",
    "            return b[j].item()\n",
    "        b_j_new = b_j_new - f / Df\n",
    "        \n",
    "        if (i % 10) == 0:\n",
    "            print(f'grad at b_{j} = {b_j_new:.2f} is {f.item():.2f}')\n",
    "    print('exceeded max iter. no solution found.')\n",
    "    \n",
    "    return b[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b631f91e-ef4c-46bc-9ceb-cf55619d1aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad at b_0 = 52.50 is 55009.64\n",
      "grad at b_0 = 96.43 is 0.00\n",
      "grad at b_0 = 97.43 is 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "97.43159305719438"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newton_bj(0, b, beta, y_k, r, train_k_ids, train_t_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba024d2-963a-45fb-831e-66ea2545ba66",
   "metadata": {},
   "source": [
    "#### 2) find $\\beta_{jtl}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "927faf00-5d9a-4065-b5bc-eb21ade3666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_grad_beta_jtl(beta_jtl, j, t, l, beta_jt_no_l, beta_no_j_t, b, y_k, r, k_idxs, t_idxs):\n",
    "    val = 0\n",
    "    no_j = np.concatenate([np.arange(j), np.arange(j+1,len(b))])\n",
    "    no_l = np.concatenate([np.arange(l), np.arange(l+1,len(t_idxs))])\n",
    "    for k in range(len(k_idxs)):\n",
    "        k_t_idx = np.intersect1d(k_idxs[k], t_idxs[t])\n",
    "        num = b[j] + beta_jtl * y_k[k][l] + beta_jt_no_l @ y_k[k][no_l]\n",
    "        x = b[no_j] + beta_no_j_t @ y_k[k]\n",
    "        y = (b[j] + np.hstack([beta_jtl, beta_jt_no_l]) @ \\\n",
    "             np.hstack([y_k[k][l], y_k[k][no_l]])).reshape(-1)\n",
    "        denom = logsumexp(np.hstack([x, y]), 0)\n",
    "        val += ( r[k_t_idx, j, None] * y_k[k][l] * ( 1 - np.exp(num - denom) ) ).sum()\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8e1aae48-4cd1-460f-9253-439d68205659",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 3; t = 5; l = 1\n",
    "no_j = torch.cat([torch.arange(j), torch.arange(j+1, len(b))])\n",
    "no_t = torch.cat([torch.arange(t), torch.arange(t+1, len(train_t_ids))])\n",
    "no_l = torch.cat([torch.arange(l), torch.arange(l+1,len(train_t_ids))])\n",
    "beta_jtl = beta[j,t,l]\n",
    "beta_jt_no_l = beta[j,t,no_l]\n",
    "beta_no_j_t = beta[no_j][:,t]\n",
    "\n",
    "res = [safe_grad_beta_jtl(beta_jtl.detach().numpy(), \n",
    "                     j, t, l, \n",
    "                     beta_jt_no_l.detach().numpy(), \n",
    "                     beta_no_j_t.detach().numpy(), \n",
    "                     b.detach().numpy(), \n",
    "                     y_k.detach().numpy(), \n",
    "                     r.detach().numpy(), \n",
    "                     train_k_ids, train_t_ids) for l in range(len(train_t_ids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b07c73a4-b00b-4ea4-82ee-a7a2c28368c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(res);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fad509f-9f9d-4001-a314-e45432fb0c70",
   "metadata": {},
   "source": [
    "#### CAVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5687868e-e024-4c4d-ac5e-24804bda583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cavi(s, y_k, means, covs, K, T, C, D, k_idxs, t_idxs, max_iter=20, eps=1e-10):\n",
    "    \n",
    "    # initialize\n",
    "    r = torch.ones((s.shape[0], C)) / C\n",
    "    mu, cov = torch.tensor(means), torch.tensor(covs)\n",
    "    b = torch.normal(0, 1, size=(C,))\n",
    "    beta = torch.normal(0, 1, size=(C,T,T))\n",
    "\n",
    "    log_lam = torch.zeros((K, C, T))\n",
    "    for k in range(K):\n",
    "        for t in range(T):\n",
    "            log_lam[k,:,t] = b + beta[:,t] @ y_k[k]\n",
    "\n",
    "    log_norm_lam = log_lam - torch.logsumexp(log_lam, 1)[:,None,:]\n",
    "    \n",
    "    # compute ELBO\n",
    "    elbo, log_dens = compute_elbo(s, r, mu, cov, log_norm_lam, k_idxs, t_idxs, K, T, C)\n",
    "    convergence = 1.\n",
    "    elbos = [elbo]\n",
    "    print(f'initial elbo: {elbos[-1]:.2f}')\n",
    "    \n",
    "    it = 1\n",
    "    while convergence > eps or convergence < 0:  # while ELBO not converged  \n",
    "        \n",
    "        b = b.requires_grad_(True)\n",
    "        \n",
    "        # update q(z)\n",
    "        # r_{ijt}^k = rho_{ijt}^k / sum_j rho_{ijt}^k\n",
    "        # rho_{ijt}^k = exp( log(dN(s_i^kt; mu_j, cov_j)) + \n",
    "        #                      ( log(lambda_jt^(k)) - log(sum_j' lambda_j't^(k)) ) )   \n",
    "        for k in range(K):\n",
    "            for t in range(T):\n",
    "                k_t_idx = np.intersect1d(k_idxs[k], t_idxs[t])\n",
    "                r[k_t_idx] = torch.exp(log_dens[k_t_idx] + log_norm_lam[k,:,t])\n",
    "                r[k_t_idx] = torch.einsum('ij,i->ij', r[k_t_idx], 1/r[k_t_idx].sum(1))\n",
    "            \n",
    "            \n",
    "        # update b \n",
    "        b_new = torch.zeros_like(b)\n",
    "        for j in range(C):\n",
    "            b_new[j] = newton_bj(j, b, beta, y_k, r, k_idxs, t_idxs)\n",
    "        b = b_new.clone().requires_grad_(False)\n",
    "            \n",
    "            \n",
    "        # update beta\n",
    "        beta_new = torch.zeros_like(beta)\n",
    "        for j in range(C):\n",
    "            no_j = torch.cat([torch.arange(j), torch.arange(j+1, len(b))])\n",
    "            for t in range(T):\n",
    "                for l in range(T):\n",
    "                    no_l = torch.cat([torch.arange(l), torch.arange(l+1,len(t_idxs))])\n",
    "                    \n",
    "                    inputs = ( j, t, l, \n",
    "                               beta[j,t,no_l].detach().numpy(), \n",
    "                               beta[no_j][:,t].detach().numpy(),\n",
    "                               b.detach().numpy(), \n",
    "                               y_k.detach().numpy(), \n",
    "                               r.detach().numpy(), \n",
    "                               k_idxs, t_idxs\n",
    "                              )\n",
    "                    sol = optimize.root(safe_grad_beta_jtl, beta[j,t,l].detach().numpy(), \n",
    "                                                    args = inputs, method='lm')\n",
    "                    beta_new[j,t,l] = sol.x[0]\n",
    "                    res = sol.fun\n",
    "                    counter = 0\n",
    "                    init_guess = sol.x[0] - (2*sol.x[0])\n",
    "                    while not np.allclose(res, 0): \n",
    "                        init_guess += sol.x[0] / 10.\n",
    "                        sol = optimize.root(safe_grad_beta_jtl, init_guess, args = inputs, method='lm')\n",
    "                        beta_new[j,t,l] = sol.x[0]\n",
    "                        res = sol.fun.item()\n",
    "                        counter += 1\n",
    "                    if np.logical_and((t+1) % 1 == 0, (l+1) % 1 == 0):\n",
    "                        print(f'grad at beta (j = {j}, t = {t}, l = {l}) : {beta_new[j,t,l]:.2f} is {res:.2f}.')\n",
    "        beta = beta_new.clone()\n",
    "        \n",
    "        \n",
    "        # compute lambda's\n",
    "        log_lam = torch.zeros((K, C, T))\n",
    "        for k in range(K):\n",
    "            for t in range(T):\n",
    "                log_lam[k,:,t] = b + beta[:,t] @ y_k[k]\n",
    "\n",
    "        log_norm_lam = log_lam - torch.logsumexp(log_lam, 1)[:,None,:]\n",
    "        \n",
    "        \n",
    "        # update means and covs\n",
    "        norm = r.sum(0)\n",
    "        mu = torch.einsum('j,ij,ip->jp', 1/norm, r, s)\n",
    "        cov = [torch.einsum(\n",
    "            ',i,ip,id->pd', 1/norm[j], r[:,j], s-mu[j], s-mu[j] \n",
    "            ) for j in range(C)]\n",
    "        \n",
    "        # compute ELBO\n",
    "        elbo, log_dens = compute_elbo(s, r, mu, cov, log_norm_lam, k_idxs, t_idxs, K, T, C)\n",
    "        elbos.append(elbo)\n",
    "        convergence = elbos[-1] - elbos[-2]\n",
    "        \n",
    "        print(f'iter: {it} elbo: {elbos[-1]:.2f}.')\n",
    "        it +=1 \n",
    "        if it > max_iter: \n",
    "            print('reached max iter allowed.')\n",
    "            break\n",
    "            \n",
    "    if abs(convergence) <= eps:\n",
    "        print('converged.')\n",
    "    \n",
    "    return r, log_lam, log_norm_lam, b, beta, mu, cov, elbos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30205606-6402-4fec-915b-fcae3f47e96e",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "641524f1-d532-4222-b802-4da5e6a74258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elbo 1: -115223206.2406357\n",
      "elbo 2: -8201607.994920854\n",
      "elbo 3: 1368129.286682262\n",
      "initial elbo: -122056684.95\n",
      "grad at b_0 = 19.41 is 108503.64\n",
      "grad at b_0 = 68.98 is 0.62\n",
      "grad at b_0 = 79.00 is 0.00\n",
      "grad at b_0 = 82.00 is 0.00\n",
      "grad at b_1 = 48.58 is 73197.92\n",
      "grad at b_1 = 100.03 is 0.00\n",
      "grad at b_1 = 106.03 is 0.00\n",
      "grad at b_2 = 18.57 is 44286.00\n",
      "grad at b_2 = 65.19 is 0.01\n",
      "grad at b_2 = 73.32 is 0.00\n",
      "grad at b_3 = 21.73 is 43860.51\n",
      "grad at b_3 = 64.63 is 0.05\n",
      "grad at b_3 = 74.66 is 0.00\n",
      "grad at b_3 = 74.66 is 0.00\n",
      "grad at b_4 = 26.18 is 34730.24\n",
      "grad at b_4 = 86.54 is 0.00\n",
      "grad at b_4 = 92.54 is 0.00\n",
      "grad at b_5 = 15.12 is 38133.05\n",
      "grad at b_5 = 50.80 is 0.21\n",
      "grad at b_5 = 64.87 is 0.00\n",
      "grad at b_5 = 66.93 is 0.00\n",
      "grad at b_6 = 22.75 is 30082.46\n",
      "grad at b_6 = 95.20 is 0.03\n",
      "grad at b_6 = 105.20 is 0.00\n",
      "grad at b_6 = 105.20 is 0.00\n",
      "grad at b_7 = 15.54 is 34362.33\n",
      "grad at b_7 = 62.79 is 0.14\n",
      "grad at b_7 = 74.01 is 0.00\n",
      "grad at b_7 = 75.04 is 0.00\n",
      "grad at b_8 = 14.66 is 25898.12\n",
      "grad at b_8 = 58.66 is 0.21\n",
      "grad at b_8 = 69.50 is 0.00\n",
      "grad at b_8 = 70.63 is 0.00\n",
      "grad at b_9 = 15.53 is 21974.87\n",
      "grad at b_9 = 51.19 is 0.16\n",
      "grad at b_9 = 91.61 is -0.00\n",
      "grad at beta (j = 0, t = 0, l = 0) : -199.69 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 1) : -168.37 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 2) : -204.29 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 3) : -226.76 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 4) : -557.15 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 5) : -537.80 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 6) : -527.92 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 7) : -781.51 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 8) : -304.70 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 9) : 222.31 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 10) : 36.76 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 11) : 21.58 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 12) : 15.67 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 13) : 17.77 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 14) : 14.88 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 15) : 16.76 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 16) : 17.17 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 17) : 16.71 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 18) : 18.16 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 19) : 16.91 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 20) : 19.14 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 21) : 21.85 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 22) : 21.17 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 23) : 27.22 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 24) : 27.52 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 25) : 28.33 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 26) : 25.95 is 0.00.\n",
      "grad at beta (j = 0, t = 0, l = 27) : 26.86 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 28) : 21.42 is -0.00.\n",
      "grad at beta (j = 0, t = 0, l = 29) : 22.58 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 0) : -231.20 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 1) : -409.99 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 2) : -577.85 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 3) : -584.29 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 4) : -720.55 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 5) : -653.35 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 6) : -497.47 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 7) : -766.75 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 8) : 224.81 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 9) : 220.96 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 10) : 39.23 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 11) : 19.59 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 12) : 17.04 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 13) : 16.49 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 14) : 17.94 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 15) : 17.53 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 16) : 22.47 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 17) : 23.13 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 18) : 22.22 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 19) : 18.26 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 20) : 17.52 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 21) : 16.29 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 22) : 14.82 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 23) : 14.01 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 24) : 15.90 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 25) : 16.63 is 0.00.\n",
      "grad at beta (j = 0, t = 1, l = 26) : 16.56 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 27) : 17.40 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 28) : 15.19 is -0.00.\n",
      "grad at beta (j = 0, t = 1, l = 29) : 15.08 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 0) : -216.05 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 1) : -343.30 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 2) : -595.60 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 3) : -522.85 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 4) : -776.79 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 5) : -381.04 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 6) : -506.61 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 7) : -765.62 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 8) : 144.53 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 9) : 225.60 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 10) : 42.50 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 11) : 19.63 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 12) : 16.00 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 13) : 17.84 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 14) : 20.14 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 15) : 19.65 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 16) : 20.94 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 17) : 22.20 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 18) : 20.03 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 19) : 19.88 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 20) : 17.60 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 21) : 18.99 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 22) : 17.48 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 23) : 15.28 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 24) : 15.19 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 25) : 18.89 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 26) : 18.99 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 27) : 22.68 is -0.00.\n",
      "grad at beta (j = 0, t = 2, l = 28) : 21.90 is 0.00.\n",
      "grad at beta (j = 0, t = 2, l = 29) : 26.52 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 0) : -163.11 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 1) : -163.74 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 2) : -347.77 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 3) : -285.70 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 4) : -394.76 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 5) : -365.38 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 6) : -390.28 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 7) : -518.48 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 8) : 44.10 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 9) : 164.15 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 10) : 25.01 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 11) : 13.48 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 12) : 12.14 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 13) : 10.31 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 14) : 12.41 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 15) : 14.62 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 16) : 13.15 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 17) : 8.58 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 18) : 13.22 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 19) : 13.65 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 20) : 17.17 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 21) : 14.70 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 22) : 18.00 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 23) : 18.69 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 24) : 19.74 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 25) : 18.42 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 26) : 14.51 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 27) : 14.44 is -0.00.\n",
      "grad at beta (j = 0, t = 3, l = 28) : 11.61 is 0.00.\n",
      "grad at beta (j = 0, t = 3, l = 29) : 15.13 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 0) : 2.92 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 1) : -34.08 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 2) : -46.81 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 3) : -63.76 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 4) : -174.08 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 5) : -37.78 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 6) : -90.85 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 7) : -310.86 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 8) : -80.13 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 9) : 118.44 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 10) : 20.15 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 11) : 11.76 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 12) : 10.14 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 13) : 7.95 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 14) : 10.30 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 15) : 11.02 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 16) : 13.23 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 17) : 12.03 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 18) : 15.73 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 19) : 14.96 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 20) : 16.66 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 21) : 16.37 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 22) : 15.69 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 23) : 10.08 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 24) : 11.01 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 25) : 9.33 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 26) : 7.58 is 0.00.\n",
      "grad at beta (j = 0, t = 4, l = 27) : 8.75 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 28) : 6.05 is -0.00.\n",
      "grad at beta (j = 0, t = 4, l = 29) : 8.56 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 0) : -80.50 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 1) : -46.09 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 2) : -100.76 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 3) : -174.12 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 4) : -312.02 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 5) : -147.61 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 6) : -187.33 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 7) : -387.81 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 8) : 220.43 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 9) : 129.50 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 10) : 20.90 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 11) : 11.32 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 12) : 11.40 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 13) : 12.03 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 14) : 13.69 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 15) : 13.99 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 16) : 16.58 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 17) : 11.94 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 18) : 12.45 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 19) : 9.52 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 20) : 6.95 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 21) : 7.86 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 22) : 6.06 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 23) : 3.46 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 24) : 4.92 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 25) : 7.84 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 26) : 6.81 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 27) : 7.26 is -0.00.\n",
      "grad at beta (j = 0, t = 5, l = 28) : 5.75 is 0.00.\n",
      "grad at beta (j = 0, t = 5, l = 29) : 6.60 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 0) : -191.30 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 1) : -331.89 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 2) : -449.65 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 3) : -584.82 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 4) : -985.84 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 5) : -696.38 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 6) : 253.35 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 7) : -720.07 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 8) : 212.89 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 9) : 198.04 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 10) : 36.71 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 11) : 19.40 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 12) : 15.51 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 13) : 16.04 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 14) : 16.68 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 15) : 16.80 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 16) : 15.36 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 17) : 15.09 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 18) : 17.54 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 19) : 17.64 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 20) : 17.41 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 21) : 18.86 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 22) : 20.69 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 23) : 19.78 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 24) : 23.79 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 25) : 20.13 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 26) : 22.15 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 27) : 19.13 is -0.00.\n",
      "grad at beta (j = 0, t = 6, l = 28) : 20.16 is 0.00.\n",
      "grad at beta (j = 0, t = 6, l = 29) : 26.62 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 0) : -182.57 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 1) : -285.09 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 2) : -656.58 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 3) : -621.48 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 4) : -1055.57 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 5) : -771.65 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 6) : -248.43 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 7) : -448.71 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 8) : 311.15 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 9) : 140.50 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 10) : 24.75 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 11) : 12.97 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 12) : 13.21 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 13) : 9.83 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 14) : 9.81 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 15) : 11.35 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 16) : 13.45 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 17) : 12.59 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 18) : 12.51 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 19) : 12.52 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 20) : 13.98 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 21) : 12.82 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 22) : 11.64 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 23) : 9.57 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 24) : 11.40 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 25) : 12.88 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 26) : 13.71 is -0.00.\n",
      "grad at beta (j = 0, t = 7, l = 27) : 13.40 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 28) : 15.92 is 0.00.\n",
      "grad at beta (j = 0, t = 7, l = 29) : 20.25 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 0) : -169.04 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 1) : -257.90 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 2) : -580.38 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 3) : -641.02 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 4) : -904.71 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 5) : -744.98 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 6) : -300.14 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 7) : -446.13 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 8) : 451.68 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 9) : 149.05 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 10) : 26.25 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 11) : 13.74 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 12) : 11.92 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 13) : 13.35 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 14) : 12.97 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 15) : 12.63 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 16) : 12.59 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 17) : 13.71 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 18) : 13.83 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 19) : 15.25 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 20) : 16.34 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 21) : 16.98 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 22) : 18.73 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 23) : 18.74 is -0.00.\n",
      "grad at beta (j = 0, t = 8, l = 24) : 20.80 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 25) : 20.12 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 26) : 18.43 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 27) : 19.96 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 28) : 22.08 is 0.00.\n",
      "grad at beta (j = 0, t = 8, l = 29) : 24.31 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 0) : -179.05 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 1) : 248.81 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 2) : -263.79 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 3) : -518.95 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 4) : -733.57 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 5) : -783.45 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 6) : 191.90 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 7) : -432.01 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 8) : 235.04 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 9) : 110.40 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 10) : 17.93 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 11) : 11.63 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 12) : 10.29 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 13) : 11.28 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 14) : 11.23 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 15) : 10.14 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 16) : 11.14 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 17) : 12.23 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 18) : 11.75 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 19) : 14.10 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 20) : 15.38 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 21) : 12.64 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 22) : 16.35 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 23) : 18.34 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 24) : 20.79 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 25) : 25.57 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 26) : 26.08 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 27) : 24.63 is -0.00.\n",
      "grad at beta (j = 0, t = 9, l = 28) : 21.27 is 0.00.\n",
      "grad at beta (j = 0, t = 9, l = 29) : 24.07 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 0) : -258.09 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 1) : -408.10 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 2) : -389.76 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 3) : -381.02 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 4) : -575.83 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 5) : -515.06 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 6) : -536.92 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 7) : -686.54 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 8) : -195.86 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 9) : 243.53 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 10) : 41.19 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 11) : 22.31 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 12) : 20.28 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 13) : 19.28 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 14) : 17.93 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 15) : 18.62 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 16) : 16.35 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 17) : 15.49 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 18) : 19.23 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 19) : 17.56 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 20) : 16.51 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 21) : 15.12 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 22) : 17.98 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 23) : 16.53 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 24) : 17.93 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 25) : 18.87 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 26) : 20.15 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 27) : 15.92 is -0.00.\n",
      "grad at beta (j = 0, t = 10, l = 28) : 17.67 is 0.00.\n",
      "grad at beta (j = 0, t = 10, l = 29) : 20.22 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 0) : -220.63 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 1) : 155.13 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 2) : -466.74 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 3) : -451.44 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 4) : -751.56 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 5) : -684.99 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 6) : -595.10 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 7) : -945.96 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 8) : 124.36 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 9) : 245.08 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 10) : 43.15 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 11) : 18.99 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 12) : 19.25 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 13) : 17.31 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 14) : 18.52 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 15) : 18.71 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 16) : 22.77 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 17) : 20.15 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 18) : 21.56 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 19) : 24.23 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 20) : 23.48 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 21) : 22.73 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 22) : 22.12 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 23) : 18.81 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 24) : 16.72 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 25) : 17.34 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 26) : 21.52 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 27) : 20.45 is -0.00.\n",
      "grad at beta (j = 0, t = 11, l = 28) : 18.96 is 0.00.\n",
      "grad at beta (j = 0, t = 11, l = 29) : 27.70 is -0.00.\n",
      "grad at beta (j = 0, t = 12, l = 0) : -274.40 is 0.00.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "Cell \u001b[0;32mIn [26], line 69\u001b[0m, in \u001b[0;36mcavi\u001b[0;34m(s, y_k, means, covs, K, T, C, D, k_idxs, t_idxs, max_iter, eps)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(res, \u001b[38;5;241m0\u001b[39m): \n\u001b[1;32m     68\u001b[0m     init_guess \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sol\u001b[38;5;241m.\u001b[39mx[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10.\u001b[39m\n\u001b[0;32m---> 69\u001b[0m     sol \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m(\u001b[49m\u001b[43msafe_grad_beta_jtl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_guess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     beta_new[j,t,l] \u001b[38;5;241m=\u001b[39m sol\u001b[38;5;241m.\u001b[39mx[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     71\u001b[0m     res \u001b[38;5;241m=\u001b[39m sol\u001b[38;5;241m.\u001b[39mfun\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/clusterless/lib/python3.8/site-packages/scipy/optimize/_root.py:236\u001b[0m, in \u001b[0;36mroot\u001b[0;34m(fun, x0, args, method, jac, tol, callback, options)\u001b[0m\n\u001b[1;32m    234\u001b[0m     sol \u001b[38;5;241m=\u001b[39m _root_hybr(fun, x0, args\u001b[38;5;241m=\u001b[39margs, jac\u001b[38;5;241m=\u001b[39mjac, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlm\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 236\u001b[0m     sol \u001b[38;5;241m=\u001b[39m \u001b[43m_root_leastsq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf-sane\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    238\u001b[0m     _warn_jac_unused(jac, method)\n",
      "File \u001b[0;32m~/anaconda3/envs/clusterless/lib/python3.8/site-packages/scipy/optimize/_root.py:294\u001b[0m, in \u001b[0;36m_root_leastsq\u001b[0;34m(fun, x0, args, jac, col_deriv, xtol, ftol, gtol, maxiter, eps, factor, diag, **unknown_options)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03mSolve for least squares with Levenberg-Marquardt\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    N positive entries that serve as a scale factors for the variables.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    293\u001b[0m _check_unknown_options(unknown_options)\n\u001b[0;32m--> 294\u001b[0m x, cov_x, info, msg, ier \u001b[38;5;241m=\u001b[39m \u001b[43mleastsq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDfun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mcol_deriv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcol_deriv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mftol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mftol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mmaxfev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsfcn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mfactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m sol \u001b[38;5;241m=\u001b[39m OptimizeResult(x\u001b[38;5;241m=\u001b[39mx, message\u001b[38;5;241m=\u001b[39mmsg, status\u001b[38;5;241m=\u001b[39mier,\n\u001b[1;32m    301\u001b[0m                      success\u001b[38;5;241m=\u001b[39mier \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m), cov_x\u001b[38;5;241m=\u001b[39mcov_x,\n\u001b[1;32m    302\u001b[0m                      fun\u001b[38;5;241m=\u001b[39minfo\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfvec\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    303\u001b[0m sol\u001b[38;5;241m.\u001b[39mupdate(info)\n",
      "File \u001b[0;32m~/anaconda3/envs/clusterless/lib/python3.8/site-packages/scipy/optimize/_minpack_py.py:410\u001b[0m, in \u001b[0;36mleastsq\u001b[0;34m(func, x0, args, Dfun, full_output, col_deriv, ftol, xtol, gtol, maxfev, epsfcn, factor, diag)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    409\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[0;32m--> 410\u001b[0m shape, dtype \u001b[38;5;241m=\u001b[39m \u001b[43m_check_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleastsq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfunc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m m \u001b[38;5;241m=\u001b[39m shape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m>\u001b[39m m:\n",
      "File \u001b[0;32m~/anaconda3/envs/clusterless/lib/python3.8/site-packages/scipy/optimize/_minpack_py.py:24\u001b[0m, in \u001b[0;36m_check_func\u001b[0;34m(checker, argname, thefunc, x0, args, numinputs, output_shape)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_func\u001b[39m(checker, argname, thefunc, x0, args, numinputs,\n\u001b[1;32m     23\u001b[0m                 output_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 24\u001b[0m     res \u001b[38;5;241m=\u001b[39m atleast_1d(\u001b[43mthefunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnuminputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (output_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (shape(res) \u001b[38;5;241m!=\u001b[39m output_shape):\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (output_shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "Cell \u001b[0;32mIn [24], line 6\u001b[0m, in \u001b[0;36msafe_grad_beta_jtl\u001b[0;34m(beta_jtl, j, t, l, beta_jt_no_l, beta_no_j_t, b, y_k, r, k_idxs, t_idxs)\u001b[0m\n\u001b[1;32m      4\u001b[0m no_l \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([np\u001b[38;5;241m.\u001b[39marange(l), np\u001b[38;5;241m.\u001b[39marange(l\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(t_idxs))])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(k_idxs)):\n\u001b[0;32m----> 6\u001b[0m     k_t_idx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintersect1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk_idxs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_idxs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     num \u001b[38;5;241m=\u001b[39m b[j] \u001b[38;5;241m+\u001b[39m beta_jtl \u001b[38;5;241m*\u001b[39m y_k[k][l] \u001b[38;5;241m+\u001b[39m beta_jt_no_l \u001b[38;5;241m@\u001b[39m y_k[k][no_l]\n\u001b[1;32m      8\u001b[0m     x \u001b[38;5;241m=\u001b[39m b[no_j] \u001b[38;5;241m+\u001b[39m beta_no_j_t \u001b[38;5;241m@\u001b[39m y_k[k]\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mintersect1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/clusterless/lib/python3.8/site-packages/numpy/lib/arraysetops.py:445\u001b[0m, in \u001b[0;36mintersect1d\u001b[0;34m(ar1, ar2, assume_unique, return_indices)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    444\u001b[0m         ar1 \u001b[38;5;241m=\u001b[39m unique(ar1)\n\u001b[0;32m--> 445\u001b[0m         ar2 \u001b[38;5;241m=\u001b[39m \u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     ar1 \u001b[38;5;241m=\u001b[39m ar1\u001b[38;5;241m.\u001b[39mravel()\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/clusterless/lib/python3.8/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/clusterless/lib/python3.8/site-packages/numpy/lib/arraysetops.py:354\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     mask[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m=\u001b[39m aux[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m!=\u001b[39m aux[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 354\u001b[0m ret \u001b[38;5;241m=\u001b[39m (\u001b[43maux\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m,)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_index:\n\u001b[1;32m    356\u001b[0m     ret \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (perm[mask],)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "enc_r, enc_lam, enc_b, enc_beta, enc_mu, enc_cov, enc_elbo = cavi(\n",
    "                                                    s=s, \n",
    "                                                    y_k=torch.tensor(data_loader.y_train), \n",
    "                                                    means=gmm.means_, \n",
    "                                                    covs=gmm.covariances_, \n",
    "                                                    K=K, \n",
    "                                                    T=T, \n",
    "                                                    C=C, \n",
    "                                                    D=train_trials.shape[1], \n",
    "                                                    k_idxs=train_k_ids,\n",
    "                                                    t_idxs=train_t_ids, \n",
    "                                                    max_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdbe2e3-fd39-40bb-aa51-70d8975dfd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(enc_elbo)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('ELBO')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a82a7b-7155-4ada-92f7-4b70166abfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "f1 = ax1.imshow(enc_lam[:,:,0], aspect='auto', cmap='cubehelix')\n",
    "ax1.set_title(f'left')\n",
    "ax1.set_xlabel('time')\n",
    "ax1.set_ylabel('component')\n",
    "fig.colorbar(f1, ax = ax1)\n",
    "\n",
    "f2 = ax2.imshow(enc_lam[:,:,1], aspect='auto', cmap='cubehelix')\n",
    "ax2.set_title(f'right')\n",
    "ax2.set_xlabel('time')\n",
    "fig.colorbar(f2, ax = ax2)\n",
    "\n",
    "f3 = ax3.imshow(enc_lam[:,:,0] - enc_lam[:,:,1], aspect='auto', cmap='cubehelix')\n",
    "ax3.set_title('left - right')\n",
    "ax3.set_xlabel('time')\n",
    "fig.colorbar(f3, ax = ax3)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a8ba57-0027-465a-8f8c-223aca996415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
