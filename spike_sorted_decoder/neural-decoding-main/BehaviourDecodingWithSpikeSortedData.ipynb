{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7QMdZNlPtOu",
        "outputId": "1e7c3fd0-1a25-4e37-9303-523b797ea197"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ONE-api\n",
            "  Downloading one_api-3.0.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting ruff (from ONE-api)\n",
            "  Downloading ruff-0.9.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.11/dist-packages (from ONE-api) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from ONE-api) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.32.1 in /usr/local/lib/python3.11/dist-packages (from ONE-api) (4.67.1)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from ONE-api) (2.32.3)\n",
            "Collecting iblutil>=1.14.0 (from ONE-api)\n",
            "  Downloading iblutil-1.14.0-py3-none-any.whl.metadata (785 bytes)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ONE-api) (24.2)\n",
            "Collecting boto3 (from ONE-api)\n",
            "  Downloading boto3-1.37.8-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from ONE-api) (6.0.2)\n",
            "Collecting colorlog>=6.0.0 (from iblutil>=1.14.0->ONE-api)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from iblutil>=1.14.0->ONE-api) (0.60.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from iblutil>=1.14.0->ONE-api) (18.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->ONE-api) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->ONE-api) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->ONE-api) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ONE-api) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ONE-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ONE-api) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ONE-api) (2025.1.31)\n",
            "Collecting botocore<1.38.0,>=1.37.8 (from boto3->ONE-api)\n",
            "  Downloading botocore-1.37.8-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->ONE-api)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3->ONE-api)\n",
            "  Downloading s3transfer-0.11.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->ONE-api) (1.17.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->iblutil>=1.14.0->ONE-api) (0.43.0)\n",
            "Downloading one_api-3.0.0-py3-none-any.whl (993 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.3/993.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iblutil-1.14.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.37.8-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.5/139.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruff-0.9.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.37.8-py3-none-any.whl (13.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.11.4-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ruff, jmespath, colorlog, botocore, s3transfer, iblutil, boto3, ONE-api\n",
            "Successfully installed ONE-api-3.0.0 boto3-1.37.8 botocore-1.37.8 colorlog-6.9.0 iblutil-1.14.0 jmespath-1.0.1 ruff-0.9.10 s3transfer-0.11.4\n",
            "Collecting ibllib\n",
            "  Downloading ibllib-3.3.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from ibllib) (1.37.8)\n",
            "Requirement already satisfied: click>=7.0.0 in /usr/local/lib/python3.11/dist-packages (from ibllib) (8.1.8)\n",
            "Requirement already satisfied: colorlog>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from ibllib) (6.9.0)\n",
            "Collecting flake8>=3.7.8 (from ibllib)\n",
            "  Downloading flake8-7.1.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting globus-sdk (from ibllib)\n",
            "  Downloading globus_sdk-3.51.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from ibllib) (0.20.3)\n",
            "Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from ibllib) (3.10.0)\n",
            "Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.11/dist-packages (from ibllib) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.11/dist-packages (from ibllib) (1.26.4)\n",
            "Collecting nptdms (from ibllib)\n",
            "  Downloading nptdms-1.10.0.tar.gz (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from ibllib) (4.11.0.86)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ibllib) (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from ibllib) (18.1.0)\n",
            "Collecting pynrrd>=0.4.0 (from ibllib)\n",
            "  Downloading pynrrd-1.1.3-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from ibllib) (8.3.5)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from ibllib) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.11/dist-packages (from ibllib) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from ibllib) (1.13.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from ibllib) (0.25.2)\n",
            "Collecting imagecodecs (from ibllib)\n",
            "  Downloading imagecodecs-2024.12.30-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting sparse (from ibllib)\n",
            "  Downloading sparse-0.15.5-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ibllib) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.32.1 in /usr/local/lib/python3.11/dist-packages (from ibllib) (4.67.1)\n",
            "Collecting iblatlas>=0.5.3 (from ibllib)\n",
            "  Downloading iblatlas-0.5.6-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting ibl-neuropixel>=1.6.2 (from ibllib)\n",
            "  Downloading ibl_neuropixel-1.6.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: iblutil>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from ibllib) (1.14.0)\n",
            "Collecting iblqt>=0.4.2 (from ibllib)\n",
            "  Downloading iblqt-0.4.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting mtscomp>=1.0.1 (from ibllib)\n",
            "  Downloading mtscomp-1.0.2-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: ONE-api>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from ibllib) (3.0.0)\n",
            "Collecting phylib>=2.6.0 (from ibllib)\n",
            "  Downloading phylib-2.6.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting psychofit (from ibllib)\n",
            "  Downloading Psychofit-1.0.0.post0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting slidingRP>=1.1.1 (from ibllib)\n",
            "  Downloading slidingRP-1.1.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting pyqt5 (from ibllib)\n",
            "  Downloading PyQt5-5.15.11-cp38-abi3-manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting ibl-style (from ibllib)\n",
            "  Downloading ibl_style-0.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting mccabe<0.8.0,>=0.7.0 (from flake8>=3.7.8->ibllib)\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting pycodestyle<2.13.0,>=2.12.0 (from flake8>=3.7.8->ibllib)\n",
            "  Downloading pycodestyle-2.12.1-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting pyflakes<3.3.0,>=3.2.0 (from flake8>=3.7.8->ibllib)\n",
            "  Downloading pyflakes-3.2.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from ibl-neuropixel>=1.6.2->ibllib) (1.4.2)\n",
            "Collecting qtpy>=2.4.1 (from iblqt>=0.4.2->ibllib)\n",
            "  Downloading QtPy-2.4.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pyqtgraph>=0.13.7 (from iblqt>=0.4.2->ibllib)\n",
            "  Downloading pyqtgraph-0.13.7-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from iblutil>=1.13.0->ibllib) (24.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->ibllib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->ibllib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->ibllib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->ibllib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->ibllib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->ibllib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->ibllib) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.56->ibllib) (0.43.0)\n",
            "Requirement already satisfied: ruff in /usr/local/lib/python3.11/dist-packages (from ONE-api>=3.0.0->ibllib) (0.9.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from ONE-api>=3.0.0->ibllib) (6.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ibllib) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ibllib) (2025.1)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.11/dist-packages (from phylib>=2.6.0->ibllib) (2024.12.1)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.11/dist-packages (from phylib>=2.6.0->ibllib) (0.12.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from pynrrd>=0.4.0->ibllib) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ibllib) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ibllib) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ibllib) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ibllib) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.1->ibllib) (3.5.0)\n",
            "Requirement already satisfied: colorcet in /usr/local/lib/python3.11/dist-packages (from slidingRP>=1.1.1->ibllib) (3.1.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from slidingRP>=1.1.1->ibllib) (0.14.4)\n",
            "Requirement already satisfied: botocore<1.38.0,>=1.37.8 in /usr/local/lib/python3.11/dist-packages (from boto3->ibllib) (1.37.8)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->ibllib) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from boto3->ibllib) (0.11.4)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]<3.0.0,>=2.0.0->globus-sdk->ibllib) (2.10.1)\n",
            "Requirement already satisfied: cryptography!=3.4.0,>=3.3.1 in /usr/local/lib/python3.11/dist-packages (from globus-sdk->ibllib) (43.0.3)\n",
            "Collecting figrid (from ibl-style->ibllib)\n",
            "  Downloading figrid-0.1.7-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting PyQt5-sip<13,>=12.15 (from pyqt5->ibllib)\n",
            "  Downloading PyQt5_sip-12.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (472 bytes)\n",
            "Collecting PyQt5-Qt5<5.16.0,>=5.15.2 (from pyqt5->ibllib)\n",
            "  Downloading PyQt5_Qt5-5.15.16-1-py3-none-manylinux2014_x86_64.whl.metadata (536 bytes)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->ibllib) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->ibllib) (1.5.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->ibllib) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->ibllib) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->ibllib) (2025.2.18)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->ibllib) (0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography!=3.4.0,>=3.3.1->globus-sdk->ibllib) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.3->ibllib) (1.17.0)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask->phylib>=2.6.0->ibllib) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask->phylib>=2.6.0->ibllib) (2024.10.0)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask->phylib>=2.6.0->ibllib) (1.4.2)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask->phylib>=2.6.0->ibllib) (8.6.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->slidingRP>=1.1.1->ibllib) (1.0.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography!=3.4.0,>=3.3.1->globus-sdk->ibllib) (2.22)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask->phylib>=2.6.0->ibllib) (3.21.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask->phylib>=2.6.0->ibllib) (1.0.0)\n",
            "Downloading ibllib-3.3.0-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flake8-7.1.2-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ibl_neuropixel-1.6.2-py3-none-any.whl (100 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iblatlas-0.5.6-py3-none-any.whl (174 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.5/174.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iblqt-0.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading mtscomp-1.0.2-py2.py3-none-any.whl (16 kB)\n",
            "Downloading phylib-2.6.0-py2.py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.4/80.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynrrd-1.1.3-py3-none-any.whl (23 kB)\n",
            "Downloading slidingRP-1.1.1-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading globus_sdk-3.51.0-py3-none-any.whl (397 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.1/397.1 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ibl_style-0.1.0-py3-none-any.whl (11 kB)\n",
            "Downloading imagecodecs-2024.12.30-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Psychofit-1.0.0.post0-py3-none-any.whl (5.7 kB)\n",
            "Downloading PyQt5-5.15.11-cp38-abi3-manylinux_2_17_x86_64.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sparse-0.15.5-py2.py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Downloading pycodestyle-2.12.1-py2.py3-none-any.whl (31 kB)\n",
            "Downloading pyflakes-3.2.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyQt5_Qt5-5.15.16-1-py3-none-manylinux2014_x86_64.whl (61.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyQt5_sip-12.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.whl (276 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.4/276.4 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyqtgraph-0.13.7-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading QtPy-2.4.3-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.0/95.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading figrid-0.1.7-py3-none-any.whl (7.1 kB)\n",
            "Building wheels for collected packages: nptdms\n",
            "  Building wheel for nptdms (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nptdms: filename=nptdms-1.10.0-py3-none-any.whl size=108395 sha256=39df597a906c618f3f3ab3172a80aa26ea7f8ba70bbb0485ea17bf9ebd664849\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/4b/17/21e8b03b37ea51ce7ec9f5570cdf0decca93f537d61c06880f\n",
            "Successfully built nptdms\n",
            "Installing collected packages: PyQt5-Qt5, mtscomp, qtpy, pyqtgraph, PyQt5-sip, pynrrd, pyflakes, pycodestyle, nptdms, mccabe, imagecodecs, sparse, pyqt5, psychofit, flake8, phylib, slidingRP, globus-sdk, figrid, ibl-style, iblqt, iblatlas, ibl-neuropixel, ibllib\n",
            "Successfully installed PyQt5-Qt5-5.15.16 PyQt5-sip-12.17.0 figrid-0.1.7 flake8-7.1.2 globus-sdk-3.51.0 ibl-neuropixel-1.6.2 ibl-style-0.1.0 iblatlas-0.5.6 ibllib-3.3.0 iblqt-0.4.2 imagecodecs-2024.12.30 mccabe-0.7.0 mtscomp-1.0.2 nptdms-1.10.0 phylib-2.6.0 psychofit-1.0.0.post0 pycodestyle-2.12.1 pyflakes-3.2.0 pynrrd-1.1.3 pyqt5-5.15.11 pyqtgraph-0.13.7 qtpy-2.4.3 slidingRP-1.1.1 sparse-0.15.5\n"
          ]
        }
      ],
      "source": [
        "! pip install ONE-api\n",
        "! pip install ibllib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sWfiy_gO_Qu"
      },
      "outputs": [],
      "source": [
        "from one.api import ONE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import r2_score\n",
        "from brainbox.io.one import SpikeSortingLoader\n",
        "from ibllib.io.extractors.ephys_fpga import extract_wheel_moves\n",
        "\n",
        "# Initialize ONE API with caching\n",
        "one = ONE(base_url='https://openalyx.internationalbrainlab.org',\n",
        "          cache_dir='~/ibl_data_cache', silent=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXonCrmVQ2Ag",
        "outputId": "751a687d-5c85-4534-d808-bda01bcef5bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected to https://openalyx.internationalbrainlab.org as user \"intbrainlab\"\n"
          ]
        }
      ],
      "source": [
        "from one.api import ONE\n",
        "ONE.setup(base_url='https://openalyx.internationalbrainlab.org', silent=True)\n",
        "one = ONE(password='international')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivHVyVx6PoAE",
        "outputId": "0bb44268-b549-4707-e969-e80aa0077beb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 483 valid sessions\n"
          ]
        }
      ],
      "source": [
        "# Find sessions with complete wheel and spike data\n",
        "eids = one.search(project='ibl_neuropixel_brainwide_01',\n",
        "                 datasets=['spikes.times.npy','_ibl_wheel.position.npy'])\n",
        "print(f\"Found {len(eids)} valid sessions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d08Hpvw7ShwO"
      },
      "outputs": [],
      "source": [
        "def load_spike_features(eid, probe='probe00', bin_size=0.02):\n",
        "    sl = SpikeSortingLoader(eid=eid, one=one,pname=probe)\n",
        "    spikes, clusters, channels = sl.load_spike_sorting()\n",
        "    clusters = sl.merge_clusters(spikes, clusters, channels)\n",
        "    cluster_id = np.where(clusters['label'] == 1)\n",
        "    print(f\"Found {len(cluster_id[0])} clusters\")\n",
        "\n",
        "\n",
        "    # Convert to spike counts per time bin\n",
        "    times = np.arange(spikes['times'].min(), spikes['times'].max(), bin_size)\n",
        "    spike_counts = np.zeros((len(times)-1, len(cluster_id[0])))\n",
        "    for i, cluster in enumerate(cluster_id[0]):\n",
        "        cluster_spikes = spikes['times'][spikes['clusters'] == cluster]\n",
        "        spike_counts[:,i], _ = np.histogram(cluster_spikes, bins=times)\n",
        "    return times[:-1], spike_counts, clusters,cluster_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vk_gVXf6Sslj"
      },
      "outputs": [],
      "source": [
        "# working code\n",
        "def process_wheel_data(eid, bin_size=0.02):\n",
        "    wheel = one.load_object(eid, 'wheel', collection='alf')\n",
        "    pos = wheel['position']\n",
        "    ts = wheel['timestamps']\n",
        "\n",
        "    # Compute velocity using central differences\n",
        "    dt = np.diff(ts)\n",
        "    vel = np.gradient(pos, ts)\n",
        "\n",
        "    # Resample to match neural data bins\n",
        "    bins = np.arange(ts[0], ts[-1], bin_size)\n",
        "    vel_binned = np.interp(bins, ts, vel)\n",
        "    return bins, vel_binned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSvTE76fyOh1"
      },
      "outputs": [],
      "source": [
        "def select_features(spike_counts, clusters,cluster_id ,threshold=0.5): # Increased threshold\n",
        "    # Filter based on firing rate stability\n",
        "    fr_mean = spike_counts.mean(0)\n",
        "    fr_std = spike_counts.std(0)\n",
        "    stable_units = (fr_std / (fr_mean + 1e-6)) < threshold\n",
        "    print(f\"Found {np.sum(stable_units)} stable units\")\n",
        "    clusters_df = pd.DataFrame(clusters)\n",
        "    # Ensure at least one feature is selected:\n",
        "    if np.sum(stable_units) == 0:\n",
        "        stable_units = np.ones_like(stable_units, dtype=bool) # Select all if none are stable\n",
        "        print(\"Warning: No stable units found, using all units for decoding.\")\n",
        "    return spike_counts[:, stable_units],  clusters_df.iloc[cluster_id[0].tolist()][stable_units]\n",
        "\n",
        "def align_data(neural_times, neural_features, wheel_times, wheel_speed):\n",
        "    # Find overlapping time window\n",
        "    t_start = max(neural_times[0], wheel_times[0])\n",
        "    t_end = min(neural_times[-1], wheel_times[-1])\n",
        "\n",
        "    # Create common time basis\n",
        "    common_time = np.arange(t_start, t_end, 0.02)\n",
        "\n",
        "    # Interpolate both datasets\n",
        "    # Reshape neural_features to 1D if necessary\n",
        "    if neural_features.ndim > 1:\n",
        "        neural_features = neural_features.reshape(-1) # Reshape to 1D\n",
        "    neural_aligned = np.interp(common_time, neural_times, neural_features)\n",
        "    wheel_aligned = np.interp(common_time, wheel_times, wheel_speed)\n",
        "\n",
        "    return common_time, neural_aligned, wheel_aligned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0L9f59-SxA4"
      },
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "def select_features(spike_counts, clusters, cluster_id, threshold=0.1, min_units=10):\n",
        "    # Dynamic threshold adjustment\n",
        "    fr_mean = spike_counts.mean(0)\n",
        "    fr_std = spike_counts.std(0)\n",
        "    stability = fr_std / (fr_mean + 1e-6)\n",
        "\n",
        "    # Ensure minimum unit count\n",
        "    valid_units = stability < threshold\n",
        "    if np.sum(valid_units) < min_units:\n",
        "        valid_units = stability < np.quantile(stability, min_units/len(stability))\n",
        "    clusters_df = pd.DataFrame(clusters)\n",
        "\n",
        "    # Convert stable_units to a NumPy array to ensure boolean indexing\n",
        "    stable_units_idx = clusters_df.iloc[cluster_id[0].tolist()][valid_units].index\n",
        "    stable_units = np.zeros(clusters_df.shape[0], dtype=bool)\n",
        "    stable_units[stable_units_idx] = True  # Setting to True based on selected indices\n",
        "\n",
        "    print(f\"Found {len(stable_units_idx)} stable units\")  # Updated to use stable_units_idx\n",
        "\n",
        "    return spike_counts[:, valid_units], clusters_df.iloc[cluster_id[0].tolist()][valid_units]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgRl2wjCS0W9"
      },
      "outputs": [],
      "source": [
        "def align_data(neural_times, neural_features, wheel_times, wheel_speed, bin_size=0.02):\n",
        "    # Validate temporal overlap\n",
        "    overlap_start = max(neural_times[0], wheel_times[0])\n",
        "    overlap_end = min(neural_times[-1], wheel_times[-1])\n",
        "\n",
        "    if overlap_start >= overlap_end:\n",
        "        raise ValueError(\n",
        "            f\"No temporal overlap between neural ({neural_times[0]:.2f}-{neural_times[-1]:.2f}s) \"\n",
        "            f\"and wheel ({wheel_times[0]:.2f}-{wheel_times[-1]:.2f}s) data\"\n",
        "        )\n",
        "\n",
        "    # Create unified timebase with phase alignment\n",
        "    common_time = np.arange(overlap_start, overlap_end, bin_size)\n",
        "\n",
        "    # Handle edge case for single-bin data\n",
        "    if len(common_time) < 2:\n",
        "        common_time = np.array([overlap_start])\n",
        "\n",
        "    # Parallelized interpolation\n",
        "    neural_aligned = np.empty((len(common_time), neural_features.shape[1]))\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        futures = []\n",
        "        for col in range(neural_features.shape[1]):\n",
        "            futures.append(\n",
        "                executor.submit(\n",
        "                    np.interp,\n",
        "                    common_time,\n",
        "                    neural_times,\n",
        "                    neural_features[:, col]\n",
        "                )\n",
        "            )\n",
        "        for col, future in enumerate(futures):\n",
        "            neural_aligned[:, col] = future.result()\n",
        "\n",
        "    wheel_aligned = np.interp(common_time, wheel_times, wheel_speed)\n",
        "\n",
        "    return common_time, neural_aligned, wheel_aligned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSVNC_RZYJrc",
        "outputId": "5977e36c-0d77-4b07-b115-666e1386194d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/one/util.py:406: ALFWarning: No default revision for dataset alf/probe00/pykilosort/#2024-05-06#/spikes.amps.npy; using most recent\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/one/util.py:406: ALFWarning: No default revision for dataset alf/probe00/pykilosort/#2024-03-22#/spikes.clusters.npy; using most recent\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/one/util.py:406: ALFWarning: No default revision for dataset alf/probe00/pykilosort/#2024-05-06#/spikes.depths.npy; using most recent\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/one/util.py:406: ALFWarning: No default revision for dataset alf/probe00/pykilosort/spikes.times.npy; using most recent\n",
            "  warnings.warn(\n",
            "100%|██████████| 3/3.0 [00:04<00:00,  1.60s/it]\n",
            "100%|██████████| 3/3.0 [00:03<00:00,  1.27s/it]\n",
            "100%|██████████| 3/3.0 [00:04<00:00,  1.49s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/one/util.py:406: ALFWarning: No default revision for dataset alf/probe00/pykilosort/#2024-05-06#/clusters.channels.npy; using most recent\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/one/util.py:406: ALFWarning: No default revision for dataset alf/probe00/pykilosort/#2024-03-22#/clusters.depths.npy; using most recent\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/one/util.py:406: ALFWarning: No default revision for dataset alf/probe00/pykilosort/#2024-05-06#/clusters.metrics.pqt; using most recent\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/one/util.py:406: ALFWarning: No default revision for dataset alf/probe00/pykilosort/clusters.uuids.csv; using most recent\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/one/util.py:406: ALFWarning: No default revision for dataset alf/probe00/pykilosort/#2024-05-06#/channels.brainLocationIds_ccf_2017.npy; using most recent\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/one/util.py:406: ALFWarning: No default revision for dataset alf/probe00/pykilosort/channels.localCoordinates.npy; using most recent\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/one/util.py:406: ALFWarning: No default revision for dataset alf/probe00/pykilosort/channels.mlapdv.npy; using most recent\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/one/util.py:406: ALFWarning: No default revision for dataset alf/probe00/pykilosort/#2024-05-06#/channels.rawInd.npy; using most recent\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/one/util.py:406: ALFWarning: No default revision for dataset alf/probe00/pykilosort/#2024-05-06#/channels.brainLocationIds_ccf_2017.npy; using most recent\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/one/util.py:406: ALFWarning: No default revision for dataset alf/probe00/pykilosort/channels.localCoordinates.npy; using most recent\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/one/util.py:406: ALFWarning: No default revision for dataset alf/probe00/pykilosort/channels.mlapdv.npy; using most recent\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/one/util.py:406: ALFWarning: No default revision for dataset alf/probe00/pykilosort/#2024-05-06#/channels.rawInd.npy; using most recent\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 291 clusters\n",
            "Found 10 stable units\n",
            "Session ebce500b-c530-47de-8cb1-963c552703ea achieved Validation R²=-0.068\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "class NeuralDecoderMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=256, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.temporal_encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, hidden_size//2),\n",
        "            nn.LayerNorm(hidden_size//2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.temporal_context = nn.LSTM(hidden_size//2, hidden_size//4, batch_first=True)\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(hidden_size//4, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.temporal_encoder(x)\n",
        "        x, _ = self.temporal_context(x)\n",
        "        return self.regressor(x[:, -1, :])\n",
        "\n",
        "def prepare_sequences(X, y, window=15):\n",
        "    \"\"\"Create temporal sequences for neural network input\"\"\"\n",
        "    X_seq = np.zeros((len(X)-window, window, X.shape[1]))\n",
        "    y_seq = np.zeros(len(X)-window)\n",
        "    for i in range(len(X)-window):\n",
        "        X_seq[i] = X[i:i+window]\n",
        "        y_seq[i] = y[i+window]\n",
        "    return X_seq, y_seq\n",
        "\n",
        "def train_mlp_decoder(X, y, device='cuda', n_epochs=100):\n",
        "    # Temporal sequence preparation\n",
        "    X_seq, y_seq = prepare_sequences(X, y)\n",
        "\n",
        "    # Train/val split preserving temporal order\n",
        "    split = int(0.8*len(X_seq))\n",
        "    X_train, X_val = X_seq[:split], X_seq[split:]\n",
        "    y_train, y_val = y_seq[:split], y_seq[split:]\n",
        "\n",
        "    # Normalization per feature channel\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1]))\\\n",
        "                    .reshape(X_train.shape)\n",
        "    X_val = scaler.transform(X_val.reshape(-1, X_val.shape[-1]))\\\n",
        "                 .reshape(X_val.shape)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    train_data = TensorDataset(torch.FloatTensor(X_train),\n",
        "                              torch.FloatTensor(y_train))\n",
        "    val_data = TensorDataset(torch.FloatTensor(X_val),\n",
        "                            torch.FloatTensor(y_val))\n",
        "\n",
        "    # Model configuration\n",
        "    model = NeuralDecoderMLP(input_size=X.shape[1],\n",
        "                            hidden_size=256).to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "    loss_fn = nn.HuberLoss()\n",
        "\n",
        "    # Training loop with early stopping\n",
        "    best_loss = np.inf\n",
        "    patience_counter = 0\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(X_batch.to(device)).squeeze()\n",
        "            loss = loss_fn(pred, y_batch.to(device))\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loader = DataLoader(val_data, batch_size=512)\n",
        "            val_preds = []\n",
        "            val_targets = []  # Accumulate true values as well\n",
        "\n",
        "            for X_val_batch, y_val_batch in val_loader:\n",
        "                pred = model(X_val_batch.to(device)).cpu().numpy().squeeze()\n",
        "                val_preds.extend(pred)  # Use extend for adding individual items\n",
        "                val_targets.extend(y_val_batch.cpu().numpy())  # Collect true values\n",
        "\n",
        "            # Calculate R2 outside the loop on full arrays\n",
        "            val_r2 = r2_score(val_targets, val_preds)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_r2 > best_loss:\n",
        "            best_loss = val_r2\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= 10:\n",
        "            break\n",
        "\n",
        "    # Load best model\n",
        "    #model.load_state_dict(torch.load('best_model.pth'))\n",
        "    return model, val_r2\n",
        "\n",
        "# Modified main analysis loop\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "for eid in eids[:1]:\n",
        "    # try:\n",
        "        # Data loading (same as before)\n",
        "        neural_times, spike_counts, clusters,cluster_id = load_spike_features(eid)\n",
        "        wheel_times, wheel_speed = process_wheel_data(eid)\n",
        "\n",
        "        # Feature selection and alignment\n",
        "        X, cluster_ids = select_features(spike_counts, clusters,cluster_id,threshold=0.1)\n",
        "        common_time, X_aligned, y_aligned = align_data(neural_times, X,\n",
        "                                                      wheel_times, wheel_speed)\n",
        "\n",
        "        # Train MLP decoder\n",
        "        model, val_r2 = train_mlp_decoder(X_aligned, y_aligned, device=device)\n",
        "        print(f\"Session {eid} achieved Validation R²={val_r2:.3f}\")\n",
        "\n",
        "        # Full sequence prediction\n",
        "        X_full, y_full = prepare_sequences(X_aligned, y_aligned)\n",
        "        with torch.no_grad():\n",
        "            full_pred = model(torch.FloatTensor(X_full).to(device)).cpu().numpy()\n",
        "\n",
        "        # Visualization with temporal offset\n",
        "        plt.figure(figsize=(12,4))\n",
        "        plt.plot(common_time[15:], y_full, label='Actual Speed')\n",
        "        plt.plot(common_time[15:], full_pred, label='MLP Prediction')\n",
        "        plt.xlabel('Time (s)')\n",
        "        plt.ylabel('Wheel Speed (cm/s)')\n",
        "        plt.legend()\n",
        "        plt.title(f'MLP Decoding Performance - {eid}')\n",
        "        plt.show()\n",
        "\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Error processing {eid}: {str(e)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.2 (main, Feb  4 2025, 14:51:09) [Clang 16.0.0 (clang-1600.0.26.6)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
